{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 13: PPO and DDPG\n",
    "\n",
    "In this lab, we continue with policy gradient RL methods.\n",
    "There are many algorithms that attempt to improve upon the ordinary Actor critic.\n",
    "Today, we sample two of these: PPO and DDPG.\n",
    "\n",
    "References for this lab:\n",
    "- https://medium.com/deepgamingai/proximal-policy-optimization-tutorial-part-2-2-gae-and-ppo-loss-22337981f815\n",
    "- https://github.com/alirezakazemipour/Continuous-PPO\n",
    "- https://github.com/MWeltevrede/PPO\n",
    "- https://github.com/alirezakazemipour/DDPG-HER\n",
    "- https://sites.ualberta.ca/~pilarski/docs/papers/Pilarski_2013_ICORR_Postprint.pdf\n",
    "- https://github.com/TianhongDai/hindsight-experience-replay\n",
    "- https://github.com/alirezakazemipour/DDPG-HER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proximal Policy Optimization (PPO)\n",
    "\n",
    "The PPO algorithm was introduced by OpenAI in 2017 and quickly surpassed DQN in popularity.\n",
    "PPO collects a small batch of experiences of the agent interacting with the environment and\n",
    "uses that batch to update the policy. Experiences are only used once, immediately, making the\n",
    "method an on-policy method.\n",
    "\n",
    "The key contribution of PPO is a regularization method ensuring that new updates to the policy do not change\n",
    "it too much, leading to lower variance at the cost of some bias, smooth training, without any catastrophic\n",
    "optimizations that lead to useless actions.\n",
    "\n",
    "We'll use the GPU today:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor and critic models\n",
    "\n",
    "As in other AC methods, PPO's actor learns an action distribution conditioned on a particular observed state,\n",
    "while the critic learns the value function.\n",
    "\n",
    "Here we assume the state observation is embedded as a vector and that the action space is continuous. The action\n",
    "distribution will therefore be a Gaussian with diagonal covariance. The value output by the critic, of course, is\n",
    "a single scalar. Both models have two fully connected layers with tanh activations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.py\n",
    "\n",
    "from torch import nn\n",
    "from torch.distributions import normal\n",
    "import torch\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "\n",
    "    def __init__(self, n_states, n_actions):\n",
    "        super(Actor, self).__init__()\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=self.n_states, out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=64)\n",
    "        self.mu = nn.Linear(in_features=64, out_features=self.n_actions)\n",
    "\n",
    "        self.log_std = nn.Parameter(torch.zeros(1, self.n_actions))\n",
    "\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.orthogonal_(layer.weight)\n",
    "                layer.bias.data.zero_()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        mu = self.mu(x)\n",
    "\n",
    "        std = self.log_std.exp()\n",
    "        dist = normal.Normal(mu, std)\n",
    "\n",
    "        return dist\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, n_states):\n",
    "        super(Critic, self).__init__()\n",
    "        self.n_states = n_states\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=self.n_states, out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=64)\n",
    "        self.value = nn.Linear(in_features=64, out_features=1)\n",
    "\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.orthogonal_(layer.weight)\n",
    "                layer.bias.data.zero_()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        value = self.value(x)\n",
    "\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent\n",
    "\n",
    "The agent class wraps the actor and critic models along with optimizers\n",
    "and other bookkeeping functions (saving and loading weights, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.py\n",
    "\n",
    "# from model import Actor, Critic\n",
    "from torch.optim import Adam\n",
    "from torch import from_numpy\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env_name, n_iter, n_states, action_bounds, n_actions, lr):\n",
    "        self.env_name = env_name\n",
    "        self.n_iter = n_iter\n",
    "        self.action_bounds = action_bounds\n",
    "        self.n_actions = n_actions\n",
    "        self.n_states = n_states\n",
    "        self.device = torch.device(process_device)\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.current_policy = Actor(n_states=self.n_states,\n",
    "                                    n_actions=self.n_actions).to(self.device)\n",
    "        self.critic = Critic(n_states=self.n_states).to(self.device)\n",
    "\n",
    "        self.actor_optimizer = Adam(self.current_policy.parameters(), lr=self.lr, eps=1e-5)\n",
    "        self.critic_optimizer = Adam(self.critic.parameters(), lr=self.lr, eps=1e-5)\n",
    "\n",
    "        self.critic_loss = torch.nn.MSELoss()\n",
    "\n",
    "        self.scheduler = lambda step: max(1.0 - float(step / self.n_iter), 0)\n",
    "\n",
    "        self.actor_scheduler = LambdaLR(self.actor_optimizer, lr_lambda=self.scheduler)\n",
    "        self.critic_scheduler = LambdaLR(self.actor_optimizer, lr_lambda=self.scheduler)\n",
    "\n",
    "    def choose_dist(self, state):\n",
    "        state = np.expand_dims(state, 0)\n",
    "        state = from_numpy(state).float().to(self.device)\n",
    "        with torch.no_grad():\n",
    "            dist = self.current_policy(state)\n",
    "\n",
    "        # action *= self.action_bounds[1]\n",
    "        # action = np.clip(action, self.action_bounds[0], self.action_bounds[1])\n",
    "\n",
    "        return dist\n",
    "\n",
    "    def get_value(self, state):\n",
    "        state = np.expand_dims(state, 0)\n",
    "        state = from_numpy(state).float().to(self.device)\n",
    "        with torch.no_grad():\n",
    "            value = self.critic(state)\n",
    "\n",
    "        return value.detach().cpu().numpy()\n",
    "\n",
    "    def optimize(self, actor_loss, critic_loss):\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(self.current_policy.parameters(), 0.5)\n",
    "        # torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 0.5)\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(self.current_policy.parameters(), 0.5)\n",
    "        # torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 0.5)\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "    def schedule_lr(self):\n",
    "        # self.total_scheduler.step()\n",
    "        self.actor_scheduler.step()\n",
    "        self.critic_scheduler.step()\n",
    "\n",
    "    def save_weights(self, iteration, state_rms):\n",
    "        torch.save({\"current_policy_state_dict\": self.current_policy.state_dict(),\n",
    "                    \"critic_state_dict\": self.critic.state_dict(),\n",
    "                    \"actor_optimizer_state_dict\": self.actor_optimizer.state_dict(),\n",
    "                    \"critic_optimizer_state_dict\": self.critic_optimizer.state_dict(),\n",
    "                    \"actor_scheduler_state_dict\": self.actor_scheduler.state_dict(),\n",
    "                    \"critic_scheduler_state_dict\": self.critic_scheduler.state_dict(),\n",
    "                    \"iteration\": iteration,\n",
    "                    \"state_rms_mean\": state_rms.mean,\n",
    "                    \"state_rms_var\": state_rms.var,\n",
    "                    \"state_rms_count\": state_rms.count}, self.env_name + \"_weights.pth\")\n",
    "\n",
    "    def load_weights(self):\n",
    "        checkpoint = torch.load(self.env_name + \"_weights.pth\")\n",
    "        self.current_policy.load_state_dict(checkpoint[\"current_policy_state_dict\"])\n",
    "        self.critic.load_state_dict(checkpoint[\"critic_state_dict\"])\n",
    "        self.actor_optimizer.load_state_dict(checkpoint[\"actor_optimizer_state_dict\"])\n",
    "        self.critic_optimizer.load_state_dict(checkpoint[\"critic_optimizer_state_dict\"])\n",
    "        self.actor_scheduler.load_state_dict(checkpoint[\"actor_scheduler_state_dict\"])\n",
    "        self.critic_scheduler.load_state_dict(checkpoint[\"critic_scheduler_state_dict\"])\n",
    "        iteration = checkpoint[\"iteration\"]\n",
    "        state_rms_mean = checkpoint[\"state_rms_mean\"]\n",
    "        state_rms_var = checkpoint[\"state_rms_var\"]\n",
    "\n",
    "        return iteration, state_rms_mean, state_rms_var\n",
    "\n",
    "    def set_to_eval_mode(self):\n",
    "        self.current_policy.eval()\n",
    "        self.critic.eval()\n",
    "\n",
    "    def set_to_train_mode(self):\n",
    "        self.current_policy.train()\n",
    "        self.critic.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State normalization\n",
    "\n",
    "Recall from batch normalization that it's useful to normalize the output of a layer to a mean of 0 and a standard deviation of 1 before\n",
    "performing further processing.\n",
    "\n",
    "We have also seen that machine learning models can be adversely affected by unusual ranges or scales of inputs.\n",
    "\n",
    "Here we introduce a clever module able to keep track of running means and standard deviations of the state input vector\n",
    "so that we can normalize it before presenting it to the actor or critic models. This means we can deal with any\n",
    "distribution over the state observations with the same model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running_mean_std.py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class RunningMeanStd(object):\n",
    "    # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm\n",
    "    # -> It's indeed batch normalization :D\n",
    "    def __init__(self, epsilon=1e-4, shape=()):\n",
    "        self.mean = np.zeros(shape, 'float64')\n",
    "        self.var = np.ones(shape, 'float64')\n",
    "        self.count = epsilon\n",
    "\n",
    "    def update(self, x):\n",
    "        batch_mean = np.mean(x, axis=0)\n",
    "        batch_var = np.var(x, axis=0)\n",
    "        batch_count = x.shape[0]\n",
    "        self.update_from_moments(batch_mean, batch_var, batch_count)\n",
    "\n",
    "    def update_from_moments(self, batch_mean, batch_var, batch_count):\n",
    "        self.mean, self.var, self.count = update_mean_var_count_from_moments(\n",
    "            self.mean, self.var, self.count, batch_mean, batch_var, batch_count)\n",
    "\n",
    "\n",
    "def update_mean_var_count_from_moments(mean, var, count, batch_mean, batch_var, batch_count):\n",
    "    delta = batch_mean - mean\n",
    "    tot_count = count + batch_count\n",
    "\n",
    "    new_mean = mean + delta * batch_count / tot_count\n",
    "    m_a = var * count\n",
    "    m_b = batch_var * batch_count\n",
    "    M2 = m_a + m_b + np.square(delta) * count * batch_count / tot_count\n",
    "    new_var = M2 / tot_count\n",
    "    new_count = tot_count\n",
    "\n",
    "    return new_mean, new_var, new_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model function\n",
    "\n",
    "`evaluate_model()` runs the agent on\n",
    "a full episode and returns the total reward. Note that it uses the running mean/variance module\n",
    "to normalize and clip state observations.\n",
    "\n",
    "This function is used to see how well the agent is doing during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def evaluate_model(agent, env, state_rms, action_bounds):\n",
    "    total_rewards = 0\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        s = np.clip((s - state_rms.mean) / (state_rms.var ** 0.5 + 1e-8), -5.0, 5.0)\n",
    "        dist = agent.choose_dist(s)\n",
    "        action = dist.sample().cpu().numpy()[0]\n",
    "        # action = np.clip(action, action_bounds[0], action_bounds[1])\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        # env.render()\n",
    "        s = next_state\n",
    "        total_rewards += reward\n",
    "    # env.close()\n",
    "    return total_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO steps\n",
    "\n",
    "As mentioned earlier, PPO trains on short sequences of steps, called PPO steps. While executing these steps, we\n",
    "collect the resulting state-action-reward tuples. After taking the fixed number of steps, we calculate state\n",
    "values and action advantages similar to A2C.\n",
    "\n",
    "### Generalized Advantage Estimation (GAE)\n",
    "\n",
    "As we learned last week, *advantage* measures how good each action is compared to the others we might take in\n",
    "a particular state. We'll use the rewards we collected at each time step, use the critic as a baseline, and\n",
    "combine them to calculate the advantage we obtained (if any) over the critic's estimate of the value at each step.\n",
    "This advantage is used to increase the agent's probability of taking actions with positive advantages and decrease\n",
    "the agent's probability of taking actions with negative advantages.\n",
    "\n",
    "PPO uses Generalized Advantage Estimation (GAE) for this process. We assume $T$ steps. Note that we\n",
    "don't have the benefit of finishing the episode, so we have to use the critic not only as the baseline\n",
    "but also as the estimator of future discounted rewards. Besides the advantage at each individual step,\n",
    "we include the future advantages in the estimate of the advantage for the current step, but discounted\n",
    "by a factor $\\lambda$:\n",
    "\n",
    "1. Initialization: $A_T=0$\n",
    "2. Loop backwards: $t=T-1$ to $t=0$\n",
    "3. Calculate delta: $\\delta = r_t + \\gamma \\cdot V(s_{t+1}) \\cdot m_t - V(s_t)$\n",
    "4. Calculate GAE: $A_t = \\delta + \\gamma \\cdot \\lambda \\cdot m_t \\cdot A_{t+1}$\n",
    "5. Calculate return: $R_t(s_t, a_t) = A_t + V(s_t)$\n",
    "6. Continue loop from step 2\n",
    "\n",
    "We use the following terms:\n",
    "- $m_t$ is a *mask* set to 0 at the end of an episode and 1 otherwise. The mask prevents one episode's initial value from leaking into the\n",
    "  advantage calculation for the previous eposide.\n",
    "- $\\gamma$ is the usual discount factor. We'll use $\\gamma=0.99$.\n",
    "- $\\lambda$ is discounts future advantages when calculating the advantage for the current step. We'll use $\\lambda = 0.95$.\n",
    "\n",
    "### PPO loss\n",
    "\n",
    "Let $\\pi$ be the policy (actor). PPO uses policy gradient with the generalized advantage estimation, but during a\n",
    "batch of training, it ensures that the probability of the selected action does not change too much during optimization.\n",
    "As $\\pi$ changes due to each mini-batch in the larger batch, for a particular state/action pair, we keep track of\n",
    "$\\pi_{old}(s)(a)$ (the probability density for $a$ in state $s$ under the original policy) and calculate\n",
    "$\\pi_{new}(s)(a)$ (the probability density for $a$ in state $s$ under the current policy). We do the\n",
    "following:\n",
    "\n",
    "1. Calculate how much the policy has changed: $r = \\pi_{new}(s)(a) / \\pi_{old}(s)(a)$. For numerical stability, we use the\n",
    "   log form $r = \\exp(\\log (\\pi_{new}(s)(a)) - \\log(\\pi_{old}(s)(a)))$.\n",
    "2. Actor loss is the minimum of two functions $L_{actor} = \\min(p_1, p_2)$ where\n",
    "   $$p_1 = r \\cdot A$$\n",
    "   $$p_2 = \\text{clip}(r, 1-\\epsilon, 1+\\epsilon) \\cdot A$$\n",
    "   We use $\\epsilon=0.2$, for example.\n",
    "3. Calculate critic loss as MSE between returns and predicted critic values: $L_{critic} = (R-V(s))^2$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.py\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "# from running_mean_std import RunningMeanStd\n",
    "# from test import evaluate_model\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "class Train:\n",
    "    def __init__(self, env, test_env, env_name, n_iterations, agent, epochs, mini_batch_size, epsilon, horizon):\n",
    "        self.env = env\n",
    "        self.env_name = env_name\n",
    "        self.test_env = test_env\n",
    "        self.agent = agent\n",
    "        self.epsilon = epsilon\n",
    "        self.horizon = horizon\n",
    "        self.epochs = epochs\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.n_iterations = n_iterations\n",
    "\n",
    "        self.start_time = 0\n",
    "        self.state_rms = RunningMeanStd(shape=(self.agent.n_states,))\n",
    "\n",
    "        self.running_reward = 0\n",
    "\n",
    "    @staticmethod\n",
    "    def choose_mini_batch(mini_batch_size, states, actions, returns, advs, values, log_probs):\n",
    "        full_batch_size = len(states)\n",
    "        for _ in range(full_batch_size // mini_batch_size):\n",
    "            indices = np.random.randint(0, full_batch_size, mini_batch_size)\n",
    "            yield states[indices], actions[indices], returns[indices], advs[indices], values[indices],\\\n",
    "                  log_probs[indices]\n",
    "\n",
    "    def train(self, states, actions, advs, values, log_probs):\n",
    "\n",
    "        values = np.vstack(values[:-1])\n",
    "        log_probs = np.vstack(log_probs)\n",
    "        returns = advs + values\n",
    "        advs = (advs - advs.mean()) / (advs.std() + 1e-8)\n",
    "        actions = np.vstack(actions)\n",
    "        for epoch in range(self.epochs):\n",
    "            for state, action, return_, adv, old_value, old_log_prob in self.choose_mini_batch(self.mini_batch_size,\n",
    "                                                                                               states, actions, returns,\n",
    "                                                                                               advs, values, log_probs):\n",
    "                state = torch.Tensor(state).to(self.agent.device)\n",
    "                action = torch.Tensor(action).to(self.agent.device)\n",
    "                return_ = torch.Tensor(return_).to(self.agent.device)\n",
    "                adv = torch.Tensor(adv).to(self.agent.device)\n",
    "                old_value = torch.Tensor(old_value).to(self.agent.device)\n",
    "                old_log_prob = torch.Tensor(old_log_prob).to(self.agent.device)\n",
    "\n",
    "                value = self.agent.critic(state)\n",
    "                # clipped_value = old_value + torch.clamp(value - old_value, -self.epsilon, self.epsilon)\n",
    "                # clipped_v_loss = (clipped_value - return_).pow(2)\n",
    "                # unclipped_v_loss = (value - return_).pow(2)\n",
    "                # critic_loss = 0.5 * torch.max(clipped_v_loss, unclipped_v_loss).mean()\n",
    "                critic_loss = self.agent.critic_loss(value, return_)\n",
    "\n",
    "                new_log_prob = self.calculate_log_probs(self.agent.current_policy, state, action)\n",
    "\n",
    "                ratio = (new_log_prob - old_log_prob).exp()\n",
    "                actor_loss = self.compute_actor_loss(ratio, adv)\n",
    "\n",
    "                self.agent.optimize(actor_loss, critic_loss)\n",
    "\n",
    "        return actor_loss, critic_loss\n",
    "\n",
    "    def step(self):\n",
    "        state = self.env.reset()\n",
    "        for iteration in range(1, 1 + self.n_iterations):\n",
    "            states = []\n",
    "            actions = []\n",
    "            rewards = []\n",
    "            values = []\n",
    "            log_probs = []\n",
    "            dones = []\n",
    "\n",
    "            self.start_time = time.time()\n",
    "            for t in range(self.horizon):\n",
    "                # self.state_rms.update(state)\n",
    "                state = np.clip((state - self.state_rms.mean) / (self.state_rms.var ** 0.5 + 1e-8), -5, 5)\n",
    "                dist = self.agent.choose_dist(state)\n",
    "                action = dist.sample()\n",
    "                # action = np.clip(action, self.agent.action_bounds[0], self.agent.action_bounds[1])\n",
    "                log_prob = dist.log_prob(action).cpu()\n",
    "                action = action.cpu().numpy()[0]\n",
    "                value = self.agent.get_value(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                values.append(value)\n",
    "                log_probs.append(log_prob)\n",
    "                dones.append(done)\n",
    "\n",
    "                if done:\n",
    "                    state = self.env.reset()\n",
    "                else:\n",
    "                    state = next_state\n",
    "                    \n",
    "            # self.state_rms.update(next_state)\n",
    "            next_state = np.clip((next_state - self.state_rms.mean) / (self.state_rms.var ** 0.5 + 1e-8), -5, 5)\n",
    "            next_value = self.agent.get_value(next_state) * (1 - done)\n",
    "            values.append(next_value)\n",
    "\n",
    "            advs = self.get_gae(rewards, values, dones)\n",
    "            states = np.vstack(states)\n",
    "            actor_loss, critic_loss = self.train(states, actions, advs, values, log_probs)\n",
    "            # self.agent.set_weights()\n",
    "            self.agent.schedule_lr()\n",
    "            eval_rewards = evaluate_model(self.agent, self.test_env, self.state_rms, self.agent.action_bounds)\n",
    "            self.state_rms.update(states)\n",
    "            self.print_logs(iteration, actor_loss, critic_loss, eval_rewards)\n",
    "            print(\"iteration: \", iteration, \"\\teval_rewards: \", eval_rewards)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_gae(rewards, values, dones, gamma=0.99, lam=0.95):\n",
    "\n",
    "        advs = []\n",
    "        gae = 0\n",
    "\n",
    "        dones.append(0)\n",
    "        for step in reversed(range(len(rewards))):\n",
    "            delta = rewards[step] + gamma * (values[step + 1]) * (1 - dones[step]) - values[step]\n",
    "            gae = delta + gamma * lam * (1 - dones[step]) * gae\n",
    "            advs.append(gae)\n",
    "\n",
    "        advs.reverse()\n",
    "        return np.vstack(advs)\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_log_probs(model, states, actions):\n",
    "        policy_distribution = model(states)\n",
    "        return policy_distribution.log_prob(actions)\n",
    "\n",
    "    def compute_actor_loss(self, ratio, adv):\n",
    "        pg_loss1 = adv * ratio\n",
    "        pg_loss2 = adv * torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon)\n",
    "        loss = -torch.min(pg_loss1, pg_loss2).mean()\n",
    "        return loss\n",
    "\n",
    "    def print_logs(self, iteration, actor_loss, critic_loss, eval_rewards):\n",
    "        if iteration == 1:\n",
    "            self.running_reward = eval_rewards\n",
    "        else:\n",
    "            self.running_reward = self.running_reward * 0.99 + eval_rewards * 0.01\n",
    "\n",
    "        if iteration % 100 == 0:\n",
    "            print(f\"Iter:{iteration}| \"\n",
    "                  f\"Ep_Reward:{eval_rewards:.3f}| \"\n",
    "                  f\"Running_reward:{self.running_reward:.3f}| \"\n",
    "                  f\"Actor_Loss:{actor_loss:.3f}| \"\n",
    "                  f\"Critic_Loss:{critic_loss:.3f}| \"\n",
    "                  f\"Iter_duration:{time.time() - self.start_time:.3f}| \"\n",
    "                  f\"lr:{self.agent.actor_scheduler.get_last_lr()}\")\n",
    "            self.agent.save_weights(iteration, self.state_rms)\n",
    "\n",
    "        with SummaryWriter(self.env_name + \"/logs\") as writer:\n",
    "            writer.add_scalar(\"Episode running reward\", self.running_reward, iteration)\n",
    "            writer.add_scalar(\"Episode reward\", eval_rewards, iteration)\n",
    "            writer.add_scalar(\"Actor loss\", actor_loss, iteration)\n",
    "            writer.add_scalar(\"Critic loss\", critic_loss, iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":100\n"
     ]
    }
   ],
   "source": [
    "# You need a DISPLAY for GlfwContext. For Jupyter, try\n",
    "# export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libGLEW.so\n",
    "# xvfb-run -a -s \"-screen 0 1400x900x24\" jupyter lab --ip=<server-ip>\n",
    "\n",
    "!echo $DISPLAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating offscreen glfw\n"
     ]
    }
   ],
   "source": [
    "# play.py\n",
    "\n",
    "from mujoco_py.generated import const\n",
    "from mujoco_py import GlfwContext\n",
    "import numpy as np\n",
    "import cv2\n",
    "GlfwContext(offscreen=True)\n",
    "\n",
    "\n",
    "class Play:\n",
    "    def __init__(self, env, agent, env_name, max_episode=1):\n",
    "        self.env = env\n",
    "        self.max_episode = max_episode\n",
    "        self.agent = agent\n",
    "        _, self.state_rms_mean, self.state_rms_var = self.agent.load_weights()\n",
    "        self.agent.set_to_eval_mode()\n",
    "        self.device = torch.device(process_device)\n",
    "        self.fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "        self.VideoWriter = cv2.VideoWriter(env_name + \".avi\", self.fourcc, 50.0, (250, 250))\n",
    "\n",
    "    def evaluate(self):\n",
    "\n",
    "        for _ in range(self.max_episode):\n",
    "            s = self.env.reset()\n",
    "            episode_reward = 0\n",
    "            for _ in range(self.env._max_episode_steps):\n",
    "                s = np.clip((s - self.state_rms_mean) / (self.state_rms_var ** 0.5 + 1e-8), -5.0, 5.0)\n",
    "                dist = self.agent.choose_dist(s)\n",
    "                action = dist.sample().cpu().numpy()[0]\n",
    "                s_, r, done, _ = self.env.step(action)\n",
    "                episode_reward += r\n",
    "                if done:\n",
    "                    break\n",
    "                s = s_\n",
    "                # self.env.render(mode=\"human\")\n",
    "                # self.env.viewer.cam.type = const.CAMERA_FIXED\n",
    "                # self.env.viewer.cam.fixedcamid = 0\n",
    "                # time.sleep(0.03)\n",
    "                I = self.env.render(mode='rgb_array')\n",
    "                I = cv2.cvtColor(I, cv2.COLOR_RGB2BGR)\n",
    "                I = cv2.resize(I, (250, 250))\n",
    "                self.VideoWriter.write(I)\n",
    "                # cv2.imshow(\"env\", I)\n",
    "                # cv2.waitKey(10)\n",
    "            print(f\"episode reward:{episode_reward:3.3f}\")\n",
    "        self.env.close()\n",
    "        self.VideoWriter.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import mujoco_py\n",
    "# from agent import Agent\n",
    "# from train import Train\n",
    "# from play import Play"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double inverted pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.06460662  0.01146233  0.06048356  0.99993431  0.99816919  0.16894243\n",
      " -0.09502159  0.01060331  0.          0.          0.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mdailey/.local/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "ENV_NAME = \"InvertedDoublePendulum\"\n",
    "TRAIN_FLAG = True\n",
    "test_env = gym.make(ENV_NAME + \"-v2\")\n",
    "\n",
    "print(test_env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = test_env.observation_space.shape[0]\n",
    "action_bounds = [test_env.action_space.low[0], test_env.action_space.high[0]]\n",
    "n_actions = test_env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 500\n",
    "lr = 3e-4\n",
    "epochs = 10\n",
    "clip_range = 0.2\n",
    "mini_batch_size = 64\n",
    "T = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of states:11\n",
      "action bounds:[-1.0, 1.0]\n",
      "number of actions:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mdailey/.local/lib/python3.8/site-packages/h5py/__init__.py:46: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/mdailey/.local/lib/python3.8/site-packages/keras_preprocessing/image/utils.py:23: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  'nearest': pil_image.NEAREST,\n",
      "/home/mdailey/.local/lib/python3.8/site-packages/keras_preprocessing/image/utils.py:24: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  'bilinear': pil_image.BILINEAR,\n",
      "/home/mdailey/.local/lib/python3.8/site-packages/keras_preprocessing/image/utils.py:25: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  'bicubic': pil_image.BICUBIC,\n",
      "/home/mdailey/.local/lib/python3.8/site-packages/keras_preprocessing/image/utils.py:28: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "  if hasattr(pil_image, 'HAMMING'):\n",
      "/home/mdailey/.local/lib/python3.8/site-packages/keras_preprocessing/image/utils.py:30: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  if hasattr(pil_image, 'BOX'):\n",
      "/home/mdailey/.local/lib/python3.8/site-packages/keras_preprocessing/image/utils.py:33: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  if hasattr(pil_image, 'LANCZOS'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  1 \teval_rewards:  45.346473071167615\n",
      "iteration:  2 \teval_rewards:  44.91536430902875\n",
      "iteration:  3 \teval_rewards:  44.92921612766664\n",
      "iteration:  4 \teval_rewards:  82.6538338670625\n",
      "iteration:  5 \teval_rewards:  63.379318111364675\n",
      "iteration:  6 \teval_rewards:  109.81196357687085\n",
      "iteration:  7 \teval_rewards:  63.61989216029566\n",
      "iteration:  8 \teval_rewards:  92.02592167801484\n",
      "iteration:  9 \teval_rewards:  129.22982225496716\n",
      "iteration:  10 \teval_rewards:  118.48907660564143\n",
      "iteration:  11 \teval_rewards:  210.49159521738014\n",
      "iteration:  12 \teval_rewards:  100.93378208439057\n",
      "iteration:  13 \teval_rewards:  324.2461107545972\n",
      "iteration:  14 \teval_rewards:  231.0173071090224\n",
      "iteration:  15 \teval_rewards:  202.25619686186843\n",
      "iteration:  16 \teval_rewards:  129.13539094076623\n",
      "iteration:  17 \teval_rewards:  285.77507420292903\n",
      "iteration:  18 \teval_rewards:  110.37304122282644\n",
      "iteration:  19 \teval_rewards:  220.7514185818968\n",
      "iteration:  20 \teval_rewards:  304.782376569117\n",
      "iteration:  21 \teval_rewards:  165.28531358594356\n",
      "iteration:  22 \teval_rewards:  165.7106681833569\n",
      "iteration:  23 \teval_rewards:  295.387115479218\n",
      "iteration:  24 \teval_rewards:  277.2607630784062\n",
      "iteration:  25 \teval_rewards:  109.8413649850259\n",
      "iteration:  26 \teval_rewards:  331.98035494887495\n",
      "iteration:  27 \teval_rewards:  564.4723686741844\n",
      "iteration:  28 \teval_rewards:  445.6981277646079\n",
      "iteration:  29 \teval_rewards:  147.23383373701876\n",
      "iteration:  30 \teval_rewards:  370.4375554821468\n",
      "iteration:  31 \teval_rewards:  445.1001138289285\n",
      "iteration:  32 \teval_rewards:  380.2246151949146\n",
      "iteration:  33 \teval_rewards:  313.73723145888005\n",
      "iteration:  34 \teval_rewards:  239.68423376669185\n",
      "iteration:  35 \teval_rewards:  417.0129200245029\n",
      "iteration:  36 \teval_rewards:  398.79727035005925\n",
      "iteration:  37 \teval_rewards:  548.5263693727338\n",
      "iteration:  38 \teval_rewards:  771.6005303947886\n",
      "iteration:  39 \teval_rewards:  323.48905962762217\n",
      "iteration:  40 \teval_rewards:  295.8482050467252\n",
      "iteration:  41 \teval_rewards:  1032.249211076539\n",
      "iteration:  42 \teval_rewards:  901.4376241929534\n",
      "iteration:  43 \teval_rewards:  622.5807893822486\n",
      "iteration:  44 \teval_rewards:  371.1886448427981\n",
      "iteration:  45 \teval_rewards:  1935.4206268056903\n",
      "iteration:  46 \teval_rewards:  957.3767699095406\n",
      "iteration:  47 \teval_rewards:  1459.872438054924\n",
      "iteration:  48 \teval_rewards:  1255.5040805770075\n",
      "iteration:  49 \teval_rewards:  1764.9729998479909\n",
      "iteration:  50 \teval_rewards:  557.7182184405269\n",
      "iteration:  51 \teval_rewards:  730.3093268604458\n",
      "iteration:  52 \teval_rewards:  1384.7662837164403\n",
      "iteration:  53 \teval_rewards:  3619.1712852824976\n",
      "iteration:  54 \teval_rewards:  2175.727718325426\n",
      "iteration:  55 \teval_rewards:  8111.147647633033\n",
      "iteration:  56 \teval_rewards:  9307.90724366319\n",
      "iteration:  57 \teval_rewards:  1842.4239337146962\n",
      "iteration:  58 \teval_rewards:  967.464966642509\n",
      "iteration:  59 \teval_rewards:  948.9058257340864\n",
      "iteration:  60 \teval_rewards:  3173.649832464398\n",
      "iteration:  61 \teval_rewards:  948.8865917631606\n",
      "iteration:  62 \teval_rewards:  4300.982757438228\n",
      "iteration:  63 \teval_rewards:  3146.000297414394\n",
      "iteration:  64 \teval_rewards:  9317.568920557924\n",
      "iteration:  65 \teval_rewards:  3566.0546077108197\n",
      "iteration:  66 \teval_rewards:  9315.978987150129\n",
      "iteration:  67 \teval_rewards:  9318.000504326556\n",
      "iteration:  68 \teval_rewards:  9000.305270372797\n",
      "iteration:  69 \teval_rewards:  620.1435759894945\n",
      "iteration:  70 \teval_rewards:  9319.661053173115\n",
      "iteration:  71 \teval_rewards:  3746.526657839672\n",
      "iteration:  72 \teval_rewards:  5480.367700785723\n",
      "iteration:  73 \teval_rewards:  240.25672703399954\n",
      "iteration:  74 \teval_rewards:  9322.209987767488\n",
      "iteration:  75 \teval_rewards:  9325.621732304931\n",
      "iteration:  76 \teval_rewards:  7376.426776387881\n",
      "iteration:  77 \teval_rewards:  3298.350553324919\n",
      "iteration:  78 \teval_rewards:  4577.162429456634\n",
      "iteration:  79 \teval_rewards:  9322.442309032309\n",
      "iteration:  80 \teval_rewards:  9323.226638511802\n",
      "iteration:  81 \teval_rewards:  9317.229575809846\n",
      "iteration:  82 \teval_rewards:  9326.411666516862\n",
      "iteration:  83 \teval_rewards:  9326.549575398081\n",
      "iteration:  84 \teval_rewards:  9325.461849821386\n",
      "iteration:  85 \teval_rewards:  1714.4713391872117\n",
      "iteration:  86 \teval_rewards:  5385.945780099653\n",
      "iteration:  87 \teval_rewards:  1275.8318704364578\n",
      "iteration:  88 \teval_rewards:  2955.5478149988744\n",
      "iteration:  89 \teval_rewards:  6199.880360292815\n",
      "iteration:  90 \teval_rewards:  4006.430513791066\n",
      "iteration:  91 \teval_rewards:  9274.012592108324\n",
      "iteration:  92 \teval_rewards:  520.9299307771918\n",
      "iteration:  93 \teval_rewards:  4490.6812957184375\n",
      "iteration:  94 \teval_rewards:  1163.7695552258324\n",
      "iteration:  95 \teval_rewards:  9319.614457871898\n",
      "iteration:  96 \teval_rewards:  9322.732430805721\n",
      "iteration:  97 \teval_rewards:  9320.407843966937\n",
      "iteration:  98 \teval_rewards:  7684.623504128503\n",
      "iteration:  99 \teval_rewards:  2215.352384671677\n",
      "Iter:100| Ep_Reward:9318.957| Running_reward:2345.871| Actor_Loss:-0.071| Critic_Loss:2654.845| Iter_duration:2.521| lr:[0.00023999999999999998]\n",
      "iteration:  100 \teval_rewards:  9318.957164868953\n",
      "iteration:  101 \teval_rewards:  9317.336897853918\n",
      "iteration:  102 \teval_rewards:  9318.373764710987\n",
      "iteration:  103 \teval_rewards:  9319.833532654931\n",
      "iteration:  104 \teval_rewards:  9323.44697579913\n",
      "iteration:  105 \teval_rewards:  9317.893519085355\n",
      "iteration:  106 \teval_rewards:  9320.854786627835\n",
      "iteration:  107 \teval_rewards:  9322.557424195464\n",
      "iteration:  108 \teval_rewards:  9323.68591015103\n",
      "iteration:  109 \teval_rewards:  127.27174438459488\n",
      "iteration:  110 \teval_rewards:  9322.366915932404\n",
      "iteration:  111 \teval_rewards:  3290.357206050046\n",
      "iteration:  112 \teval_rewards:  9329.03200374843\n",
      "iteration:  113 \teval_rewards:  4082.626700297133\n",
      "iteration:  114 \teval_rewards:  9324.762879566832\n",
      "iteration:  115 \teval_rewards:  3243.8456174504754\n",
      "iteration:  116 \teval_rewards:  9325.86210466199\n",
      "iteration:  117 \teval_rewards:  6842.890940454021\n",
      "iteration:  118 \teval_rewards:  9327.778730871622\n",
      "iteration:  119 \teval_rewards:  9322.489302377384\n",
      "iteration:  120 \teval_rewards:  9326.762074226395\n",
      "iteration:  121 \teval_rewards:  7628.065875542337\n",
      "iteration:  122 \teval_rewards:  9326.230652725166\n",
      "iteration:  123 \teval_rewards:  9323.963025743617\n",
      "iteration:  124 \teval_rewards:  9322.818267292569\n",
      "iteration:  125 \teval_rewards:  9321.770464384519\n",
      "iteration:  126 \teval_rewards:  9323.911845514136\n",
      "iteration:  127 \teval_rewards:  109.6215329954633\n",
      "iteration:  128 \teval_rewards:  9317.42032056113\n",
      "iteration:  129 \teval_rewards:  6612.7164878076965\n",
      "iteration:  130 \teval_rewards:  9320.07011422983\n",
      "iteration:  131 \teval_rewards:  3118.4078135076184\n",
      "iteration:  132 \teval_rewards:  9321.798109434056\n",
      "iteration:  133 \teval_rewards:  4135.878077828872\n",
      "iteration:  134 \teval_rewards:  9320.170151281432\n",
      "iteration:  135 \teval_rewards:  9319.405840624302\n",
      "iteration:  136 \teval_rewards:  9315.398135796338\n",
      "iteration:  137 \teval_rewards:  7365.856969874727\n",
      "iteration:  138 \teval_rewards:  9318.609302301631\n",
      "iteration:  139 \teval_rewards:  9319.216297927855\n",
      "iteration:  140 \teval_rewards:  788.0534490160911\n",
      "iteration:  141 \teval_rewards:  9317.033955880233\n",
      "iteration:  142 \teval_rewards:  9313.986722268655\n",
      "iteration:  143 \teval_rewards:  9313.742493338588\n",
      "iteration:  144 \teval_rewards:  9315.960073456485\n",
      "iteration:  145 \teval_rewards:  9317.857752914628\n",
      "iteration:  146 \teval_rewards:  9315.019700567951\n",
      "iteration:  147 \teval_rewards:  8364.229402006506\n",
      "iteration:  148 \teval_rewards:  9320.541994978958\n",
      "iteration:  149 \teval_rewards:  9319.662956568973\n",
      "iteration:  150 \teval_rewards:  9321.867624792807\n",
      "iteration:  151 \teval_rewards:  9323.148198972041\n",
      "iteration:  152 \teval_rewards:  9322.88959472492\n",
      "iteration:  153 \teval_rewards:  9321.768762669362\n",
      "iteration:  154 \teval_rewards:  9323.45596165239\n",
      "iteration:  155 \teval_rewards:  9246.557991985845\n",
      "iteration:  156 \teval_rewards:  9321.746204566389\n",
      "iteration:  157 \teval_rewards:  9323.21368348827\n",
      "iteration:  158 \teval_rewards:  7504.488243298034\n",
      "iteration:  159 \teval_rewards:  9323.119118388293\n",
      "iteration:  160 \teval_rewards:  9319.559460357232\n",
      "iteration:  161 \teval_rewards:  9323.795150262491\n",
      "iteration:  162 \teval_rewards:  9321.559735886956\n",
      "iteration:  163 \teval_rewards:  9320.035866040984\n",
      "iteration:  164 \teval_rewards:  9318.435354828349\n",
      "iteration:  165 \teval_rewards:  9319.633221891678\n",
      "iteration:  166 \teval_rewards:  9316.169996881321\n",
      "iteration:  167 \teval_rewards:  9317.744055746869\n",
      "iteration:  168 \teval_rewards:  9317.140013324428\n",
      "iteration:  169 \teval_rewards:  9319.474346201614\n",
      "iteration:  170 \teval_rewards:  9321.777392641514\n",
      "iteration:  171 \teval_rewards:  799.6497598091228\n",
      "iteration:  172 \teval_rewards:  9320.16466891599\n",
      "iteration:  173 \teval_rewards:  6631.25261914229\n",
      "iteration:  174 \teval_rewards:  9321.022145916899\n",
      "iteration:  175 \teval_rewards:  9324.49664185366\n",
      "iteration:  176 \teval_rewards:  9327.402479218259\n",
      "iteration:  177 \teval_rewards:  9325.705811554271\n",
      "iteration:  178 \teval_rewards:  9323.223363641877\n",
      "iteration:  179 \teval_rewards:  9326.32586572579\n",
      "iteration:  180 \teval_rewards:  9325.120777269538\n",
      "iteration:  181 \teval_rewards:  9327.686054910771\n",
      "iteration:  182 \teval_rewards:  9324.755159662878\n",
      "iteration:  183 \teval_rewards:  9326.123152344397\n",
      "iteration:  184 \teval_rewards:  9329.443411842603\n",
      "iteration:  185 \teval_rewards:  9328.43270796557\n",
      "iteration:  186 \teval_rewards:  9324.726243276276\n",
      "iteration:  187 \teval_rewards:  9326.878480908328\n",
      "iteration:  188 \teval_rewards:  9330.479478751085\n",
      "iteration:  189 \teval_rewards:  9330.57376483855\n",
      "iteration:  190 \teval_rewards:  9332.081012600347\n",
      "iteration:  191 \teval_rewards:  9331.417945437552\n",
      "iteration:  192 \teval_rewards:  9330.805016384702\n",
      "iteration:  193 \teval_rewards:  9330.780449638092\n",
      "iteration:  194 \teval_rewards:  9328.497134539375\n",
      "iteration:  195 \teval_rewards:  9327.840689989749\n",
      "iteration:  196 \teval_rewards:  9327.329653436764\n",
      "iteration:  197 \teval_rewards:  9327.977592377398\n",
      "iteration:  198 \teval_rewards:  9331.231537264888\n",
      "iteration:  199 \teval_rewards:  9330.866021894566\n",
      "Iter:200| Ep_Reward:4028.580| Running_reward:6314.592| Actor_Loss:-0.143| Critic_Loss:209.988| Iter_duration:2.301| lr:[0.00017999999999999998]\n",
      "iteration:  200 \teval_rewards:  4028.579672743189\n",
      "iteration:  201 \teval_rewards:  9332.079153895735\n",
      "iteration:  202 \teval_rewards:  9330.585637250779\n",
      "iteration:  203 \teval_rewards:  9328.801983202777\n",
      "iteration:  204 \teval_rewards:  9328.914985126494\n",
      "iteration:  205 \teval_rewards:  9329.516429254045\n",
      "iteration:  206 \teval_rewards:  9322.97136902525\n",
      "iteration:  207 \teval_rewards:  9325.056619648352\n",
      "iteration:  208 \teval_rewards:  9326.568396008624\n",
      "iteration:  209 \teval_rewards:  9329.350381601404\n",
      "iteration:  210 \teval_rewards:  9334.512645507264\n",
      "iteration:  211 \teval_rewards:  9335.814268293785\n",
      "iteration:  212 \teval_rewards:  9333.14848021855\n",
      "iteration:  213 \teval_rewards:  9337.450196830792\n",
      "iteration:  214 \teval_rewards:  9340.429870724065\n",
      "iteration:  215 \teval_rewards:  9337.803154822503\n",
      "iteration:  216 \teval_rewards:  9337.39010628585\n",
      "iteration:  217 \teval_rewards:  9336.226131287862\n",
      "iteration:  218 \teval_rewards:  3806.3317507610745\n",
      "iteration:  219 \teval_rewards:  9334.4257482297\n",
      "iteration:  220 \teval_rewards:  1836.3256506929406\n",
      "iteration:  221 \teval_rewards:  147.32343109901265\n",
      "iteration:  222 \teval_rewards:  9331.241819123952\n",
      "iteration:  223 \teval_rewards:  9331.943644430383\n",
      "iteration:  224 \teval_rewards:  9335.211139605955\n",
      "iteration:  225 \teval_rewards:  2982.31140782144\n",
      "iteration:  226 \teval_rewards:  9333.928794513422\n",
      "iteration:  227 \teval_rewards:  9333.078937498753\n",
      "iteration:  228 \teval_rewards:  6611.886309566846\n",
      "iteration:  229 \teval_rewards:  9334.310138946741\n",
      "iteration:  230 \teval_rewards:  689.704340362256\n",
      "iteration:  231 \teval_rewards:  9340.667115190248\n",
      "iteration:  232 \teval_rewards:  9339.036326032172\n",
      "iteration:  233 \teval_rewards:  9336.111563733315\n",
      "iteration:  234 \teval_rewards:  9336.28806558162\n",
      "iteration:  235 \teval_rewards:  9337.307717196843\n",
      "iteration:  236 \teval_rewards:  9335.39799952935\n",
      "iteration:  237 \teval_rewards:  6141.5219104919615\n",
      "iteration:  238 \teval_rewards:  9337.753226006454\n",
      "iteration:  239 \teval_rewards:  9337.373524351633\n",
      "iteration:  240 \teval_rewards:  9335.905726679894\n",
      "iteration:  241 \teval_rewards:  3060.720528634081\n",
      "iteration:  242 \teval_rewards:  9335.774861069618\n",
      "iteration:  243 \teval_rewards:  2678.273378168627\n",
      "iteration:  244 \teval_rewards:  9337.344536168232\n",
      "iteration:  245 \teval_rewards:  9337.301450469962\n",
      "iteration:  246 \teval_rewards:  9337.297598679334\n",
      "iteration:  247 \teval_rewards:  9339.150577592296\n",
      "iteration:  248 \teval_rewards:  9338.92714958901\n",
      "iteration:  249 \teval_rewards:  9337.976664934471\n",
      "iteration:  250 \teval_rewards:  9337.895033677572\n",
      "iteration:  251 \teval_rewards:  9337.770507659605\n",
      "iteration:  252 \teval_rewards:  9334.95959359312\n",
      "iteration:  253 \teval_rewards:  9335.164146393992\n",
      "iteration:  254 \teval_rewards:  9335.050829351141\n",
      "iteration:  255 \teval_rewards:  9336.01898822483\n",
      "iteration:  256 \teval_rewards:  5907.190728398626\n",
      "iteration:  257 \teval_rewards:  82.54370883289567\n",
      "iteration:  258 \teval_rewards:  9330.501421912431\n",
      "iteration:  259 \teval_rewards:  9329.69196076871\n",
      "iteration:  260 \teval_rewards:  9330.616536830945\n",
      "iteration:  261 \teval_rewards:  2507.039776444613\n",
      "iteration:  262 \teval_rewards:  9327.318149452442\n",
      "iteration:  263 \teval_rewards:  9329.479470628967\n",
      "iteration:  264 \teval_rewards:  9328.601429420089\n",
      "iteration:  265 \teval_rewards:  9329.069106901394\n",
      "iteration:  266 \teval_rewards:  9332.201825832584\n",
      "iteration:  267 \teval_rewards:  9331.751675017547\n",
      "iteration:  268 \teval_rewards:  63.647035559871455\n",
      "iteration:  269 \teval_rewards:  9332.10567998145\n",
      "iteration:  270 \teval_rewards:  9332.962604816037\n",
      "iteration:  271 \teval_rewards:  9336.086408887928\n",
      "iteration:  272 \teval_rewards:  9334.985677677145\n",
      "iteration:  273 \teval_rewards:  9337.739708096418\n",
      "iteration:  274 \teval_rewards:  9334.087672486765\n",
      "iteration:  275 \teval_rewards:  9336.076074153689\n",
      "iteration:  276 \teval_rewards:  9336.809613696945\n",
      "iteration:  277 \teval_rewards:  9336.347429745201\n",
      "iteration:  278 \teval_rewards:  9338.89116252199\n",
      "iteration:  279 \teval_rewards:  9339.215325648714\n",
      "iteration:  280 \teval_rewards:  9340.092803134808\n",
      "iteration:  281 \teval_rewards:  9339.922497826785\n",
      "iteration:  282 \teval_rewards:  9339.572529513434\n",
      "iteration:  283 \teval_rewards:  9338.221252148891\n",
      "iteration:  284 \teval_rewards:  9339.58333938701\n",
      "iteration:  285 \teval_rewards:  9338.476955362352\n",
      "iteration:  286 \teval_rewards:  9337.735376906956\n",
      "iteration:  287 \teval_rewards:  9336.254716890468\n",
      "iteration:  288 \teval_rewards:  9337.2480885739\n",
      "iteration:  289 \teval_rewards:  9338.575699556906\n",
      "iteration:  290 \teval_rewards:  9337.663300757586\n",
      "iteration:  291 \teval_rewards:  9339.504972612089\n",
      "iteration:  292 \teval_rewards:  9338.664527975661\n",
      "iteration:  293 \teval_rewards:  9338.849522991979\n",
      "iteration:  294 \teval_rewards:  9339.313044039172\n",
      "iteration:  295 \teval_rewards:  8970.74506023067\n",
      "iteration:  296 \teval_rewards:  390.4754521889693\n",
      "iteration:  297 \teval_rewards:  1670.1030565983353\n",
      "iteration:  298 \teval_rewards:  1735.6615749558757\n",
      "iteration:  299 \teval_rewards:  4265.909367465384\n",
      "Iter:300| Ep_Reward:2614.245| Running_reward:7404.248| Actor_Loss:-0.060| Critic_Loss:3413.060| Iter_duration:2.207| lr:[0.00011999999999999999]\n",
      "iteration:  300 \teval_rewards:  2614.2451543857137\n",
      "iteration:  301 \teval_rewards:  9340.150478668233\n",
      "iteration:  302 \teval_rewards:  9340.276173963932\n",
      "iteration:  303 \teval_rewards:  9342.512361287925\n",
      "iteration:  304 \teval_rewards:  9340.13275526427\n",
      "iteration:  305 \teval_rewards:  9340.267120251012\n",
      "iteration:  306 \teval_rewards:  9342.222335185235\n",
      "iteration:  307 \teval_rewards:  9341.232797962002\n",
      "iteration:  308 \teval_rewards:  9341.597053751124\n",
      "iteration:  309 \teval_rewards:  9342.461678667378\n",
      "iteration:  310 \teval_rewards:  9339.53493842438\n",
      "iteration:  311 \teval_rewards:  4845.7435717283515\n",
      "iteration:  312 \teval_rewards:  9340.678089954952\n",
      "iteration:  313 \teval_rewards:  202.02434881661887\n",
      "iteration:  314 \teval_rewards:  9341.378124689581\n",
      "iteration:  315 \teval_rewards:  9341.306984882041\n",
      "iteration:  316 \teval_rewards:  781.7450143614551\n",
      "iteration:  317 \teval_rewards:  9342.611044882306\n",
      "iteration:  318 \teval_rewards:  9343.502105462268\n",
      "iteration:  319 \teval_rewards:  9338.050427428543\n",
      "iteration:  320 \teval_rewards:  9338.752935144355\n",
      "iteration:  321 \teval_rewards:  9341.649728871724\n",
      "iteration:  322 \teval_rewards:  82.27887427241158\n",
      "iteration:  323 \teval_rewards:  3454.056538235836\n",
      "iteration:  324 \teval_rewards:  9341.303735460071\n",
      "iteration:  325 \teval_rewards:  9340.242299226458\n",
      "iteration:  326 \teval_rewards:  5153.504958615133\n",
      "iteration:  327 \teval_rewards:  9339.445236786834\n",
      "iteration:  328 \teval_rewards:  7346.121125798365\n",
      "iteration:  329 \teval_rewards:  9338.254891390023\n",
      "iteration:  330 \teval_rewards:  754.3014474687657\n",
      "iteration:  331 \teval_rewards:  670.3310489511872\n",
      "iteration:  332 \teval_rewards:  388.8207868956408\n",
      "iteration:  333 \teval_rewards:  3314.54795235845\n",
      "iteration:  334 \teval_rewards:  9340.23923754592\n",
      "iteration:  335 \teval_rewards:  9342.900881573247\n",
      "iteration:  336 \teval_rewards:  9341.72070738998\n",
      "iteration:  337 \teval_rewards:  9341.88423244789\n",
      "iteration:  338 \teval_rewards:  9343.418430689511\n",
      "iteration:  339 \teval_rewards:  9343.293468797854\n",
      "iteration:  340 \teval_rewards:  689.8541395328342\n",
      "iteration:  341 \teval_rewards:  9342.712496754062\n",
      "iteration:  342 \teval_rewards:  9342.943208424602\n",
      "iteration:  343 \teval_rewards:  9344.163024055662\n",
      "iteration:  344 \teval_rewards:  9345.092945228153\n",
      "iteration:  345 \teval_rewards:  9345.18378396993\n",
      "iteration:  346 \teval_rewards:  9343.809229748846\n",
      "iteration:  347 \teval_rewards:  9345.145278400167\n",
      "iteration:  348 \teval_rewards:  9343.351947310488\n",
      "iteration:  349 \teval_rewards:  9342.943383811904\n",
      "iteration:  350 \teval_rewards:  9343.517606299987\n",
      "iteration:  351 \teval_rewards:  9345.118227018995\n",
      "iteration:  352 \teval_rewards:  9345.614174121356\n",
      "iteration:  353 \teval_rewards:  9345.771494293\n",
      "iteration:  354 \teval_rewards:  9346.029419630688\n",
      "iteration:  355 \teval_rewards:  9344.35334751307\n",
      "iteration:  356 \teval_rewards:  9342.875293403047\n",
      "iteration:  357 \teval_rewards:  9343.778731112192\n",
      "iteration:  358 \teval_rewards:  9343.892240876425\n",
      "iteration:  359 \teval_rewards:  9342.142599669827\n",
      "iteration:  360 \teval_rewards:  9344.046435201122\n",
      "iteration:  361 \teval_rewards:  3518.757743356669\n",
      "iteration:  362 \teval_rewards:  9342.881655784931\n",
      "iteration:  363 \teval_rewards:  9341.602069461262\n",
      "iteration:  364 \teval_rewards:  9341.562315522997\n",
      "iteration:  365 \teval_rewards:  9344.093404893514\n",
      "iteration:  366 \teval_rewards:  9342.673570836692\n",
      "iteration:  367 \teval_rewards:  9341.009184513488\n",
      "iteration:  368 \teval_rewards:  9342.534343828203\n",
      "iteration:  369 \teval_rewards:  9343.365739432811\n",
      "iteration:  370 \teval_rewards:  1371.0513464087437\n",
      "iteration:  371 \teval_rewards:  9342.250089601926\n",
      "iteration:  372 \teval_rewards:  9340.728214124016\n",
      "iteration:  373 \teval_rewards:  1678.6993898929225\n",
      "iteration:  374 \teval_rewards:  9337.949899273166\n",
      "iteration:  375 \teval_rewards:  9337.298158951384\n",
      "iteration:  376 \teval_rewards:  9341.136904135788\n",
      "iteration:  377 \teval_rewards:  6340.426515206658\n",
      "iteration:  378 \teval_rewards:  1564.2992287606257\n",
      "iteration:  379 \teval_rewards:  7251.809216959315\n",
      "iteration:  380 \teval_rewards:  9340.86948798053\n",
      "iteration:  381 \teval_rewards:  9336.97873996308\n",
      "iteration:  382 \teval_rewards:  9340.50275000588\n",
      "iteration:  383 \teval_rewards:  9340.60617143669\n",
      "iteration:  384 \teval_rewards:  9341.58114790333\n",
      "iteration:  385 \teval_rewards:  9343.869931240013\n",
      "iteration:  386 \teval_rewards:  9344.77097885302\n",
      "iteration:  387 \teval_rewards:  9339.595844636204\n",
      "iteration:  388 \teval_rewards:  9339.625231638007\n",
      "iteration:  389 \teval_rewards:  9341.096506586893\n",
      "iteration:  390 \teval_rewards:  9343.677530874025\n",
      "iteration:  391 \teval_rewards:  9342.656523487827\n",
      "iteration:  392 \teval_rewards:  9343.68263979773\n",
      "iteration:  393 \teval_rewards:  9342.817473586203\n",
      "iteration:  394 \teval_rewards:  9344.790501317906\n",
      "iteration:  395 \teval_rewards:  9341.621301864112\n",
      "iteration:  396 \teval_rewards:  9338.447390193933\n",
      "iteration:  397 \teval_rewards:  9339.85085287173\n",
      "iteration:  398 \teval_rewards:  7733.102597480646\n",
      "iteration:  399 \teval_rewards:  9334.490620889012\n",
      "Iter:400| Ep_Reward:9337.627| Running_reward:7955.572| Actor_Loss:-0.073| Critic_Loss:2648.894| Iter_duration:2.470| lr:[5.999999999999998e-05]\n",
      "iteration:  400 \teval_rewards:  9337.626747958018\n",
      "iteration:  401 \teval_rewards:  9339.584643007476\n",
      "iteration:  402 \teval_rewards:  9340.100961055017\n",
      "iteration:  403 \teval_rewards:  9342.270048519948\n",
      "iteration:  404 \teval_rewards:  9342.682224557464\n",
      "iteration:  405 \teval_rewards:  9340.51234811003\n",
      "iteration:  406 \teval_rewards:  9342.77320989145\n",
      "iteration:  407 \teval_rewards:  9335.180127077514\n",
      "iteration:  408 \teval_rewards:  9338.57197057003\n",
      "iteration:  409 \teval_rewards:  9333.27273277399\n",
      "iteration:  410 \teval_rewards:  9338.663881539562\n",
      "iteration:  411 \teval_rewards:  9340.413029021945\n",
      "iteration:  412 \teval_rewards:  9341.55801059392\n",
      "iteration:  413 \teval_rewards:  9340.935342345767\n",
      "iteration:  414 \teval_rewards:  9342.652960730042\n",
      "iteration:  415 \teval_rewards:  9341.884186322615\n",
      "iteration:  416 \teval_rewards:  9340.918792168575\n",
      "iteration:  417 \teval_rewards:  9340.767780848326\n",
      "iteration:  418 \teval_rewards:  9340.029903024315\n",
      "iteration:  419 \teval_rewards:  9339.68411534873\n",
      "iteration:  420 \teval_rewards:  9340.544771506375\n"
     ]
    }
   ],
   "source": [
    "print(f\"number of states:{n_states}\\n\"\n",
    "      f\"action bounds:{action_bounds}\\n\"\n",
    "      f\"number of actions:{n_actions}\")\n",
    "\n",
    "if not os.path.exists(ENV_NAME):\n",
    "    os.mkdir(ENV_NAME)\n",
    "    os.mkdir(ENV_NAME + \"/logs\")\n",
    "\n",
    "env = gym.make(ENV_NAME + \"-v2\")\n",
    "\n",
    "agent = Agent(n_states=n_states,\n",
    "              n_iter=n_iterations,\n",
    "              env_name=ENV_NAME,\n",
    "              action_bounds=action_bounds,\n",
    "              n_actions=n_actions,\n",
    "              lr=lr)\n",
    "if TRAIN_FLAG:\n",
    "    trainer = Train(env=env,\n",
    "                    test_env=test_env,\n",
    "                    env_name=ENV_NAME,\n",
    "                    agent=agent,\n",
    "                    horizon=T,\n",
    "                    n_iterations=n_iterations,\n",
    "                    epochs=epochs,\n",
    "                    mini_batch_size=mini_batch_size,\n",
    "                    epsilon=clip_range)\n",
    "    trainer.step()\n",
    "\n",
    "player = Play(env, agent, ENV_NAME)\n",
    "player.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Walker 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"Walker2d\"\n",
    "TRAIN_FLAG = True\n",
    "test_env = gym.make(ENV_NAME + \"-v2\")\n",
    "\n",
    "n_iterations = 1500\n",
    "\n",
    "n_states = test_env.observation_space.shape[0]\n",
    "action_bounds = [test_env.action_space.low[0], test_env.action_space.high[0]]\n",
    "n_actions = test_env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of states:17\n",
      "action bounds:[-1.0, 1.0]\n",
      "number of actions:6\n",
      "iteration:  1 \teval_rewards:  0.5474504746026824\n",
      "iteration:  2 \teval_rewards:  0.6139819397717825\n",
      "iteration:  3 \teval_rewards:  15.773683076676134\n",
      "iteration:  4 \teval_rewards:  19.35315174005951\n",
      "iteration:  5 \teval_rewards:  31.776205368983803\n",
      "iteration:  6 \teval_rewards:  51.92884940344699\n",
      "iteration:  7 \teval_rewards:  124.46137137369442\n",
      "iteration:  8 \teval_rewards:  89.33800299587827\n",
      "iteration:  9 \teval_rewards:  37.98785101513507\n",
      "iteration:  10 \teval_rewards:  57.80770145720188\n",
      "iteration:  11 \teval_rewards:  398.0232305750942\n",
      "iteration:  12 \teval_rewards:  219.2012841307943\n",
      "iteration:  13 \teval_rewards:  316.2882735451255\n",
      "iteration:  14 \teval_rewards:  91.5811223126468\n",
      "iteration:  15 \teval_rewards:  59.94967645616366\n",
      "iteration:  16 \teval_rewards:  316.7300725306001\n",
      "iteration:  17 \teval_rewards:  482.75372123908244\n",
      "iteration:  18 \teval_rewards:  307.34882314051424\n",
      "iteration:  19 \teval_rewards:  331.29939983334435\n",
      "iteration:  20 \teval_rewards:  301.1742615336524\n",
      "iteration:  21 \teval_rewards:  376.6402810871299\n",
      "iteration:  22 \teval_rewards:  54.50847683739232\n",
      "iteration:  23 \teval_rewards:  225.8424724376022\n",
      "iteration:  24 \teval_rewards:  85.11283301095176\n",
      "iteration:  25 \teval_rewards:  264.69305420062295\n",
      "iteration:  26 \teval_rewards:  101.80136629185344\n",
      "iteration:  27 \teval_rewards:  443.8832785417097\n",
      "iteration:  28 \teval_rewards:  335.35966617120425\n",
      "iteration:  29 \teval_rewards:  371.7098934080925\n",
      "iteration:  30 \teval_rewards:  452.2855896744194\n",
      "iteration:  31 \teval_rewards:  346.3394909968723\n",
      "iteration:  32 \teval_rewards:  361.27945818164585\n",
      "iteration:  33 \teval_rewards:  325.3510988520634\n",
      "iteration:  34 \teval_rewards:  542.9441835353157\n",
      "iteration:  35 \teval_rewards:  352.44056232000514\n",
      "iteration:  36 \teval_rewards:  446.8188004273092\n",
      "iteration:  37 \teval_rewards:  301.7306753160963\n",
      "iteration:  38 \teval_rewards:  451.7311844584459\n",
      "iteration:  39 \teval_rewards:  409.5915051993828\n",
      "iteration:  40 \teval_rewards:  316.4847398534381\n",
      "iteration:  41 \teval_rewards:  420.9624814847448\n",
      "iteration:  42 \teval_rewards:  347.930781491284\n",
      "iteration:  43 \teval_rewards:  344.67953169006023\n",
      "iteration:  44 \teval_rewards:  402.3824933616361\n",
      "iteration:  45 \teval_rewards:  369.6998788658723\n",
      "iteration:  46 \teval_rewards:  441.8482856445946\n",
      "iteration:  47 \teval_rewards:  354.5344333094015\n",
      "iteration:  48 \teval_rewards:  352.84760055582126\n",
      "iteration:  49 \teval_rewards:  437.3039756776059\n",
      "iteration:  50 \teval_rewards:  370.21791284869454\n",
      "iteration:  51 \teval_rewards:  358.85820902775095\n",
      "iteration:  52 \teval_rewards:  442.68507748078133\n",
      "iteration:  53 \teval_rewards:  364.8800655727081\n",
      "iteration:  54 \teval_rewards:  452.3467152546603\n",
      "iteration:  55 \teval_rewards:  479.7036796264677\n",
      "iteration:  56 \teval_rewards:  264.48349290203464\n",
      "iteration:  57 \teval_rewards:  435.320797149117\n",
      "iteration:  58 \teval_rewards:  426.8863861724255\n",
      "iteration:  59 \teval_rewards:  446.45544996245985\n",
      "iteration:  60 \teval_rewards:  398.1964663329382\n",
      "iteration:  61 \teval_rewards:  443.74053227516384\n",
      "iteration:  62 \teval_rewards:  385.91935364890804\n",
      "iteration:  63 \teval_rewards:  469.6354877742809\n",
      "iteration:  64 \teval_rewards:  453.82205524211764\n",
      "iteration:  65 \teval_rewards:  358.50739411384865\n",
      "iteration:  66 \teval_rewards:  487.6037282565553\n",
      "iteration:  67 \teval_rewards:  484.18767651427436\n",
      "iteration:  68 \teval_rewards:  420.05898741807306\n",
      "iteration:  69 \teval_rewards:  223.85949281403788\n",
      "iteration:  70 \teval_rewards:  459.8187964413443\n",
      "iteration:  71 \teval_rewards:  324.6348517657128\n",
      "iteration:  72 \teval_rewards:  422.4056979177005\n",
      "iteration:  73 \teval_rewards:  479.44707255209585\n",
      "iteration:  74 \teval_rewards:  282.451484368281\n",
      "iteration:  75 \teval_rewards:  422.18353490144113\n",
      "iteration:  76 \teval_rewards:  482.17788999179\n",
      "iteration:  77 \teval_rewards:  444.9704891534046\n",
      "iteration:  78 \teval_rewards:  443.8499688171544\n",
      "iteration:  79 \teval_rewards:  445.77642238040545\n",
      "iteration:  80 \teval_rewards:  464.52888970301507\n",
      "iteration:  81 \teval_rewards:  484.9415634990634\n",
      "iteration:  82 \teval_rewards:  476.5609131420507\n",
      "iteration:  83 \teval_rewards:  376.24183922594756\n",
      "iteration:  84 \teval_rewards:  452.51840014936806\n",
      "iteration:  85 \teval_rewards:  455.0199362401184\n",
      "iteration:  86 \teval_rewards:  409.90359612114696\n",
      "iteration:  87 \teval_rewards:  412.45865282359483\n",
      "iteration:  88 \teval_rewards:  333.1842048785958\n",
      "iteration:  89 \teval_rewards:  224.24783727986033\n",
      "iteration:  90 \teval_rewards:  522.2913366766113\n",
      "iteration:  91 \teval_rewards:  464.9580982993599\n",
      "iteration:  92 \teval_rewards:  443.9528705978266\n",
      "iteration:  93 \teval_rewards:  458.0554273471745\n",
      "iteration:  94 \teval_rewards:  500.82146425340466\n",
      "iteration:  95 \teval_rewards:  443.85410357214647\n",
      "iteration:  96 \teval_rewards:  403.3360267879232\n",
      "iteration:  97 \teval_rewards:  365.90700766437493\n",
      "iteration:  98 \teval_rewards:  483.2692450243112\n",
      "iteration:  99 \teval_rewards:  433.0498108170547\n",
      "Iter:100| Ep_Reward:396.552| Running_reward:235.732| Actor_Loss:-0.156| Critic_Loss:10.738| Iter_duration:3.169| lr:[0.00028]\n",
      "iteration:  100 \teval_rewards:  396.5521111200651\n",
      "iteration:  101 \teval_rewards:  417.6713996895905\n",
      "iteration:  102 \teval_rewards:  490.9627280276418\n",
      "iteration:  103 \teval_rewards:  341.70896034568244\n",
      "iteration:  104 \teval_rewards:  453.5898046065161\n",
      "iteration:  105 \teval_rewards:  497.64530129710784\n",
      "iteration:  106 \teval_rewards:  554.2823772333034\n",
      "iteration:  107 \teval_rewards:  534.1963435823847\n",
      "iteration:  108 \teval_rewards:  498.495000791943\n",
      "iteration:  109 \teval_rewards:  519.7387215057364\n",
      "iteration:  110 \teval_rewards:  434.097142688096\n",
      "iteration:  111 \teval_rewards:  555.9378364127708\n",
      "iteration:  112 \teval_rewards:  492.00208011164324\n",
      "iteration:  113 \teval_rewards:  429.6317175638894\n",
      "iteration:  114 \teval_rewards:  546.5674302016367\n",
      "iteration:  115 \teval_rewards:  696.2436695554828\n",
      "iteration:  116 \teval_rewards:  561.6923251862275\n",
      "iteration:  117 \teval_rewards:  534.9991540387811\n",
      "iteration:  118 \teval_rewards:  643.9005925711871\n",
      "iteration:  119 \teval_rewards:  453.20816575617454\n",
      "iteration:  120 \teval_rewards:  1347.7433640295067\n",
      "iteration:  121 \teval_rewards:  698.3207692426327\n",
      "iteration:  122 \teval_rewards:  649.9613060866725\n",
      "iteration:  123 \teval_rewards:  652.8115646623563\n",
      "iteration:  124 \teval_rewards:  562.6173586713331\n",
      "iteration:  125 \teval_rewards:  606.701501351786\n",
      "iteration:  126 \teval_rewards:  527.2836441448703\n",
      "iteration:  127 \teval_rewards:  568.6497324257176\n",
      "iteration:  128 \teval_rewards:  542.8459197686998\n",
      "iteration:  129 \teval_rewards:  857.1354435039602\n",
      "iteration:  130 \teval_rewards:  121.79630654377911\n",
      "iteration:  131 \teval_rewards:  1288.0356246108227\n",
      "iteration:  132 \teval_rewards:  511.9075478241832\n",
      "iteration:  133 \teval_rewards:  624.5151792098001\n",
      "iteration:  134 \teval_rewards:  343.69449313087887\n",
      "iteration:  135 \teval_rewards:  574.359194154154\n",
      "iteration:  136 \teval_rewards:  646.6026984728007\n",
      "iteration:  137 \teval_rewards:  294.4978326381444\n",
      "iteration:  138 \teval_rewards:  1467.0568150889474\n",
      "iteration:  139 \teval_rewards:  117.09991925190705\n",
      "iteration:  140 \teval_rewards:  435.51005669789055\n",
      "iteration:  141 \teval_rewards:  605.5076587726687\n",
      "iteration:  142 \teval_rewards:  565.177573696768\n",
      "iteration:  143 \teval_rewards:  631.2395505069435\n",
      "iteration:  144 \teval_rewards:  714.2241284826652\n",
      "iteration:  145 \teval_rewards:  532.463242666546\n",
      "iteration:  146 \teval_rewards:  760.7851556900689\n",
      "iteration:  147 \teval_rewards:  614.3490305296282\n",
      "iteration:  148 \teval_rewards:  716.2750111306282\n",
      "iteration:  149 \teval_rewards:  553.5487836007885\n",
      "iteration:  150 \teval_rewards:  514.6030661966337\n",
      "iteration:  151 \teval_rewards:  644.6538613384864\n",
      "iteration:  152 \teval_rewards:  634.6104564097416\n",
      "iteration:  153 \teval_rewards:  615.5302833760343\n",
      "iteration:  154 \teval_rewards:  482.5230357558021\n",
      "iteration:  155 \teval_rewards:  548.2218643849773\n",
      "iteration:  156 \teval_rewards:  655.1106433851832\n",
      "iteration:  157 \teval_rewards:  700.5153024977085\n",
      "iteration:  158 \teval_rewards:  412.04995358684437\n",
      "iteration:  159 \teval_rewards:  862.1369856585155\n",
      "iteration:  160 \teval_rewards:  669.994692081618\n",
      "iteration:  161 \teval_rewards:  921.2295594226193\n",
      "iteration:  162 \teval_rewards:  653.906974086118\n",
      "iteration:  163 \teval_rewards:  888.9483645152048\n",
      "iteration:  164 \teval_rewards:  486.75770694428275\n",
      "iteration:  165 \teval_rewards:  1015.5358849543604\n",
      "iteration:  166 \teval_rewards:  546.9023407497731\n",
      "iteration:  167 \teval_rewards:  676.1004398603636\n",
      "iteration:  168 \teval_rewards:  643.6662969409498\n",
      "iteration:  169 \teval_rewards:  1182.8665920716717\n",
      "iteration:  170 \teval_rewards:  695.2769996475423\n",
      "iteration:  171 \teval_rewards:  804.0799886059275\n",
      "iteration:  172 \teval_rewards:  1761.5279231806273\n",
      "iteration:  173 \teval_rewards:  995.0857910712595\n",
      "iteration:  174 \teval_rewards:  2142.5040084635475\n",
      "iteration:  175 \teval_rewards:  527.7998313297564\n",
      "iteration:  176 \teval_rewards:  1566.2782646551846\n",
      "iteration:  177 \teval_rewards:  807.0183580925667\n",
      "iteration:  178 \teval_rewards:  544.0792757393914\n",
      "iteration:  179 \teval_rewards:  632.9602496972141\n",
      "iteration:  180 \teval_rewards:  802.0993088228045\n",
      "iteration:  181 \teval_rewards:  497.9137518096788\n",
      "iteration:  182 \teval_rewards:  857.392931675635\n",
      "iteration:  183 \teval_rewards:  531.7404465263121\n",
      "iteration:  184 \teval_rewards:  744.1995553222857\n",
      "iteration:  185 \teval_rewards:  378.2446700245827\n",
      "iteration:  186 \teval_rewards:  884.0758180161774\n",
      "iteration:  187 \teval_rewards:  811.9287591111594\n",
      "iteration:  188 \teval_rewards:  646.6309777422708\n",
      "iteration:  189 \teval_rewards:  489.5928869581648\n",
      "iteration:  190 \teval_rewards:  729.9695889433043\n",
      "iteration:  191 \teval_rewards:  480.99203544375405\n",
      "iteration:  192 \teval_rewards:  533.3402530188833\n",
      "iteration:  193 \teval_rewards:  706.0175895723726\n",
      "iteration:  194 \teval_rewards:  489.20878918487597\n",
      "iteration:  195 \teval_rewards:  627.735137216927\n",
      "iteration:  196 \teval_rewards:  534.6595079688465\n",
      "iteration:  197 \teval_rewards:  933.8070453488897\n",
      "iteration:  198 \teval_rewards:  922.922672974753\n",
      "iteration:  199 \teval_rewards:  741.0464035388676\n",
      "Iter:200| Ep_Reward:742.490| Running_reward:525.643| Actor_Loss:-0.163| Critic_Loss:54.673| Iter_duration:3.220| lr:[0.00026]\n",
      "iteration:  200 \teval_rewards:  742.4896288691257\n",
      "iteration:  201 \teval_rewards:  704.5348717736215\n",
      "iteration:  202 \teval_rewards:  583.9011797555761\n",
      "iteration:  203 \teval_rewards:  429.37107556462365\n",
      "iteration:  204 \teval_rewards:  651.0377815252061\n",
      "iteration:  205 \teval_rewards:  829.020725256228\n",
      "iteration:  206 \teval_rewards:  713.9502600247063\n",
      "iteration:  207 \teval_rewards:  977.4183065559819\n",
      "iteration:  208 \teval_rewards:  739.4041098947205\n",
      "iteration:  209 \teval_rewards:  590.1448116899422\n",
      "iteration:  210 \teval_rewards:  1884.4753333334916\n",
      "iteration:  211 \teval_rewards:  581.8981442056106\n",
      "iteration:  212 \teval_rewards:  1285.9039247751332\n",
      "iteration:  213 \teval_rewards:  319.33790016579985\n",
      "iteration:  214 \teval_rewards:  1587.638450654253\n",
      "iteration:  215 \teval_rewards:  973.8315849377432\n",
      "iteration:  216 \teval_rewards:  695.5679850446606\n",
      "iteration:  217 \teval_rewards:  1058.9336194934835\n",
      "iteration:  218 \teval_rewards:  1300.0958644986988\n",
      "iteration:  219 \teval_rewards:  847.4030255125813\n",
      "iteration:  220 \teval_rewards:  623.8488123160914\n",
      "iteration:  221 \teval_rewards:  633.5386392170792\n",
      "iteration:  222 \teval_rewards:  856.190060039549\n",
      "iteration:  223 \teval_rewards:  961.2209188799535\n",
      "iteration:  224 \teval_rewards:  1074.0171347287533\n",
      "iteration:  225 \teval_rewards:  756.2519365443942\n",
      "iteration:  226 \teval_rewards:  787.2425502017803\n",
      "iteration:  227 \teval_rewards:  534.5056409670007\n",
      "iteration:  228 \teval_rewards:  618.0508472874434\n",
      "iteration:  229 \teval_rewards:  436.64748025848\n",
      "iteration:  230 \teval_rewards:  2398.830102836111\n",
      "iteration:  231 \teval_rewards:  825.5649254861476\n",
      "iteration:  232 \teval_rewards:  1879.248731389248\n",
      "iteration:  233 \teval_rewards:  1025.3507914455258\n",
      "iteration:  234 \teval_rewards:  1145.2600269070347\n",
      "iteration:  235 \teval_rewards:  640.1499016550423\n",
      "iteration:  236 \teval_rewards:  452.7264060204373\n",
      "iteration:  237 \teval_rewards:  1474.9141203554066\n",
      "iteration:  238 \teval_rewards:  589.8137590192491\n",
      "iteration:  239 \teval_rewards:  1191.904069680011\n",
      "iteration:  240 \teval_rewards:  964.3747561005822\n",
      "iteration:  241 \teval_rewards:  460.69808822570513\n",
      "iteration:  242 \teval_rewards:  757.2448575388548\n",
      "iteration:  243 \teval_rewards:  626.3216623050537\n",
      "iteration:  244 \teval_rewards:  605.5284821199465\n",
      "iteration:  245 \teval_rewards:  547.2361977350338\n",
      "iteration:  246 \teval_rewards:  1025.7920687296644\n",
      "iteration:  247 \teval_rewards:  663.4245283399033\n",
      "iteration:  248 \teval_rewards:  471.7039817682362\n",
      "iteration:  249 \teval_rewards:  451.47789632990134\n",
      "iteration:  250 \teval_rewards:  456.68621754865984\n",
      "iteration:  251 \teval_rewards:  632.3339710066049\n",
      "iteration:  252 \teval_rewards:  340.23233842853915\n",
      "iteration:  253 \teval_rewards:  622.7219712194136\n",
      "iteration:  254 \teval_rewards:  611.9398143326507\n",
      "iteration:  255 \teval_rewards:  562.1931898234989\n",
      "iteration:  256 \teval_rewards:  1116.70071356647\n",
      "iteration:  257 \teval_rewards:  462.0069088321889\n",
      "iteration:  258 \teval_rewards:  521.7816638719002\n",
      "iteration:  259 \teval_rewards:  725.5802469216246\n",
      "iteration:  260 \teval_rewards:  583.652065362437\n",
      "iteration:  261 \teval_rewards:  669.8417354940265\n",
      "iteration:  262 \teval_rewards:  1035.162840148211\n",
      "iteration:  263 \teval_rewards:  930.4000482451462\n",
      "iteration:  264 \teval_rewards:  892.2537466275029\n",
      "iteration:  265 \teval_rewards:  1442.3399813034678\n",
      "iteration:  266 \teval_rewards:  725.7030850001476\n",
      "iteration:  267 \teval_rewards:  1058.0563966965635\n",
      "iteration:  268 \teval_rewards:  722.5314086621322\n",
      "iteration:  269 \teval_rewards:  671.393171774189\n",
      "iteration:  270 \teval_rewards:  1285.775538999197\n",
      "iteration:  271 \teval_rewards:  1242.4579424726705\n",
      "iteration:  272 \teval_rewards:  666.9794373696002\n",
      "iteration:  273 \teval_rewards:  1073.151015375226\n",
      "iteration:  274 \teval_rewards:  2454.659771767408\n",
      "iteration:  275 \teval_rewards:  649.9903714875549\n",
      "iteration:  276 \teval_rewards:  1015.5072114822627\n",
      "iteration:  277 \teval_rewards:  1229.5945242035757\n",
      "iteration:  278 \teval_rewards:  2575.336843734838\n",
      "iteration:  279 \teval_rewards:  612.1575297533191\n",
      "iteration:  280 \teval_rewards:  822.8572687733301\n",
      "iteration:  281 \teval_rewards:  2130.1915270641875\n",
      "iteration:  282 \teval_rewards:  421.6969475418557\n",
      "iteration:  283 \teval_rewards:  2560.8595244891794\n",
      "iteration:  284 \teval_rewards:  646.420063637426\n",
      "iteration:  285 \teval_rewards:  675.075446187086\n",
      "iteration:  286 \teval_rewards:  768.982096900053\n",
      "iteration:  287 \teval_rewards:  695.9838364524206\n",
      "iteration:  288 \teval_rewards:  1118.002260611985\n",
      "iteration:  289 \teval_rewards:  1390.8143689791311\n",
      "iteration:  290 \teval_rewards:  888.0632473577779\n",
      "iteration:  291 \teval_rewards:  1027.5776332146543\n",
      "iteration:  292 \teval_rewards:  824.9727478995296\n",
      "iteration:  293 \teval_rewards:  2339.818614041663\n",
      "iteration:  294 \teval_rewards:  945.8787093812982\n",
      "iteration:  295 \teval_rewards:  1436.1034407217703\n",
      "iteration:  296 \teval_rewards:  659.1835723065308\n",
      "iteration:  297 \teval_rewards:  389.8822386140709\n",
      "iteration:  298 \teval_rewards:  2473.6951135178288\n",
      "iteration:  299 \teval_rewards:  671.5058134637059\n",
      "Iter:300| Ep_Reward:1277.397| Running_reward:810.238| Actor_Loss:-0.020| Critic_Loss:75.179| Iter_duration:3.425| lr:[0.00023999999999999998]\n",
      "iteration:  300 \teval_rewards:  1277.3973316320894\n",
      "iteration:  301 \teval_rewards:  1407.8545465240968\n",
      "iteration:  302 \teval_rewards:  778.3513161612782\n",
      "iteration:  303 \teval_rewards:  1210.8556553960725\n",
      "iteration:  304 \teval_rewards:  671.1060820567247\n",
      "iteration:  305 \teval_rewards:  715.9886337354861\n",
      "iteration:  306 \teval_rewards:  959.3504086310505\n",
      "iteration:  307 \teval_rewards:  1443.4526595340471\n",
      "iteration:  308 \teval_rewards:  1159.5316037092593\n",
      "iteration:  309 \teval_rewards:  1411.063794065072\n",
      "iteration:  310 \teval_rewards:  670.6067902817545\n",
      "iteration:  311 \teval_rewards:  489.61890252830074\n",
      "iteration:  312 \teval_rewards:  598.7641008767537\n",
      "iteration:  313 \teval_rewards:  479.2293152372152\n",
      "iteration:  314 \teval_rewards:  673.747304686357\n",
      "iteration:  315 \teval_rewards:  670.8856714100133\n",
      "iteration:  316 \teval_rewards:  450.32807965110504\n",
      "iteration:  317 \teval_rewards:  681.6949885720446\n",
      "iteration:  318 \teval_rewards:  434.2592058656319\n",
      "iteration:  319 \teval_rewards:  1570.8535429173023\n",
      "iteration:  320 \teval_rewards:  1422.9879440295254\n",
      "iteration:  321 \teval_rewards:  1183.2623901836514\n",
      "iteration:  322 \teval_rewards:  633.741767993915\n",
      "iteration:  323 \teval_rewards:  707.1715396556481\n",
      "iteration:  324 \teval_rewards:  1371.4301789346002\n",
      "iteration:  325 \teval_rewards:  1499.7494250711206\n",
      "iteration:  326 \teval_rewards:  979.1115650947456\n",
      "iteration:  327 \teval_rewards:  602.6474870994948\n",
      "iteration:  328 \teval_rewards:  886.0581047335784\n",
      "iteration:  329 \teval_rewards:  532.251519543819\n",
      "iteration:  330 \teval_rewards:  789.2531662643623\n",
      "iteration:  331 \teval_rewards:  790.8145967565578\n",
      "iteration:  332 \teval_rewards:  730.5871386352387\n",
      "iteration:  333 \teval_rewards:  948.4826599557485\n",
      "iteration:  334 \teval_rewards:  709.9121013764268\n",
      "iteration:  335 \teval_rewards:  1686.0983302527643\n",
      "iteration:  336 \teval_rewards:  913.8888129302575\n",
      "iteration:  337 \teval_rewards:  653.7435455705041\n",
      "iteration:  338 \teval_rewards:  724.1062463242588\n",
      "iteration:  339 \teval_rewards:  851.0995302574935\n",
      "iteration:  340 \teval_rewards:  507.28224071824934\n",
      "iteration:  341 \teval_rewards:  554.9228417178203\n",
      "iteration:  342 \teval_rewards:  645.4170044672135\n",
      "iteration:  343 \teval_rewards:  552.4741163862869\n",
      "iteration:  344 \teval_rewards:  1290.2075232919847\n",
      "iteration:  345 \teval_rewards:  715.2732727733832\n",
      "iteration:  346 \teval_rewards:  801.9258447116594\n",
      "iteration:  347 \teval_rewards:  1414.4849400945195\n",
      "iteration:  348 \teval_rewards:  1419.6889045336904\n",
      "iteration:  349 \teval_rewards:  517.0314688212463\n",
      "iteration:  350 \teval_rewards:  439.2547357903258\n",
      "iteration:  351 \teval_rewards:  1186.8127074817767\n",
      "iteration:  352 \teval_rewards:  847.4973245207817\n",
      "iteration:  353 \teval_rewards:  694.0353076959466\n",
      "iteration:  354 \teval_rewards:  1022.9409595072821\n",
      "iteration:  355 \teval_rewards:  590.3712128179948\n",
      "iteration:  356 \teval_rewards:  2773.1478307594425\n",
      "iteration:  357 \teval_rewards:  725.3040164243466\n",
      "iteration:  358 \teval_rewards:  522.1960074454255\n",
      "iteration:  359 \teval_rewards:  931.3607171228308\n",
      "iteration:  360 \teval_rewards:  1948.1171574891093\n",
      "iteration:  361 \teval_rewards:  825.2091975772947\n",
      "iteration:  362 \teval_rewards:  1319.31217427011\n",
      "iteration:  363 \teval_rewards:  607.8258399915321\n",
      "iteration:  364 \teval_rewards:  1148.8822074906404\n",
      "iteration:  365 \teval_rewards:  912.653996518735\n",
      "iteration:  366 \teval_rewards:  717.102408388259\n",
      "iteration:  367 \teval_rewards:  1458.3756725431126\n",
      "iteration:  368 \teval_rewards:  1323.9418906248861\n",
      "iteration:  369 \teval_rewards:  674.5897422848043\n",
      "iteration:  370 \teval_rewards:  1931.4365956111403\n",
      "iteration:  371 \teval_rewards:  675.222929433089\n",
      "iteration:  372 \teval_rewards:  717.5955242761303\n",
      "iteration:  373 \teval_rewards:  840.9550457895772\n",
      "iteration:  374 \teval_rewards:  697.4493755369583\n",
      "iteration:  375 \teval_rewards:  340.87699101504523\n",
      "iteration:  376 \teval_rewards:  805.8949053539404\n",
      "iteration:  377 \teval_rewards:  703.6090864846503\n",
      "iteration:  378 \teval_rewards:  2088.1223554889893\n",
      "iteration:  379 \teval_rewards:  1839.0156354983221\n",
      "iteration:  380 \teval_rewards:  763.4259231126191\n",
      "iteration:  381 \teval_rewards:  885.2108335366452\n",
      "iteration:  382 \teval_rewards:  1258.3436766732473\n",
      "iteration:  383 \teval_rewards:  743.8104995492567\n",
      "iteration:  384 \teval_rewards:  898.5920481675715\n",
      "iteration:  385 \teval_rewards:  524.6413586136067\n",
      "iteration:  386 \teval_rewards:  528.2275758430955\n",
      "iteration:  387 \teval_rewards:  735.0293541978955\n",
      "iteration:  388 \teval_rewards:  1047.1454423790674\n",
      "iteration:  389 \teval_rewards:  782.995923540857\n",
      "iteration:  390 \teval_rewards:  876.0489172107274\n",
      "iteration:  391 \teval_rewards:  684.6199849480715\n",
      "iteration:  392 \teval_rewards:  1427.6227311082937\n",
      "iteration:  393 \teval_rewards:  1137.4005454707446\n",
      "iteration:  394 \teval_rewards:  1271.6473879510252\n",
      "iteration:  395 \teval_rewards:  1746.9023997762054\n",
      "iteration:  396 \teval_rewards:  717.9065484139666\n",
      "iteration:  397 \teval_rewards:  1401.1732009756388\n",
      "iteration:  398 \teval_rewards:  861.6642858876208\n",
      "iteration:  399 \teval_rewards:  1004.6468217306875\n",
      "Iter:400| Ep_Reward:917.915| Running_reward:907.981| Actor_Loss:0.015| Critic_Loss:408.129| Iter_duration:3.337| lr:[0.00022]\n",
      "iteration:  400 \teval_rewards:  917.9145852733943\n",
      "iteration:  401 \teval_rewards:  877.556836065641\n",
      "iteration:  402 \teval_rewards:  692.6590206673385\n",
      "iteration:  403 \teval_rewards:  1245.7681207459682\n",
      "iteration:  404 \teval_rewards:  1079.5627841719077\n",
      "iteration:  405 \teval_rewards:  828.7654097194028\n",
      "iteration:  406 \teval_rewards:  1001.1935117498367\n",
      "iteration:  407 \teval_rewards:  1192.087683934246\n",
      "iteration:  408 \teval_rewards:  919.0390157174288\n",
      "iteration:  409 \teval_rewards:  1536.6181609838393\n",
      "iteration:  410 \teval_rewards:  1051.6691626873867\n",
      "iteration:  411 \teval_rewards:  679.2715779459187\n",
      "iteration:  412 \teval_rewards:  1993.7206574752752\n",
      "iteration:  413 \teval_rewards:  1163.7238798336812\n",
      "iteration:  414 \teval_rewards:  743.1004917267444\n",
      "iteration:  415 \teval_rewards:  742.0513552634353\n",
      "iteration:  416 \teval_rewards:  1018.8710048134443\n",
      "iteration:  417 \teval_rewards:  688.2626705109719\n",
      "iteration:  418 \teval_rewards:  606.2833829664962\n",
      "iteration:  419 \teval_rewards:  711.8974298599985\n",
      "iteration:  420 \teval_rewards:  386.87829445963933\n",
      "iteration:  421 \teval_rewards:  652.3401901388612\n",
      "iteration:  422 \teval_rewards:  732.6055815649408\n",
      "iteration:  423 \teval_rewards:  704.0858992722162\n",
      "iteration:  424 \teval_rewards:  2075.0223009161928\n",
      "iteration:  425 \teval_rewards:  2474.481520587572\n",
      "iteration:  426 \teval_rewards:  2756.711249903717\n",
      "iteration:  427 \teval_rewards:  758.5513869615289\n",
      "iteration:  428 \teval_rewards:  945.8987483858134\n",
      "iteration:  429 \teval_rewards:  1202.2937064678522\n",
      "iteration:  430 \teval_rewards:  1202.3811179540305\n",
      "iteration:  431 \teval_rewards:  706.7327113880682\n",
      "iteration:  432 \teval_rewards:  707.5026291619744\n",
      "iteration:  433 \teval_rewards:  2667.767768225849\n",
      "iteration:  434 \teval_rewards:  447.0381990045066\n",
      "iteration:  435 \teval_rewards:  663.9180950983465\n",
      "iteration:  436 \teval_rewards:  1585.6056428122367\n",
      "iteration:  437 \teval_rewards:  769.134139291551\n",
      "iteration:  438 \teval_rewards:  928.4575627414462\n",
      "iteration:  439 \teval_rewards:  1747.9361014154736\n",
      "iteration:  440 \teval_rewards:  1332.6038526882062\n",
      "iteration:  441 \teval_rewards:  813.2294731132265\n",
      "iteration:  442 \teval_rewards:  1344.456606650433\n",
      "iteration:  443 \teval_rewards:  729.8093905951913\n",
      "iteration:  444 \teval_rewards:  816.5761806441711\n",
      "iteration:  445 \teval_rewards:  783.6375346667107\n",
      "iteration:  446 \teval_rewards:  995.9449079237386\n",
      "iteration:  447 \teval_rewards:  1681.0103900623274\n",
      "iteration:  448 \teval_rewards:  519.3062556732905\n",
      "iteration:  449 \teval_rewards:  1201.2230091713122\n",
      "iteration:  450 \teval_rewards:  578.4083590540021\n",
      "iteration:  451 \teval_rewards:  1905.731750272864\n",
      "iteration:  452 \teval_rewards:  2563.9565614774856\n",
      "iteration:  453 \teval_rewards:  861.9017572858962\n",
      "iteration:  454 \teval_rewards:  661.0921882690745\n",
      "iteration:  455 \teval_rewards:  1616.945910034345\n",
      "iteration:  456 \teval_rewards:  1775.6082923915676\n",
      "iteration:  457 \teval_rewards:  2406.471253254748\n",
      "iteration:  458 \teval_rewards:  931.8056294011433\n",
      "iteration:  459 \teval_rewards:  2351.3583558851005\n",
      "iteration:  460 \teval_rewards:  820.7428945680028\n",
      "iteration:  461 \teval_rewards:  2420.3986175440527\n",
      "iteration:  462 \teval_rewards:  729.2372272533268\n",
      "iteration:  463 \teval_rewards:  687.0465463308716\n",
      "iteration:  464 \teval_rewards:  796.9189359063207\n",
      "iteration:  465 \teval_rewards:  539.9036306901718\n",
      "iteration:  466 \teval_rewards:  1871.359887825797\n",
      "iteration:  467 \teval_rewards:  669.8433130773726\n",
      "iteration:  468 \teval_rewards:  721.1206762548761\n",
      "iteration:  469 \teval_rewards:  707.6412718408914\n",
      "iteration:  470 \teval_rewards:  770.3334764807419\n",
      "iteration:  471 \teval_rewards:  460.84920717013495\n",
      "iteration:  472 \teval_rewards:  993.3776082252672\n",
      "iteration:  473 \teval_rewards:  551.3493315627005\n",
      "iteration:  474 \teval_rewards:  889.2625386354284\n",
      "iteration:  475 \teval_rewards:  2149.967586278895\n",
      "iteration:  476 \teval_rewards:  1338.2087586606733\n",
      "iteration:  477 \teval_rewards:  774.0694751684729\n",
      "iteration:  478 \teval_rewards:  503.3509535223948\n",
      "iteration:  479 \teval_rewards:  700.7700924827484\n",
      "iteration:  480 \teval_rewards:  834.3700993921316\n",
      "iteration:  481 \teval_rewards:  624.2240465079192\n",
      "iteration:  482 \teval_rewards:  647.0222968909992\n",
      "iteration:  483 \teval_rewards:  1114.5742233184785\n",
      "iteration:  484 \teval_rewards:  2772.5330016801204\n",
      "iteration:  485 \teval_rewards:  1366.6079812292865\n",
      "iteration:  486 \teval_rewards:  1373.367843586199\n",
      "iteration:  487 \teval_rewards:  656.9415617521548\n",
      "iteration:  488 \teval_rewards:  959.3114813546408\n",
      "iteration:  489 \teval_rewards:  2073.1990802598734\n",
      "iteration:  490 \teval_rewards:  1337.5094362200318\n",
      "iteration:  491 \teval_rewards:  631.4912604837631\n",
      "iteration:  492 \teval_rewards:  830.0482123801143\n",
      "iteration:  493 \teval_rewards:  707.9863908070498\n",
      "iteration:  494 \teval_rewards:  957.9265810700514\n",
      "iteration:  495 \teval_rewards:  1449.6795479068483\n",
      "iteration:  496 \teval_rewards:  1541.1122517394351\n",
      "iteration:  497 \teval_rewards:  2499.4234322375473\n",
      "iteration:  498 \teval_rewards:  1955.2041051581\n",
      "iteration:  499 \teval_rewards:  2476.883586202698\n",
      "Iter:500| Ep_Reward:1213.307| Running_reward:1082.151| Actor_Loss:0.020| Critic_Loss:109.441| Iter_duration:3.325| lr:[0.0002]\n",
      "iteration:  500 \teval_rewards:  1213.3069339481299\n",
      "iteration:  501 \teval_rewards:  512.9973448311611\n",
      "iteration:  502 \teval_rewards:  1120.7950647755526\n",
      "iteration:  503 \teval_rewards:  2231.911741214734\n",
      "iteration:  504 \teval_rewards:  948.793235237021\n",
      "iteration:  505 \teval_rewards:  745.3497456155488\n",
      "iteration:  506 \teval_rewards:  608.2316181206204\n",
      "iteration:  507 \teval_rewards:  747.4008528389311\n",
      "iteration:  508 \teval_rewards:  1466.8661243253412\n",
      "iteration:  509 \teval_rewards:  1302.6658031724344\n",
      "iteration:  510 \teval_rewards:  705.4787797039694\n",
      "iteration:  511 \teval_rewards:  843.9858731209181\n",
      "iteration:  512 \teval_rewards:  1276.068316539867\n",
      "iteration:  513 \teval_rewards:  1348.2755985790025\n",
      "iteration:  514 \teval_rewards:  532.7002191520971\n",
      "iteration:  515 \teval_rewards:  1041.161594510867\n",
      "iteration:  516 \teval_rewards:  688.053494519733\n",
      "iteration:  517 \teval_rewards:  2510.431967286535\n",
      "iteration:  518 \teval_rewards:  507.41643168047113\n",
      "iteration:  519 \teval_rewards:  615.255305249326\n",
      "iteration:  520 \teval_rewards:  480.8984436250658\n",
      "iteration:  521 \teval_rewards:  579.1567159157795\n",
      "iteration:  522 \teval_rewards:  1966.3524430691086\n",
      "iteration:  523 \teval_rewards:  290.7102615321167\n",
      "iteration:  524 \teval_rewards:  716.0525359794183\n",
      "iteration:  525 \teval_rewards:  491.0281076172615\n",
      "iteration:  526 \teval_rewards:  1769.7589071147577\n",
      "iteration:  527 \teval_rewards:  486.9445666892269\n",
      "iteration:  528 \teval_rewards:  907.0587850274114\n",
      "iteration:  529 \teval_rewards:  1855.4020793155223\n",
      "iteration:  530 \teval_rewards:  2566.825329795175\n",
      "iteration:  531 \teval_rewards:  1127.2261591348274\n",
      "iteration:  532 \teval_rewards:  1746.1682615306536\n",
      "iteration:  533 \teval_rewards:  480.9468155765919\n",
      "iteration:  534 \teval_rewards:  1173.7096461624542\n",
      "iteration:  535 \teval_rewards:  1803.371652193803\n",
      "iteration:  536 \teval_rewards:  1084.2756908992633\n",
      "iteration:  537 \teval_rewards:  191.33844865262904\n",
      "iteration:  538 \teval_rewards:  800.352128366024\n",
      "iteration:  539 \teval_rewards:  2538.2407678802933\n",
      "iteration:  540 \teval_rewards:  670.5532438248975\n",
      "iteration:  541 \teval_rewards:  2469.299712532214\n",
      "iteration:  542 \teval_rewards:  556.0427178023327\n",
      "iteration:  543 \teval_rewards:  1915.4206550527679\n",
      "iteration:  544 \teval_rewards:  608.2301267738242\n",
      "iteration:  545 \teval_rewards:  2664.468052039496\n",
      "iteration:  546 \teval_rewards:  585.7991625055698\n",
      "iteration:  547 \teval_rewards:  494.50650541473817\n",
      "iteration:  548 \teval_rewards:  539.1307796877311\n",
      "iteration:  549 \teval_rewards:  1812.792181358428\n",
      "iteration:  550 \teval_rewards:  2529.39887130253\n",
      "iteration:  551 \teval_rewards:  523.4963482297665\n",
      "iteration:  552 \teval_rewards:  1000.6171287862429\n",
      "iteration:  553 \teval_rewards:  1681.3241234572013\n",
      "iteration:  554 \teval_rewards:  1206.5417252498157\n",
      "iteration:  555 \teval_rewards:  1105.9872357289664\n",
      "iteration:  556 \teval_rewards:  524.7772293187302\n",
      "iteration:  557 \teval_rewards:  1578.9711829097607\n",
      "iteration:  558 \teval_rewards:  1418.0428629158082\n",
      "iteration:  559 \teval_rewards:  1804.209036551697\n",
      "iteration:  560 \teval_rewards:  625.0707024286287\n",
      "iteration:  561 \teval_rewards:  1247.4441868705158\n",
      "iteration:  562 \teval_rewards:  950.2229090934381\n",
      "iteration:  563 \teval_rewards:  1597.505862897453\n",
      "iteration:  564 \teval_rewards:  430.95615215508326\n",
      "iteration:  565 \teval_rewards:  1282.1407353457141\n",
      "iteration:  566 \teval_rewards:  1921.5049450206216\n",
      "iteration:  567 \teval_rewards:  973.5834937145016\n",
      "iteration:  568 \teval_rewards:  1449.081020473523\n",
      "iteration:  569 \teval_rewards:  233.5116709863598\n",
      "iteration:  570 \teval_rewards:  2747.582056636369\n",
      "iteration:  571 \teval_rewards:  1297.6941813098947\n",
      "iteration:  572 \teval_rewards:  547.9713623551876\n",
      "iteration:  573 \teval_rewards:  1460.2397555802872\n",
      "iteration:  574 \teval_rewards:  211.11879819642587\n",
      "iteration:  575 \teval_rewards:  469.6412756250097\n",
      "iteration:  576 \teval_rewards:  2482.690228037653\n",
      "iteration:  577 \teval_rewards:  475.6420059007544\n",
      "iteration:  578 \teval_rewards:  1070.002017027572\n",
      "iteration:  579 \teval_rewards:  2534.0034758792385\n",
      "iteration:  580 \teval_rewards:  801.0589670519445\n",
      "iteration:  581 \teval_rewards:  814.4445821509279\n",
      "iteration:  582 \teval_rewards:  2683.08884004033\n",
      "iteration:  583 \teval_rewards:  1239.3204898803053\n",
      "iteration:  584 \teval_rewards:  2609.4522293346918\n",
      "iteration:  585 \teval_rewards:  515.1404465918167\n",
      "iteration:  586 \teval_rewards:  405.2042392213821\n",
      "iteration:  587 \teval_rewards:  1050.5857887445366\n",
      "iteration:  588 \teval_rewards:  516.2811672199608\n",
      "iteration:  589 \teval_rewards:  472.08254951767907\n",
      "iteration:  590 \teval_rewards:  1879.3909631044614\n",
      "iteration:  591 \teval_rewards:  2623.4996534687484\n",
      "iteration:  592 \teval_rewards:  526.4228043074984\n",
      "iteration:  593 \teval_rewards:  2563.590915014906\n",
      "iteration:  594 \teval_rewards:  1103.8490107498737\n",
      "iteration:  595 \teval_rewards:  728.5253331389486\n",
      "iteration:  596 \teval_rewards:  889.996102211718\n",
      "iteration:  597 \teval_rewards:  1449.8500302448458\n",
      "iteration:  598 \teval_rewards:  633.6815194266012\n",
      "iteration:  599 \teval_rewards:  1626.852608101855\n",
      "Iter:600| Ep_Reward:1324.397| Running_reward:1163.375| Actor_Loss:0.064| Critic_Loss:157.893| Iter_duration:3.437| lr:[0.00017999999999999998]\n",
      "iteration:  600 \teval_rewards:  1324.3974922542209\n",
      "iteration:  601 \teval_rewards:  2607.4452474668683\n",
      "iteration:  602 \teval_rewards:  2506.3529540102168\n",
      "iteration:  603 \teval_rewards:  2718.530847204045\n",
      "iteration:  604 \teval_rewards:  1596.1988234507317\n",
      "iteration:  605 \teval_rewards:  618.2503618537964\n",
      "iteration:  606 \teval_rewards:  334.39108120600935\n",
      "iteration:  607 \teval_rewards:  2293.8265192594035\n",
      "iteration:  608 \teval_rewards:  1785.7458992696777\n",
      "iteration:  609 \teval_rewards:  748.9032762145991\n",
      "iteration:  610 \teval_rewards:  2631.64756254886\n",
      "iteration:  611 \teval_rewards:  2641.3700032387496\n",
      "iteration:  612 \teval_rewards:  1327.5727243250167\n",
      "iteration:  613 \teval_rewards:  1832.9012140152677\n",
      "iteration:  614 \teval_rewards:  497.8183253565109\n",
      "iteration:  615 \teval_rewards:  573.579931579518\n",
      "iteration:  616 \teval_rewards:  2591.6521207260575\n",
      "iteration:  617 \teval_rewards:  2637.540049859224\n",
      "iteration:  618 \teval_rewards:  676.083658260264\n",
      "iteration:  619 \teval_rewards:  2608.2080082436582\n",
      "iteration:  620 \teval_rewards:  795.7887231258749\n",
      "iteration:  621 \teval_rewards:  1187.309367187951\n",
      "iteration:  622 \teval_rewards:  2690.2815370371686\n",
      "iteration:  623 \teval_rewards:  881.7727604879207\n",
      "iteration:  624 \teval_rewards:  2659.7842630388755\n",
      "iteration:  625 \teval_rewards:  663.915987165416\n",
      "iteration:  626 \teval_rewards:  610.4058932155265\n",
      "iteration:  627 \teval_rewards:  682.1977017652099\n",
      "iteration:  628 \teval_rewards:  802.6535876588486\n",
      "iteration:  629 \teval_rewards:  838.2261797253169\n",
      "iteration:  630 \teval_rewards:  823.4472343938322\n",
      "iteration:  631 \teval_rewards:  1567.9389857207739\n",
      "iteration:  632 \teval_rewards:  1815.1409957413614\n",
      "iteration:  633 \teval_rewards:  2726.159924244382\n",
      "iteration:  634 \teval_rewards:  1939.3053880137743\n",
      "iteration:  635 \teval_rewards:  2573.8472099149076\n",
      "iteration:  636 \teval_rewards:  543.950384069557\n",
      "iteration:  637 \teval_rewards:  1164.107815817255\n",
      "iteration:  638 \teval_rewards:  512.5770794447862\n",
      "iteration:  639 \teval_rewards:  1578.87687140584\n",
      "iteration:  640 \teval_rewards:  1519.2794354847126\n",
      "iteration:  641 \teval_rewards:  1598.9398810048788\n",
      "iteration:  642 \teval_rewards:  329.20334954751786\n",
      "iteration:  643 \teval_rewards:  536.3997899946355\n",
      "iteration:  644 \teval_rewards:  2623.497891435181\n",
      "iteration:  645 \teval_rewards:  2770.6628659599087\n",
      "iteration:  646 \teval_rewards:  1684.8987985676747\n",
      "iteration:  647 \teval_rewards:  2125.7445452666193\n",
      "iteration:  648 \teval_rewards:  2112.7270642334765\n",
      "iteration:  649 \teval_rewards:  1138.6353686476748\n",
      "iteration:  650 \teval_rewards:  525.6522553661557\n",
      "iteration:  651 \teval_rewards:  1985.612493395937\n",
      "iteration:  652 \teval_rewards:  2582.2832149304413\n",
      "iteration:  653 \teval_rewards:  794.2654476342407\n",
      "iteration:  654 \teval_rewards:  1931.3518032204463\n",
      "iteration:  655 \teval_rewards:  1039.4047608042763\n",
      "iteration:  656 \teval_rewards:  2850.1505785430036\n",
      "iteration:  657 \teval_rewards:  747.6339768032875\n",
      "iteration:  658 \teval_rewards:  1443.0226984422939\n",
      "iteration:  659 \teval_rewards:  621.9891145764345\n",
      "iteration:  660 \teval_rewards:  1319.0201600178793\n",
      "iteration:  661 \teval_rewards:  1060.2499978008789\n",
      "iteration:  662 \teval_rewards:  1782.951748165876\n",
      "iteration:  663 \teval_rewards:  1401.0537980224715\n",
      "iteration:  664 \teval_rewards:  381.3271840340836\n",
      "iteration:  665 \teval_rewards:  1357.9034140864485\n",
      "iteration:  666 \teval_rewards:  2376.4137350178507\n",
      "iteration:  667 \teval_rewards:  2748.8839864090837\n",
      "iteration:  668 \teval_rewards:  1721.1039832516487\n",
      "iteration:  669 \teval_rewards:  1850.8001713369479\n",
      "iteration:  670 \teval_rewards:  1338.7969646326908\n",
      "iteration:  671 \teval_rewards:  2121.8222106433345\n",
      "iteration:  672 \teval_rewards:  655.9953723594274\n",
      "iteration:  673 \teval_rewards:  1623.2495417199375\n",
      "iteration:  674 \teval_rewards:  2250.2856583619755\n",
      "iteration:  675 \teval_rewards:  490.78416244087464\n",
      "iteration:  676 \teval_rewards:  545.4318136135316\n",
      "iteration:  677 \teval_rewards:  339.3492781636158\n",
      "iteration:  678 \teval_rewards:  431.7235305371264\n",
      "iteration:  679 \teval_rewards:  591.3434666012573\n",
      "iteration:  680 \teval_rewards:  2805.6740266301686\n",
      "iteration:  681 \teval_rewards:  1087.6288207014284\n",
      "iteration:  682 \teval_rewards:  2863.413576731765\n",
      "iteration:  683 \teval_rewards:  1035.0701958822367\n",
      "iteration:  684 \teval_rewards:  2214.3610275777837\n",
      "iteration:  685 \teval_rewards:  1777.648261173909\n",
      "iteration:  686 \teval_rewards:  2011.45900389174\n",
      "iteration:  687 \teval_rewards:  1092.188579693653\n",
      "iteration:  688 \teval_rewards:  1118.9785223762353\n",
      "iteration:  689 \teval_rewards:  2820.334758163283\n",
      "iteration:  690 \teval_rewards:  2498.540188936028\n",
      "iteration:  691 \teval_rewards:  2591.9049307474147\n",
      "iteration:  692 \teval_rewards:  1141.1684288587012\n",
      "iteration:  693 \teval_rewards:  1388.1020425506285\n",
      "iteration:  694 \teval_rewards:  490.86241074354956\n",
      "iteration:  695 \teval_rewards:  2261.372445022054\n",
      "iteration:  696 \teval_rewards:  1485.5846434144007\n",
      "iteration:  697 \teval_rewards:  944.0518216232097\n",
      "iteration:  698 \teval_rewards:  752.1652662247062\n",
      "iteration:  699 \teval_rewards:  552.399409085668\n",
      "Iter:700| Ep_Reward:486.728| Running_reward:1370.158| Actor_Loss:-0.028| Critic_Loss:112.076| Iter_duration:3.132| lr:[0.00015999999999999999]\n",
      "iteration:  700 \teval_rewards:  486.7278802910111\n",
      "iteration:  701 \teval_rewards:  1922.6274327253989\n",
      "iteration:  702 \teval_rewards:  2727.849387357481\n",
      "iteration:  703 \teval_rewards:  982.1215345312424\n",
      "iteration:  704 \teval_rewards:  2463.6824448754996\n",
      "iteration:  705 \teval_rewards:  2045.3671480989474\n",
      "iteration:  706 \teval_rewards:  311.7412591861553\n",
      "iteration:  707 \teval_rewards:  1169.7256409587117\n",
      "iteration:  708 \teval_rewards:  862.292364428167\n",
      "iteration:  709 \teval_rewards:  920.091987608423\n",
      "iteration:  710 \teval_rewards:  2606.152413364783\n",
      "iteration:  711 \teval_rewards:  2291.979710929352\n",
      "iteration:  712 \teval_rewards:  656.8249048240026\n",
      "iteration:  713 \teval_rewards:  2594.7481329947423\n",
      "iteration:  714 \teval_rewards:  645.1550520880088\n",
      "iteration:  715 \teval_rewards:  436.12651538914037\n",
      "iteration:  716 \teval_rewards:  829.0937590240762\n",
      "iteration:  717 \teval_rewards:  632.6031372711087\n",
      "iteration:  718 \teval_rewards:  1092.2608081731726\n",
      "iteration:  719 \teval_rewards:  2637.0140354665714\n",
      "iteration:  720 \teval_rewards:  2431.656248878565\n",
      "iteration:  721 \teval_rewards:  809.6180101288662\n",
      "iteration:  722 \teval_rewards:  1443.489614493938\n",
      "iteration:  723 \teval_rewards:  1714.916763090177\n",
      "iteration:  724 \teval_rewards:  457.2290670494474\n",
      "iteration:  725 \teval_rewards:  1264.9697321559659\n",
      "iteration:  726 \teval_rewards:  623.3470923382481\n",
      "iteration:  727 \teval_rewards:  1265.5266306648116\n",
      "iteration:  728 \teval_rewards:  1477.0164702074308\n",
      "iteration:  729 \teval_rewards:  1928.8875021308886\n",
      "iteration:  730 \teval_rewards:  2659.8284743006584\n",
      "iteration:  731 \teval_rewards:  768.804613365479\n",
      "iteration:  732 \teval_rewards:  2657.5923832279022\n",
      "iteration:  733 \teval_rewards:  1016.4187840747885\n",
      "iteration:  734 \teval_rewards:  1900.9444751834656\n",
      "iteration:  735 \teval_rewards:  2761.7199650979323\n",
      "iteration:  736 \teval_rewards:  1998.4985990431312\n",
      "iteration:  737 \teval_rewards:  2750.8189587569523\n",
      "iteration:  738 \teval_rewards:  569.0022157300918\n",
      "iteration:  739 \teval_rewards:  724.4817223767628\n",
      "iteration:  740 \teval_rewards:  1218.9801980217017\n",
      "iteration:  741 \teval_rewards:  2397.1390261776837\n",
      "iteration:  742 \teval_rewards:  1564.5743966032314\n",
      "iteration:  743 \teval_rewards:  1622.270160282363\n",
      "iteration:  744 \teval_rewards:  745.5980697301543\n",
      "iteration:  745 \teval_rewards:  1636.2812845347287\n",
      "iteration:  746 \teval_rewards:  2824.975857216035\n",
      "iteration:  747 \teval_rewards:  2649.0525295899547\n",
      "iteration:  748 \teval_rewards:  636.0064208243624\n",
      "iteration:  749 \teval_rewards:  2669.879292897324\n",
      "iteration:  750 \teval_rewards:  2735.8572525765208\n",
      "iteration:  751 \teval_rewards:  2377.470395209872\n",
      "iteration:  752 \teval_rewards:  973.9497606849493\n",
      "iteration:  753 \teval_rewards:  2691.623422128797\n",
      "iteration:  754 \teval_rewards:  2740.605081414652\n",
      "iteration:  755 \teval_rewards:  2692.0576954325584\n",
      "iteration:  756 \teval_rewards:  2712.548958330648\n",
      "iteration:  757 \teval_rewards:  766.2766188445839\n",
      "iteration:  758 \teval_rewards:  1005.5464918673166\n",
      "iteration:  759 \teval_rewards:  514.2521989332016\n",
      "iteration:  760 \teval_rewards:  1367.5336455542013\n",
      "iteration:  761 \teval_rewards:  497.57294714360404\n",
      "iteration:  762 \teval_rewards:  605.7105475116157\n",
      "iteration:  763 \teval_rewards:  1303.3395689527508\n",
      "iteration:  764 \teval_rewards:  1268.5588132328078\n",
      "iteration:  765 \teval_rewards:  697.2602339383496\n",
      "iteration:  766 \teval_rewards:  594.5276616943281\n",
      "iteration:  767 \teval_rewards:  2329.9422864763596\n",
      "iteration:  768 \teval_rewards:  632.1779213724287\n",
      "iteration:  769 \teval_rewards:  817.5296642694344\n",
      "iteration:  770 \teval_rewards:  2581.0448406850983\n",
      "iteration:  771 \teval_rewards:  439.6283053027793\n",
      "iteration:  772 \teval_rewards:  461.4955678142418\n",
      "iteration:  773 \teval_rewards:  2016.4915295824892\n",
      "iteration:  774 \teval_rewards:  611.9869907806554\n",
      "iteration:  775 \teval_rewards:  633.6942426812702\n",
      "iteration:  776 \teval_rewards:  2807.662121659918\n",
      "iteration:  777 \teval_rewards:  2355.402376329434\n",
      "iteration:  778 \teval_rewards:  505.2779751331835\n",
      "iteration:  779 \teval_rewards:  977.699914202857\n",
      "iteration:  780 \teval_rewards:  1585.5051473775502\n",
      "iteration:  781 \teval_rewards:  535.586273920057\n",
      "iteration:  782 \teval_rewards:  536.549867965659\n",
      "iteration:  783 \teval_rewards:  1626.8749502326848\n",
      "iteration:  784 \teval_rewards:  914.9270037941394\n",
      "iteration:  785 \teval_rewards:  1049.6244261748209\n",
      "iteration:  786 \teval_rewards:  957.9569075079462\n",
      "iteration:  787 \teval_rewards:  725.2689627418846\n",
      "iteration:  788 \teval_rewards:  2744.036911294263\n",
      "iteration:  789 \teval_rewards:  1599.9313166222028\n",
      "iteration:  790 \teval_rewards:  552.3841584190756\n",
      "iteration:  791 \teval_rewards:  660.7628419397845\n",
      "iteration:  792 \teval_rewards:  2015.1848366401914\n",
      "iteration:  793 \teval_rewards:  648.2406245916451\n",
      "iteration:  794 \teval_rewards:  2265.308869159267\n",
      "iteration:  795 \teval_rewards:  482.00758941219704\n",
      "iteration:  796 \teval_rewards:  519.5441019099326\n",
      "iteration:  797 \teval_rewards:  537.1723821926748\n",
      "iteration:  798 \teval_rewards:  1219.3655206832684\n",
      "iteration:  799 \teval_rewards:  500.7797519546238\n",
      "Iter:800| Ep_Reward:383.394| Running_reward:1364.743| Actor_Loss:-0.161| Critic_Loss:169.423| Iter_duration:3.459| lr:[0.00014]\n",
      "iteration:  800 \teval_rewards:  383.39358294773416\n",
      "iteration:  801 \teval_rewards:  571.4568996882704\n",
      "iteration:  802 \teval_rewards:  604.6024958141738\n",
      "iteration:  803 \teval_rewards:  1068.9886736586418\n",
      "iteration:  804 \teval_rewards:  589.3925225826565\n",
      "iteration:  805 \teval_rewards:  429.7587073986207\n",
      "iteration:  806 \teval_rewards:  455.5200392808458\n",
      "iteration:  807 \teval_rewards:  424.5053323739997\n",
      "iteration:  808 \teval_rewards:  459.988648066267\n",
      "iteration:  809 \teval_rewards:  817.6047864876816\n",
      "iteration:  810 \teval_rewards:  561.5560505974241\n",
      "iteration:  811 \teval_rewards:  2718.2151765269223\n",
      "iteration:  812 \teval_rewards:  698.7572611482726\n",
      "iteration:  813 \teval_rewards:  437.4765068420921\n",
      "iteration:  814 \teval_rewards:  829.3624326581235\n",
      "iteration:  815 \teval_rewards:  1746.107151997406\n",
      "iteration:  816 \teval_rewards:  745.7621421974696\n",
      "iteration:  817 \teval_rewards:  430.28169861134967\n",
      "iteration:  818 \teval_rewards:  1228.2587948612158\n",
      "iteration:  819 \teval_rewards:  353.1043843759735\n",
      "iteration:  820 \teval_rewards:  2891.5112289272374\n",
      "iteration:  821 \teval_rewards:  531.2980587324136\n",
      "iteration:  822 \teval_rewards:  1140.5911018639772\n",
      "iteration:  823 \teval_rewards:  1572.3382250124855\n",
      "iteration:  824 \teval_rewards:  665.3158571087376\n",
      "iteration:  825 \teval_rewards:  2246.0991015588334\n",
      "iteration:  826 \teval_rewards:  1923.9908272351238\n",
      "iteration:  827 \teval_rewards:  387.7900833808332\n",
      "iteration:  828 \teval_rewards:  1825.9307363747564\n",
      "iteration:  829 \teval_rewards:  466.4407579627138\n",
      "iteration:  830 \teval_rewards:  604.3757655239893\n",
      "iteration:  831 \teval_rewards:  545.4131209059567\n",
      "iteration:  832 \teval_rewards:  514.0135311143257\n",
      "iteration:  833 \teval_rewards:  2332.8211080668184\n",
      "iteration:  834 \teval_rewards:  641.8922059592838\n",
      "iteration:  835 \teval_rewards:  1716.370726395164\n",
      "iteration:  836 \teval_rewards:  187.13784986106234\n",
      "iteration:  837 \teval_rewards:  640.6923135206212\n",
      "iteration:  838 \teval_rewards:  2779.809952703167\n",
      "iteration:  839 \teval_rewards:  429.1587736355895\n",
      "iteration:  840 \teval_rewards:  819.4655948619221\n",
      "iteration:  841 \teval_rewards:  493.9759922792455\n",
      "iteration:  842 \teval_rewards:  532.4410770746063\n",
      "iteration:  843 \teval_rewards:  929.8841616794277\n",
      "iteration:  844 \teval_rewards:  603.3655792486526\n",
      "iteration:  845 \teval_rewards:  428.0507023969172\n",
      "iteration:  846 \teval_rewards:  1023.6322493176194\n",
      "iteration:  847 \teval_rewards:  1109.1518349505802\n",
      "iteration:  848 \teval_rewards:  551.5607193133942\n",
      "iteration:  849 \teval_rewards:  658.2998828279842\n",
      "iteration:  850 \teval_rewards:  816.1070565945774\n",
      "iteration:  851 \teval_rewards:  788.1100251022465\n",
      "iteration:  852 \teval_rewards:  341.46678303830083\n",
      "iteration:  853 \teval_rewards:  1318.9045189213984\n",
      "iteration:  854 \teval_rewards:  970.142728815565\n",
      "iteration:  855 \teval_rewards:  646.5506331484248\n",
      "iteration:  856 \teval_rewards:  649.766972929436\n",
      "iteration:  857 \teval_rewards:  1032.877138089567\n",
      "iteration:  858 \teval_rewards:  667.51382339061\n",
      "iteration:  859 \teval_rewards:  2561.957679853551\n",
      "iteration:  860 \teval_rewards:  708.4808411255061\n",
      "iteration:  861 \teval_rewards:  661.166801831469\n",
      "iteration:  862 \teval_rewards:  554.5611048630494\n",
      "iteration:  863 \teval_rewards:  580.7474427426497\n",
      "iteration:  864 \teval_rewards:  707.2966143428273\n",
      "iteration:  865 \teval_rewards:  690.9711698966163\n",
      "iteration:  866 \teval_rewards:  1222.8419699093504\n",
      "iteration:  867 \teval_rewards:  800.3016411721878\n",
      "iteration:  868 \teval_rewards:  555.1552422946991\n",
      "iteration:  869 \teval_rewards:  513.5893168192378\n",
      "iteration:  870 \teval_rewards:  1559.723071198151\n",
      "iteration:  871 \teval_rewards:  436.2742155322582\n",
      "iteration:  872 \teval_rewards:  631.1428344235417\n",
      "iteration:  873 \teval_rewards:  1119.15192891335\n",
      "iteration:  874 \teval_rewards:  848.6381422807275\n",
      "iteration:  875 \teval_rewards:  640.6289431851009\n",
      "iteration:  876 \teval_rewards:  2357.606585472615\n",
      "iteration:  877 \teval_rewards:  997.1457552668103\n",
      "iteration:  878 \teval_rewards:  284.82582882209476\n",
      "iteration:  879 \teval_rewards:  597.1372941209901\n",
      "iteration:  880 \teval_rewards:  1366.6793148746933\n",
      "iteration:  881 \teval_rewards:  2648.647081151177\n",
      "iteration:  882 \teval_rewards:  1548.7258514982307\n",
      "iteration:  883 \teval_rewards:  1593.4724428842983\n",
      "iteration:  884 \teval_rewards:  2856.3042247467993\n",
      "iteration:  885 \teval_rewards:  639.5605571362731\n",
      "iteration:  886 \teval_rewards:  1199.4953535568695\n",
      "iteration:  887 \teval_rewards:  686.0591620028848\n",
      "iteration:  888 \teval_rewards:  2065.682160741679\n",
      "iteration:  889 \teval_rewards:  1150.7699696211478\n",
      "iteration:  890 \teval_rewards:  799.4801429260337\n",
      "iteration:  891 \teval_rewards:  1507.9761193325298\n",
      "iteration:  892 \teval_rewards:  636.795266560364\n",
      "iteration:  893 \teval_rewards:  989.5035731356752\n",
      "iteration:  894 \teval_rewards:  897.7106652788663\n",
      "iteration:  895 \teval_rewards:  2125.1169048666807\n",
      "iteration:  896 \teval_rewards:  238.54446888841915\n",
      "iteration:  897 \teval_rewards:  587.1490676807251\n",
      "iteration:  898 \teval_rewards:  1452.0404783884828\n",
      "iteration:  899 \teval_rewards:  1076.8015377385752\n",
      "Iter:900| Ep_Reward:1094.511| Running_reward:1147.699| Actor_Loss:-0.156| Critic_Loss:189.057| Iter_duration:3.392| lr:[0.00011999999999999999]\n",
      "iteration:  900 \teval_rewards:  1094.51082928546\n",
      "iteration:  901 \teval_rewards:  204.97690751549425\n",
      "iteration:  902 \teval_rewards:  561.840461115984\n",
      "iteration:  903 \teval_rewards:  729.3225776890129\n",
      "iteration:  904 \teval_rewards:  809.2631155412853\n",
      "iteration:  905 \teval_rewards:  1269.693112514936\n",
      "iteration:  906 \teval_rewards:  522.3053506631185\n",
      "iteration:  907 \teval_rewards:  1006.5602297180313\n",
      "iteration:  908 \teval_rewards:  483.8370495749115\n",
      "iteration:  909 \teval_rewards:  1053.712269363256\n",
      "iteration:  910 \teval_rewards:  547.2475509516802\n",
      "iteration:  911 \teval_rewards:  802.0819488315008\n",
      "iteration:  912 \teval_rewards:  2193.227032537657\n",
      "iteration:  913 \teval_rewards:  2451.21797106162\n",
      "iteration:  914 \teval_rewards:  513.3752111779398\n",
      "iteration:  915 \teval_rewards:  399.0846108939377\n",
      "iteration:  916 \teval_rewards:  514.947291538111\n",
      "iteration:  917 \teval_rewards:  1766.3037534125365\n",
      "iteration:  918 \teval_rewards:  607.6766260384342\n",
      "iteration:  919 \teval_rewards:  1686.2281596961855\n",
      "iteration:  920 \teval_rewards:  661.5189612808913\n",
      "iteration:  921 \teval_rewards:  561.1226355718899\n",
      "iteration:  922 \teval_rewards:  889.1763171046912\n",
      "iteration:  923 \teval_rewards:  571.1248269436464\n",
      "iteration:  924 \teval_rewards:  766.5620596446407\n",
      "iteration:  925 \teval_rewards:  529.3580127543034\n",
      "iteration:  926 \teval_rewards:  1713.268961452273\n",
      "iteration:  927 \teval_rewards:  1459.5154975631003\n",
      "iteration:  928 \teval_rewards:  1585.6377248997183\n",
      "iteration:  929 \teval_rewards:  584.9336471490227\n",
      "iteration:  930 \teval_rewards:  2647.919898408627\n",
      "iteration:  931 \teval_rewards:  2301.2257253307057\n",
      "iteration:  932 \teval_rewards:  2791.401128537485\n",
      "iteration:  933 \teval_rewards:  695.4144451001467\n",
      "iteration:  934 \teval_rewards:  1158.6514055048713\n",
      "iteration:  935 \teval_rewards:  804.341187803572\n",
      "iteration:  936 \teval_rewards:  1659.8075661761222\n",
      "iteration:  937 \teval_rewards:  1233.4611425545947\n",
      "iteration:  938 \teval_rewards:  646.5634737943578\n",
      "iteration:  939 \teval_rewards:  622.3063551991798\n",
      "iteration:  940 \teval_rewards:  545.5090939713314\n",
      "iteration:  941 \teval_rewards:  449.1014106006457\n",
      "iteration:  942 \teval_rewards:  2372.349444529078\n",
      "iteration:  943 \teval_rewards:  1962.6965582339121\n",
      "iteration:  944 \teval_rewards:  541.1164251921803\n",
      "iteration:  945 \teval_rewards:  1895.6802932067428\n",
      "iteration:  946 \teval_rewards:  2039.1328279662591\n",
      "iteration:  947 \teval_rewards:  619.3802621321676\n",
      "iteration:  948 \teval_rewards:  1056.5916194602692\n",
      "iteration:  949 \teval_rewards:  3084.8104855238494\n",
      "iteration:  950 \teval_rewards:  1778.94870340678\n",
      "iteration:  951 \teval_rewards:  520.0482663133381\n",
      "iteration:  952 \teval_rewards:  928.4274289017103\n",
      "iteration:  953 \teval_rewards:  1320.3865130535864\n",
      "iteration:  954 \teval_rewards:  679.795387397044\n",
      "iteration:  955 \teval_rewards:  2937.431633070627\n",
      "iteration:  956 \teval_rewards:  1727.992218185834\n",
      "iteration:  957 \teval_rewards:  756.131075516481\n",
      "iteration:  958 \teval_rewards:  620.7755603177059\n",
      "iteration:  959 \teval_rewards:  538.3061593959194\n",
      "iteration:  960 \teval_rewards:  1790.8649061158553\n",
      "iteration:  961 \teval_rewards:  1760.044991122646\n",
      "iteration:  962 \teval_rewards:  522.3623615102218\n",
      "iteration:  963 \teval_rewards:  1939.1363589199832\n",
      "iteration:  964 \teval_rewards:  1451.1840842303131\n",
      "iteration:  965 \teval_rewards:  534.5302852902357\n",
      "iteration:  966 \teval_rewards:  553.788990643812\n",
      "iteration:  967 \teval_rewards:  590.4616635159093\n",
      "iteration:  968 \teval_rewards:  654.2919143600149\n",
      "iteration:  969 \teval_rewards:  1632.0601666371856\n",
      "iteration:  970 \teval_rewards:  546.9297138655221\n",
      "iteration:  971 \teval_rewards:  140.43172700036507\n",
      "iteration:  972 \teval_rewards:  505.1783933275028\n",
      "iteration:  973 \teval_rewards:  738.5813432600992\n",
      "iteration:  974 \teval_rewards:  1275.407676507795\n",
      "iteration:  975 \teval_rewards:  494.5683054257121\n",
      "iteration:  976 \teval_rewards:  2626.460478287207\n",
      "iteration:  977 \teval_rewards:  623.7723425596467\n",
      "iteration:  978 \teval_rewards:  1732.2319321935918\n",
      "iteration:  979 \teval_rewards:  1586.3496134924983\n",
      "iteration:  980 \teval_rewards:  555.9051968177255\n",
      "iteration:  981 \teval_rewards:  2524.5204160885473\n",
      "iteration:  982 \teval_rewards:  655.7686316052177\n",
      "iteration:  983 \teval_rewards:  887.3356955812155\n",
      "iteration:  984 \teval_rewards:  1299.6255859369298\n",
      "iteration:  985 \teval_rewards:  793.3713474434408\n",
      "iteration:  986 \teval_rewards:  945.3000969621755\n",
      "iteration:  987 \teval_rewards:  2519.752190689352\n",
      "iteration:  988 \teval_rewards:  975.9174342272017\n",
      "iteration:  989 \teval_rewards:  585.4404609282476\n",
      "iteration:  990 \teval_rewards:  1900.8090867955495\n",
      "iteration:  991 \teval_rewards:  674.6368836042983\n",
      "iteration:  992 \teval_rewards:  1626.1533715835078\n",
      "iteration:  993 \teval_rewards:  1904.268410393117\n",
      "iteration:  994 \teval_rewards:  689.2033973310778\n",
      "iteration:  995 \teval_rewards:  1211.3994192715786\n",
      "iteration:  996 \teval_rewards:  597.5984159965744\n",
      "iteration:  997 \teval_rewards:  1739.9041936172393\n",
      "iteration:  998 \teval_rewards:  616.4457237223781\n",
      "iteration:  999 \teval_rewards:  596.3846890167542\n",
      "Iter:1000| Ep_Reward:2771.943| Running_reward:1168.375| Actor_Loss:-0.129| Critic_Loss:181.223| Iter_duration:3.737| lr:[0.0001]\n",
      "iteration:  1000 \teval_rewards:  2771.9428804972085\n",
      "iteration:  1001 \teval_rewards:  684.5914434380684\n",
      "iteration:  1002 \teval_rewards:  1634.758273877401\n",
      "iteration:  1003 \teval_rewards:  833.1210180810582\n",
      "iteration:  1004 \teval_rewards:  2284.9760914348467\n",
      "iteration:  1005 \teval_rewards:  2358.8926505410905\n",
      "iteration:  1006 \teval_rewards:  516.2891231711329\n",
      "iteration:  1007 \teval_rewards:  662.2059054613583\n",
      "iteration:  1008 \teval_rewards:  2974.969063373899\n",
      "iteration:  1009 \teval_rewards:  644.2197813325796\n",
      "iteration:  1010 \teval_rewards:  640.3906790717218\n",
      "iteration:  1011 \teval_rewards:  703.011968150013\n",
      "iteration:  1012 \teval_rewards:  1539.7861719791842\n",
      "iteration:  1013 \teval_rewards:  785.5266252422257\n",
      "iteration:  1014 \teval_rewards:  858.6523843400219\n",
      "iteration:  1015 \teval_rewards:  2884.065731398655\n",
      "iteration:  1016 \teval_rewards:  1251.331911237336\n",
      "iteration:  1017 \teval_rewards:  2093.643437685275\n",
      "iteration:  1018 \teval_rewards:  2265.3225301029715\n",
      "iteration:  1019 \teval_rewards:  1706.995454373142\n",
      "iteration:  1020 \teval_rewards:  605.2811198167948\n",
      "iteration:  1021 \teval_rewards:  2618.8503426356115\n",
      "iteration:  1022 \teval_rewards:  911.5337460302701\n",
      "iteration:  1023 \teval_rewards:  899.3119640350632\n",
      "iteration:  1024 \teval_rewards:  1344.3871234696123\n",
      "iteration:  1025 \teval_rewards:  1615.8459424880461\n",
      "iteration:  1026 \teval_rewards:  1440.925915637048\n",
      "iteration:  1027 \teval_rewards:  2440.943068762158\n",
      "iteration:  1028 \teval_rewards:  1737.3118177296446\n",
      "iteration:  1029 \teval_rewards:  1722.6254963109127\n",
      "iteration:  1030 \teval_rewards:  656.6745881358414\n",
      "iteration:  1031 \teval_rewards:  975.9446117054418\n",
      "iteration:  1032 \teval_rewards:  611.9318238427528\n",
      "iteration:  1033 \teval_rewards:  2832.9045285394163\n",
      "iteration:  1034 \teval_rewards:  2510.3420027997804\n",
      "iteration:  1035 \teval_rewards:  1293.0780437080991\n",
      "iteration:  1036 \teval_rewards:  2345.275911477226\n",
      "iteration:  1037 \teval_rewards:  655.8344721937432\n",
      "iteration:  1038 \teval_rewards:  1304.46659025844\n",
      "iteration:  1039 \teval_rewards:  2604.4938891330385\n",
      "iteration:  1040 \teval_rewards:  2284.1240731570783\n",
      "iteration:  1041 \teval_rewards:  1278.7480156230208\n",
      "iteration:  1042 \teval_rewards:  338.16784309595505\n",
      "iteration:  1043 \teval_rewards:  1646.072258859769\n",
      "iteration:  1044 \teval_rewards:  1589.5358590631743\n",
      "iteration:  1045 \teval_rewards:  998.6057473473101\n",
      "iteration:  1046 \teval_rewards:  1373.828152514152\n",
      "iteration:  1047 \teval_rewards:  648.7530303437616\n",
      "iteration:  1048 \teval_rewards:  737.5925203028961\n",
      "iteration:  1049 \teval_rewards:  721.3124934873832\n",
      "iteration:  1050 \teval_rewards:  781.8674932708508\n",
      "iteration:  1051 \teval_rewards:  1813.4636465611688\n",
      "iteration:  1052 \teval_rewards:  1385.2272052894264\n",
      "iteration:  1053 \teval_rewards:  932.9007815842373\n",
      "iteration:  1054 \teval_rewards:  768.4773830114096\n",
      "iteration:  1055 \teval_rewards:  2681.4968207022\n",
      "iteration:  1056 \teval_rewards:  627.8846740809073\n",
      "iteration:  1057 \teval_rewards:  1038.119948388461\n",
      "iteration:  1058 \teval_rewards:  1317.787814941091\n",
      "iteration:  1059 \teval_rewards:  2998.1831505154823\n",
      "iteration:  1060 \teval_rewards:  1185.7822238625731\n",
      "iteration:  1061 \teval_rewards:  571.3980418741007\n",
      "iteration:  1062 \teval_rewards:  2039.7494349558183\n",
      "iteration:  1063 \teval_rewards:  2819.372695769954\n",
      "iteration:  1064 \teval_rewards:  1609.425304741525\n",
      "iteration:  1065 \teval_rewards:  496.9531276881696\n",
      "iteration:  1066 \teval_rewards:  1736.0380664961606\n",
      "iteration:  1067 \teval_rewards:  2524.225469611673\n",
      "iteration:  1068 \teval_rewards:  1621.1665379256415\n",
      "iteration:  1069 \teval_rewards:  2362.8970483086673\n",
      "iteration:  1070 \teval_rewards:  1089.8959405220423\n",
      "iteration:  1071 \teval_rewards:  1664.87832810409\n",
      "iteration:  1072 \teval_rewards:  726.4477638275847\n",
      "iteration:  1073 \teval_rewards:  1927.8978497084886\n",
      "iteration:  1074 \teval_rewards:  2839.8702104343465\n",
      "iteration:  1075 \teval_rewards:  1649.840956672697\n",
      "iteration:  1076 \teval_rewards:  3029.8665215254373\n",
      "iteration:  1077 \teval_rewards:  571.4773666568553\n",
      "iteration:  1078 \teval_rewards:  891.5989912737491\n",
      "iteration:  1079 \teval_rewards:  984.8384419012953\n",
      "iteration:  1080 \teval_rewards:  714.0417050142828\n",
      "iteration:  1081 \teval_rewards:  773.7967783017644\n",
      "iteration:  1082 \teval_rewards:  897.5631587019037\n",
      "iteration:  1083 \teval_rewards:  1096.2673963577784\n",
      "iteration:  1084 \teval_rewards:  908.8470810330339\n",
      "iteration:  1085 \teval_rewards:  1166.489295941253\n",
      "iteration:  1086 \teval_rewards:  1320.7402263440938\n",
      "iteration:  1087 \teval_rewards:  1779.0081409707946\n",
      "iteration:  1088 \teval_rewards:  609.3681138561133\n",
      "iteration:  1089 \teval_rewards:  1570.3414950420304\n",
      "iteration:  1090 \teval_rewards:  1230.6273307279898\n",
      "iteration:  1091 \teval_rewards:  1386.389393794485\n",
      "iteration:  1092 \teval_rewards:  1928.7882303445767\n",
      "iteration:  1093 \teval_rewards:  1986.4428457105018\n",
      "iteration:  1094 \teval_rewards:  695.830047331558\n",
      "iteration:  1095 \teval_rewards:  2709.1789739629166\n",
      "iteration:  1096 \teval_rewards:  1361.95183163056\n",
      "iteration:  1097 \teval_rewards:  634.7011649913205\n",
      "iteration:  1098 \teval_rewards:  471.7229222196692\n",
      "iteration:  1099 \teval_rewards:  920.6609738906106\n",
      "Iter:1100| Ep_Reward:877.908| Running_reward:1316.622| Actor_Loss:-0.021| Critic_Loss:182.784| Iter_duration:3.241| lr:[8e-05]\n",
      "iteration:  1100 \teval_rewards:  877.9076131556877\n",
      "iteration:  1101 \teval_rewards:  1290.404801285231\n",
      "iteration:  1102 \teval_rewards:  587.2091284379331\n",
      "iteration:  1103 \teval_rewards:  822.6472791684531\n",
      "iteration:  1104 \teval_rewards:  1509.5335670937181\n",
      "iteration:  1105 \teval_rewards:  535.1053765024019\n",
      "iteration:  1106 \teval_rewards:  901.1019161267911\n",
      "iteration:  1107 \teval_rewards:  1011.7936216330668\n",
      "iteration:  1108 \teval_rewards:  943.5077930043551\n",
      "iteration:  1109 \teval_rewards:  788.5641676013187\n",
      "iteration:  1110 \teval_rewards:  1000.5483649169535\n",
      "iteration:  1111 \teval_rewards:  558.5859165117055\n",
      "iteration:  1112 \teval_rewards:  621.8537506364597\n",
      "iteration:  1113 \teval_rewards:  886.9850335099117\n",
      "iteration:  1114 \teval_rewards:  390.28824860313546\n",
      "iteration:  1115 \teval_rewards:  2975.5123941041093\n",
      "iteration:  1116 \teval_rewards:  476.32788435549884\n",
      "iteration:  1117 \teval_rewards:  1435.927271278861\n",
      "iteration:  1118 \teval_rewards:  1094.3190416246196\n",
      "iteration:  1119 \teval_rewards:  900.980701970632\n",
      "iteration:  1120 \teval_rewards:  873.061647839036\n",
      "iteration:  1121 \teval_rewards:  644.714178030683\n",
      "iteration:  1122 \teval_rewards:  453.5628295371118\n",
      "iteration:  1123 \teval_rewards:  622.5684898571757\n",
      "iteration:  1124 \teval_rewards:  615.987071206417\n",
      "iteration:  1125 \teval_rewards:  854.8583860956161\n",
      "iteration:  1126 \teval_rewards:  2913.5909148741803\n",
      "iteration:  1127 \teval_rewards:  783.649533067914\n",
      "iteration:  1128 \teval_rewards:  561.5582865908829\n",
      "iteration:  1129 \teval_rewards:  557.6209507376399\n",
      "iteration:  1130 \teval_rewards:  2695.1944309606965\n",
      "iteration:  1131 \teval_rewards:  2290.8775188590134\n",
      "iteration:  1132 \teval_rewards:  572.0325087425513\n",
      "iteration:  1133 \teval_rewards:  858.7653806924037\n",
      "iteration:  1134 \teval_rewards:  392.3399198363614\n",
      "iteration:  1135 \teval_rewards:  1219.4035171419127\n",
      "iteration:  1136 \teval_rewards:  727.858215454622\n",
      "iteration:  1137 \teval_rewards:  1316.7299437986485\n",
      "iteration:  1138 \teval_rewards:  587.303552745395\n",
      "iteration:  1139 \teval_rewards:  569.0061002320992\n",
      "iteration:  1140 \teval_rewards:  1746.1915694643287\n",
      "iteration:  1141 \teval_rewards:  317.969016207632\n",
      "iteration:  1142 \teval_rewards:  1194.797809645294\n",
      "iteration:  1143 \teval_rewards:  556.6223802695123\n",
      "iteration:  1144 \teval_rewards:  481.4164176297934\n",
      "iteration:  1145 \teval_rewards:  1255.2514376012957\n",
      "iteration:  1146 \teval_rewards:  546.563825971446\n",
      "iteration:  1147 \teval_rewards:  3084.7192576793627\n",
      "iteration:  1148 \teval_rewards:  1457.6964087853428\n",
      "iteration:  1149 \teval_rewards:  875.4737938929029\n",
      "iteration:  1150 \teval_rewards:  586.285404838372\n",
      "iteration:  1151 \teval_rewards:  928.3607136155815\n",
      "iteration:  1152 \teval_rewards:  2585.0831306673463\n",
      "iteration:  1153 \teval_rewards:  943.0856329433778\n",
      "iteration:  1154 \teval_rewards:  847.4319227317278\n",
      "iteration:  1155 \teval_rewards:  716.5097668447938\n",
      "iteration:  1156 \teval_rewards:  1413.0737671259383\n",
      "iteration:  1157 \teval_rewards:  1215.1055517958966\n",
      "iteration:  1158 \teval_rewards:  2380.4953379182452\n",
      "iteration:  1159 \teval_rewards:  1428.0393056406097\n",
      "iteration:  1160 \teval_rewards:  618.0344372061398\n",
      "iteration:  1161 \teval_rewards:  644.4213631641841\n",
      "iteration:  1162 \teval_rewards:  612.8365734792698\n",
      "iteration:  1163 \teval_rewards:  1647.8209222220084\n",
      "iteration:  1164 \teval_rewards:  2375.2103324134337\n",
      "iteration:  1165 \teval_rewards:  594.1698203386653\n",
      "iteration:  1166 \teval_rewards:  580.8171090122243\n",
      "iteration:  1167 \teval_rewards:  1147.4991766242383\n",
      "iteration:  1168 \teval_rewards:  932.9132924144131\n",
      "iteration:  1169 \teval_rewards:  1479.3040440442724\n",
      "iteration:  1170 \teval_rewards:  666.787876455713\n",
      "iteration:  1171 \teval_rewards:  2663.573135321216\n",
      "iteration:  1172 \teval_rewards:  434.20092099752026\n",
      "iteration:  1173 \teval_rewards:  1465.6112097903413\n",
      "iteration:  1174 \teval_rewards:  1787.3324719579828\n",
      "iteration:  1175 \teval_rewards:  2016.869706726642\n",
      "iteration:  1176 \teval_rewards:  506.5722508474796\n",
      "iteration:  1177 \teval_rewards:  997.6028336434342\n",
      "iteration:  1178 \teval_rewards:  453.8584475143758\n",
      "iteration:  1179 \teval_rewards:  2989.5810100783724\n",
      "iteration:  1180 \teval_rewards:  1380.068238257206\n",
      "iteration:  1181 \teval_rewards:  639.0793723799441\n",
      "iteration:  1182 \teval_rewards:  2010.8075919211267\n",
      "iteration:  1183 \teval_rewards:  912.7265530225046\n",
      "iteration:  1184 \teval_rewards:  1557.6087617292676\n",
      "iteration:  1185 \teval_rewards:  1209.3791873801806\n",
      "iteration:  1186 \teval_rewards:  571.6580895026366\n",
      "iteration:  1187 \teval_rewards:  1455.1723412073497\n",
      "iteration:  1188 \teval_rewards:  1362.4569003032154\n",
      "iteration:  1189 \teval_rewards:  637.227435903061\n",
      "iteration:  1190 \teval_rewards:  552.5122413259144\n",
      "iteration:  1191 \teval_rewards:  608.0579704330241\n",
      "iteration:  1192 \teval_rewards:  1539.9398477380937\n",
      "iteration:  1193 \teval_rewards:  1538.3469414231754\n",
      "iteration:  1194 \teval_rewards:  610.752552274726\n",
      "iteration:  1195 \teval_rewards:  511.8324180994621\n",
      "iteration:  1196 \teval_rewards:  810.7461743981364\n",
      "iteration:  1197 \teval_rewards:  584.364065360799\n",
      "iteration:  1198 \teval_rewards:  693.7167658029357\n",
      "iteration:  1199 \teval_rewards:  1420.9897170356319\n",
      "Iter:1200| Ep_Reward:2008.261| Running_reward:1194.144| Actor_Loss:0.072| Critic_Loss:258.623| Iter_duration:3.410| lr:[5.999999999999998e-05]\n",
      "iteration:  1200 \teval_rewards:  2008.2608437225713\n",
      "iteration:  1201 \teval_rewards:  1212.9775733536226\n",
      "iteration:  1202 \teval_rewards:  518.6761582072556\n",
      "iteration:  1203 \teval_rewards:  1855.4092544576058\n",
      "iteration:  1204 \teval_rewards:  996.4483116260105\n",
      "iteration:  1205 \teval_rewards:  977.0127782082485\n",
      "iteration:  1206 \teval_rewards:  1299.130703854182\n",
      "iteration:  1207 \teval_rewards:  569.1310238360526\n",
      "iteration:  1208 \teval_rewards:  1916.500976343011\n",
      "iteration:  1209 \teval_rewards:  526.9598207255791\n",
      "iteration:  1210 \teval_rewards:  1297.5779258051202\n",
      "iteration:  1211 \teval_rewards:  571.7032877647862\n",
      "iteration:  1212 \teval_rewards:  1355.7470202153045\n",
      "iteration:  1213 \teval_rewards:  1094.8015349864186\n",
      "iteration:  1214 \teval_rewards:  467.0285549528327\n",
      "iteration:  1215 \teval_rewards:  550.6072211604956\n",
      "iteration:  1216 \teval_rewards:  589.1655651349672\n",
      "iteration:  1217 \teval_rewards:  600.9794014229013\n",
      "iteration:  1218 \teval_rewards:  234.29870193852213\n",
      "iteration:  1219 \teval_rewards:  635.204416640686\n",
      "iteration:  1220 \teval_rewards:  1265.6065216996005\n",
      "iteration:  1221 \teval_rewards:  1233.5578573849123\n",
      "iteration:  1222 \teval_rewards:  573.2055239592851\n",
      "iteration:  1223 \teval_rewards:  1886.25716608205\n",
      "iteration:  1224 \teval_rewards:  914.2123584258152\n",
      "iteration:  1225 \teval_rewards:  580.4300325500225\n",
      "iteration:  1226 \teval_rewards:  1810.3271136228143\n",
      "iteration:  1227 \teval_rewards:  1025.1239206861549\n",
      "iteration:  1228 \teval_rewards:  883.2459228679122\n",
      "iteration:  1229 \teval_rewards:  547.5817979313825\n",
      "iteration:  1230 \teval_rewards:  578.4324045007154\n",
      "iteration:  1231 \teval_rewards:  1618.2848233965778\n",
      "iteration:  1232 \teval_rewards:  1468.8337297255086\n",
      "iteration:  1233 \teval_rewards:  1281.8617826588154\n",
      "iteration:  1234 \teval_rewards:  1125.4951276236452\n",
      "iteration:  1235 \teval_rewards:  659.7999058389858\n",
      "iteration:  1236 \teval_rewards:  381.01831311013785\n",
      "iteration:  1237 \teval_rewards:  2738.587789097832\n",
      "iteration:  1238 \teval_rewards:  244.42512538213242\n",
      "iteration:  1239 \teval_rewards:  849.9763715155624\n",
      "iteration:  1240 \teval_rewards:  1898.8467045709554\n",
      "iteration:  1241 \teval_rewards:  2804.7072220682\n",
      "iteration:  1242 \teval_rewards:  786.4929119628147\n",
      "iteration:  1243 \teval_rewards:  907.59898149499\n",
      "iteration:  1244 \teval_rewards:  1091.8497307067466\n",
      "iteration:  1245 \teval_rewards:  2120.3662203910294\n",
      "iteration:  1246 \teval_rewards:  985.1743870434491\n",
      "iteration:  1247 \teval_rewards:  1832.798364599768\n",
      "iteration:  1248 \teval_rewards:  2754.2488277900698\n",
      "iteration:  1249 \teval_rewards:  580.8373544353926\n",
      "iteration:  1250 \teval_rewards:  2869.030502176541\n",
      "iteration:  1251 \teval_rewards:  621.9880239803852\n",
      "iteration:  1252 \teval_rewards:  779.3173730723997\n",
      "iteration:  1253 \teval_rewards:  3096.1098898815444\n",
      "iteration:  1254 \teval_rewards:  1638.957750697159\n",
      "iteration:  1255 \teval_rewards:  613.9414718056415\n",
      "iteration:  1256 \teval_rewards:  1573.3299796693593\n",
      "iteration:  1257 \teval_rewards:  1968.4441398477304\n",
      "iteration:  1258 \teval_rewards:  1583.103105380347\n",
      "iteration:  1259 \teval_rewards:  843.4143625415463\n",
      "iteration:  1260 \teval_rewards:  709.7125024592517\n",
      "iteration:  1261 \teval_rewards:  890.3141266729938\n",
      "iteration:  1262 \teval_rewards:  2530.3041066695832\n",
      "iteration:  1263 \teval_rewards:  542.9038909247349\n",
      "iteration:  1264 \teval_rewards:  2061.5855737378797\n",
      "iteration:  1265 \teval_rewards:  3051.9643259505806\n",
      "iteration:  1266 \teval_rewards:  480.9517779701573\n",
      "iteration:  1267 \teval_rewards:  431.68996595699326\n",
      "iteration:  1268 \teval_rewards:  1669.0299302725355\n",
      "iteration:  1269 \teval_rewards:  2506.1251564756103\n",
      "iteration:  1270 \teval_rewards:  1175.5448019218368\n",
      "iteration:  1271 \teval_rewards:  2510.4112005129464\n",
      "iteration:  1272 \teval_rewards:  1062.1900221486183\n",
      "iteration:  1273 \teval_rewards:  434.10638696913605\n",
      "iteration:  1274 \teval_rewards:  1622.0548236442348\n",
      "iteration:  1275 \teval_rewards:  919.0123852533609\n",
      "iteration:  1276 \teval_rewards:  2558.0660307260223\n",
      "iteration:  1277 \teval_rewards:  492.72495539947806\n",
      "iteration:  1278 \teval_rewards:  1219.2272919556792\n",
      "iteration:  1279 \teval_rewards:  1906.7329515432798\n",
      "iteration:  1280 \teval_rewards:  782.9354971568578\n",
      "iteration:  1281 \teval_rewards:  531.1370116188046\n",
      "iteration:  1282 \teval_rewards:  587.9531406054048\n",
      "iteration:  1283 \teval_rewards:  812.3936100991125\n",
      "iteration:  1284 \teval_rewards:  1059.104862145263\n",
      "iteration:  1285 \teval_rewards:  627.5585590685806\n",
      "iteration:  1286 \teval_rewards:  3138.4201091493005\n",
      "iteration:  1287 \teval_rewards:  557.2938204318643\n",
      "iteration:  1288 \teval_rewards:  590.4267152199\n",
      "iteration:  1289 \teval_rewards:  1970.18013562641\n",
      "iteration:  1290 \teval_rewards:  778.0792605462625\n",
      "iteration:  1291 \teval_rewards:  1421.718988583491\n",
      "iteration:  1292 \teval_rewards:  1163.0176934636145\n",
      "iteration:  1293 \teval_rewards:  507.9940071476854\n",
      "iteration:  1294 \teval_rewards:  791.7728771393467\n",
      "iteration:  1295 \teval_rewards:  511.61136846685815\n",
      "iteration:  1296 \teval_rewards:  1251.1933571821921\n",
      "iteration:  1297 \teval_rewards:  3054.9910788542084\n",
      "iteration:  1298 \teval_rewards:  918.3782975457999\n",
      "iteration:  1299 \teval_rewards:  948.7170405773854\n",
      "Iter:1300| Ep_Reward:637.507| Running_reward:1219.668| Actor_Loss:-0.013| Critic_Loss:153.183| Iter_duration:3.174| lr:[3.999999999999999e-05]\n",
      "iteration:  1300 \teval_rewards:  637.5066727168314\n",
      "iteration:  1301 \teval_rewards:  1495.0162239755841\n",
      "iteration:  1302 \teval_rewards:  842.7642108323695\n",
      "iteration:  1303 \teval_rewards:  1739.1199584092235\n",
      "iteration:  1304 \teval_rewards:  3106.2925636934806\n",
      "iteration:  1305 \teval_rewards:  402.16350126939625\n",
      "iteration:  1306 \teval_rewards:  885.5117738843978\n",
      "iteration:  1307 \teval_rewards:  1246.193614756612\n",
      "iteration:  1308 \teval_rewards:  631.9888861703301\n",
      "iteration:  1309 \teval_rewards:  994.7072866544804\n",
      "iteration:  1310 \teval_rewards:  1720.076597989749\n",
      "iteration:  1311 \teval_rewards:  847.2045762383933\n",
      "iteration:  1312 \teval_rewards:  1162.877836585618\n",
      "iteration:  1313 \teval_rewards:  1089.265495158093\n",
      "iteration:  1314 \teval_rewards:  564.3496070699399\n",
      "iteration:  1315 \teval_rewards:  953.2470514376135\n",
      "iteration:  1316 \teval_rewards:  2140.961973397414\n",
      "iteration:  1317 \teval_rewards:  581.7892469121833\n",
      "iteration:  1318 \teval_rewards:  1748.967279561155\n",
      "iteration:  1319 \teval_rewards:  3100.078710064144\n",
      "iteration:  1320 \teval_rewards:  620.3283485421861\n",
      "iteration:  1321 \teval_rewards:  2197.051947211731\n",
      "iteration:  1322 \teval_rewards:  2097.6048129606033\n",
      "iteration:  1323 \teval_rewards:  610.2891558155738\n",
      "iteration:  1324 \teval_rewards:  565.2407621030014\n",
      "iteration:  1325 \teval_rewards:  383.8178790044353\n",
      "iteration:  1326 \teval_rewards:  604.3872141392046\n",
      "iteration:  1327 \teval_rewards:  544.9495931325474\n",
      "iteration:  1328 \teval_rewards:  517.689608267828\n",
      "iteration:  1329 \teval_rewards:  557.1922723575716\n",
      "iteration:  1330 \teval_rewards:  2611.7432179321263\n",
      "iteration:  1331 \teval_rewards:  1381.2828073370122\n",
      "iteration:  1332 \teval_rewards:  745.5333597396716\n",
      "iteration:  1333 \teval_rewards:  1291.6474024602403\n",
      "iteration:  1334 \teval_rewards:  434.10347946425776\n",
      "iteration:  1335 \teval_rewards:  796.9575167525473\n",
      "iteration:  1336 \teval_rewards:  705.3761813562696\n",
      "iteration:  1337 \teval_rewards:  655.1735156362756\n",
      "iteration:  1338 \teval_rewards:  2211.9692434197113\n",
      "iteration:  1339 \teval_rewards:  2429.369940755966\n",
      "iteration:  1340 \teval_rewards:  1303.6050678446004\n",
      "iteration:  1341 \teval_rewards:  546.6672258846853\n",
      "iteration:  1342 \teval_rewards:  2665.9104894127618\n",
      "iteration:  1343 \teval_rewards:  1510.8608988681844\n",
      "iteration:  1344 \teval_rewards:  2137.3271492055974\n",
      "iteration:  1345 \teval_rewards:  607.0389877605588\n",
      "iteration:  1346 \teval_rewards:  1713.0017473725654\n",
      "iteration:  1347 \teval_rewards:  843.0399641771193\n",
      "iteration:  1348 \teval_rewards:  537.8010922088101\n",
      "iteration:  1349 \teval_rewards:  691.8403491277572\n",
      "iteration:  1350 \teval_rewards:  774.5077139550609\n",
      "iteration:  1351 \teval_rewards:  597.7516298294695\n",
      "iteration:  1352 \teval_rewards:  765.2524816277971\n",
      "iteration:  1353 \teval_rewards:  672.7777918866464\n",
      "iteration:  1354 \teval_rewards:  1190.1101845468704\n",
      "iteration:  1355 \teval_rewards:  1481.749486933124\n",
      "iteration:  1356 \teval_rewards:  877.3541978657021\n",
      "iteration:  1357 \teval_rewards:  3130.08461958633\n",
      "iteration:  1358 \teval_rewards:  2451.0729880929666\n",
      "iteration:  1359 \teval_rewards:  2965.0291701073907\n",
      "iteration:  1360 \teval_rewards:  1511.6615914350255\n",
      "iteration:  1361 \teval_rewards:  491.2660856075772\n",
      "iteration:  1362 \teval_rewards:  3119.7084665153448\n",
      "iteration:  1363 \teval_rewards:  992.6604752842289\n",
      "iteration:  1364 \teval_rewards:  909.87644682662\n",
      "iteration:  1365 \teval_rewards:  860.7203373387832\n",
      "iteration:  1366 \teval_rewards:  1109.3794713048355\n",
      "iteration:  1367 \teval_rewards:  3146.6239467925743\n",
      "iteration:  1368 \teval_rewards:  622.8663055299014\n",
      "iteration:  1369 \teval_rewards:  641.4124771176727\n",
      "iteration:  1370 \teval_rewards:  752.1339668974523\n",
      "iteration:  1371 \teval_rewards:  3022.9544848836645\n",
      "iteration:  1372 \teval_rewards:  1290.7720036945284\n",
      "iteration:  1373 \teval_rewards:  3065.149120305294\n",
      "iteration:  1374 \teval_rewards:  1224.9303965573936\n",
      "iteration:  1375 \teval_rewards:  516.8798200860293\n",
      "iteration:  1376 \teval_rewards:  1512.02452896766\n",
      "iteration:  1377 \teval_rewards:  750.7995880098218\n",
      "iteration:  1378 \teval_rewards:  653.5938238191579\n",
      "iteration:  1379 \teval_rewards:  3166.0987441714765\n",
      "iteration:  1380 \teval_rewards:  3030.4387012890456\n",
      "iteration:  1381 \teval_rewards:  2159.8791917544754\n",
      "iteration:  1382 \teval_rewards:  482.5340826733411\n",
      "iteration:  1383 \teval_rewards:  559.1385788111486\n",
      "iteration:  1384 \teval_rewards:  3134.388895924964\n",
      "iteration:  1385 \teval_rewards:  730.1201637010348\n",
      "iteration:  1386 \teval_rewards:  1842.412693622777\n",
      "iteration:  1387 \teval_rewards:  2036.5256548281016\n",
      "iteration:  1388 \teval_rewards:  2939.014726085423\n",
      "iteration:  1389 \teval_rewards:  1369.1905551249797\n",
      "iteration:  1390 \teval_rewards:  2094.423991892569\n",
      "iteration:  1391 \teval_rewards:  1216.894533652764\n",
      "iteration:  1392 \teval_rewards:  2299.3394206599255\n",
      "iteration:  1393 \teval_rewards:  3112.0493484233657\n",
      "iteration:  1394 \teval_rewards:  3026.367718625017\n",
      "iteration:  1395 \teval_rewards:  1393.5580181849687\n",
      "iteration:  1396 \teval_rewards:  2858.4292889062517\n",
      "iteration:  1397 \teval_rewards:  875.4982362186335\n",
      "iteration:  1398 \teval_rewards:  2179.976604130095\n",
      "iteration:  1399 \teval_rewards:  1269.4002964447582\n",
      "Iter:1400| Ep_Reward:703.226| Running_reward:1398.109| Actor_Loss:0.066| Critic_Loss:1180.378| Iter_duration:3.189| lr:[1.9999999999999995e-05]\n",
      "iteration:  1400 \teval_rewards:  703.2262710280412\n",
      "iteration:  1401 \teval_rewards:  373.0385584320485\n",
      "iteration:  1402 \teval_rewards:  306.8137796172655\n",
      "iteration:  1403 \teval_rewards:  829.5755125430907\n",
      "iteration:  1404 \teval_rewards:  657.9896419723077\n",
      "iteration:  1405 \teval_rewards:  502.5666599862758\n",
      "iteration:  1406 \teval_rewards:  2782.9613172721074\n",
      "iteration:  1407 \teval_rewards:  600.75324378047\n",
      "iteration:  1408 \teval_rewards:  1342.1455634257422\n",
      "iteration:  1409 \teval_rewards:  604.7319773344815\n",
      "iteration:  1410 \teval_rewards:  2927.5912111572056\n",
      "iteration:  1411 \teval_rewards:  1122.1382020778226\n",
      "iteration:  1412 \teval_rewards:  768.0956198633444\n",
      "iteration:  1413 \teval_rewards:  2630.659242770999\n",
      "iteration:  1414 \teval_rewards:  3154.5113139104806\n",
      "iteration:  1415 \teval_rewards:  1421.559291875078\n",
      "iteration:  1416 \teval_rewards:  501.31961287585716\n",
      "iteration:  1417 \teval_rewards:  2380.4737969697803\n",
      "iteration:  1418 \teval_rewards:  1045.9650812169775\n",
      "iteration:  1419 \teval_rewards:  3226.0153743054657\n",
      "iteration:  1420 \teval_rewards:  3183.4202415030336\n",
      "iteration:  1421 \teval_rewards:  1170.6722781988753\n",
      "iteration:  1422 \teval_rewards:  2345.4201665746345\n",
      "iteration:  1423 \teval_rewards:  1986.0711741266916\n",
      "iteration:  1424 \teval_rewards:  1161.2543408086535\n",
      "iteration:  1425 \teval_rewards:  2703.345502767102\n",
      "iteration:  1426 \teval_rewards:  3160.735850479412\n",
      "iteration:  1427 \teval_rewards:  2952.504719234715\n",
      "iteration:  1428 \teval_rewards:  602.7289085327159\n",
      "iteration:  1429 \teval_rewards:  1782.2917360522079\n",
      "iteration:  1430 \teval_rewards:  1224.3876669937054\n",
      "iteration:  1431 \teval_rewards:  1941.7274734581624\n",
      "iteration:  1432 \teval_rewards:  902.2012542182915\n",
      "iteration:  1433 \teval_rewards:  2152.247364837033\n",
      "iteration:  1434 \teval_rewards:  428.13359924395326\n",
      "iteration:  1435 \teval_rewards:  556.4095565673232\n",
      "iteration:  1436 \teval_rewards:  529.3590045842552\n",
      "iteration:  1437 \teval_rewards:  653.2577658904327\n",
      "iteration:  1438 \teval_rewards:  3168.5589121853427\n",
      "iteration:  1439 \teval_rewards:  698.4788929128598\n",
      "iteration:  1440 \teval_rewards:  655.9145477854546\n",
      "iteration:  1441 \teval_rewards:  558.8925848109072\n",
      "iteration:  1442 \teval_rewards:  773.491095918167\n",
      "iteration:  1443 \teval_rewards:  2168.5794281259714\n",
      "iteration:  1444 \teval_rewards:  636.9498650330106\n",
      "iteration:  1445 \teval_rewards:  1425.3712820819374\n",
      "iteration:  1446 \teval_rewards:  1720.6133060530794\n",
      "iteration:  1447 \teval_rewards:  516.450746294077\n",
      "iteration:  1448 \teval_rewards:  1571.4534637883924\n",
      "iteration:  1449 \teval_rewards:  1767.1971362241356\n",
      "iteration:  1450 \teval_rewards:  1136.1109298262193\n",
      "iteration:  1451 \teval_rewards:  1059.5375093884334\n",
      "iteration:  1452 \teval_rewards:  896.0684284031182\n",
      "iteration:  1453 \teval_rewards:  883.7401484044113\n",
      "iteration:  1454 \teval_rewards:  579.1684011136639\n",
      "iteration:  1455 \teval_rewards:  528.441595238264\n",
      "iteration:  1456 \teval_rewards:  2234.9381665153273\n",
      "iteration:  1457 \teval_rewards:  804.7629959696054\n",
      "iteration:  1458 \teval_rewards:  1605.042480100775\n",
      "iteration:  1459 \teval_rewards:  657.7350249555232\n",
      "iteration:  1460 \teval_rewards:  820.7979419461526\n",
      "iteration:  1461 \teval_rewards:  687.0745540186603\n",
      "iteration:  1462 \teval_rewards:  1341.2991300647527\n",
      "iteration:  1463 \teval_rewards:  862.0943285587495\n",
      "iteration:  1464 \teval_rewards:  2340.1923341750526\n",
      "iteration:  1465 \teval_rewards:  1869.1130319322438\n",
      "iteration:  1466 \teval_rewards:  2184.871089706912\n",
      "iteration:  1467 \teval_rewards:  2999.702522520607\n",
      "iteration:  1468 \teval_rewards:  1356.0539174514613\n",
      "iteration:  1469 \teval_rewards:  2076.383610681907\n",
      "iteration:  1470 \teval_rewards:  2960.945595308152\n",
      "iteration:  1471 \teval_rewards:  554.4621057162465\n",
      "iteration:  1472 \teval_rewards:  2516.1569799821204\n",
      "iteration:  1473 \teval_rewards:  1921.3128940063314\n",
      "iteration:  1474 \teval_rewards:  2074.93730359911\n",
      "iteration:  1475 \teval_rewards:  730.6928755180221\n",
      "iteration:  1476 \teval_rewards:  754.6885837309101\n",
      "iteration:  1477 \teval_rewards:  1170.7741688520707\n",
      "iteration:  1478 \teval_rewards:  2986.8449477270715\n",
      "iteration:  1479 \teval_rewards:  1666.415702934318\n",
      "iteration:  1480 \teval_rewards:  778.3883413292853\n",
      "iteration:  1481 \teval_rewards:  492.6817728877979\n",
      "iteration:  1482 \teval_rewards:  550.8048213501937\n",
      "iteration:  1483 \teval_rewards:  703.1162045337564\n",
      "iteration:  1484 \teval_rewards:  651.4252640227556\n",
      "iteration:  1485 \teval_rewards:  753.1973550713518\n",
      "iteration:  1486 \teval_rewards:  2853.5452201199932\n",
      "iteration:  1487 \teval_rewards:  665.6689442050576\n",
      "iteration:  1488 \teval_rewards:  2346.3519083215797\n",
      "iteration:  1489 \teval_rewards:  1355.0624437148756\n",
      "iteration:  1490 \teval_rewards:  1094.140822282519\n",
      "iteration:  1491 \teval_rewards:  851.9214783408217\n",
      "iteration:  1492 \teval_rewards:  1820.246431958667\n",
      "iteration:  1493 \teval_rewards:  743.3107364676333\n",
      "iteration:  1494 \teval_rewards:  2340.4753239800943\n",
      "iteration:  1495 \teval_rewards:  1135.181584568108\n",
      "iteration:  1496 \teval_rewards:  2914.5990679216807\n",
      "iteration:  1497 \teval_rewards:  556.5408933381789\n",
      "iteration:  1498 \teval_rewards:  514.0707948723912\n",
      "iteration:  1499 \teval_rewards:  3017.5610794621302\n",
      "Iter:1500| Ep_Reward:564.273| Running_reward:1415.242| Actor_Loss:0.196| Critic_Loss:65.737| Iter_duration:3.128| lr:[0.0]\n",
      "iteration:  1500 \teval_rewards:  564.2734512254788\n",
      "episode reward:1018.943\n"
     ]
    }
   ],
   "source": [
    "print(f\"number of states:{n_states}\\n\"\n",
    "      f\"action bounds:{action_bounds}\\n\"\n",
    "      f\"number of actions:{n_actions}\")\n",
    "\n",
    "if not os.path.exists(ENV_NAME):\n",
    "    os.mkdir(ENV_NAME)\n",
    "    os.mkdir(ENV_NAME + \"/logs\")\n",
    "\n",
    "env = gym.make(ENV_NAME + \"-v2\")\n",
    "\n",
    "agent = Agent(n_states=n_states,\n",
    "              n_iter=n_iterations,\n",
    "              env_name=ENV_NAME,\n",
    "              action_bounds=action_bounds,\n",
    "              n_actions=n_actions,\n",
    "              lr=lr)\n",
    "if TRAIN_FLAG:\n",
    "    trainer = Train(env=env,\n",
    "                    test_env=test_env,\n",
    "                    env_name=ENV_NAME,\n",
    "                    agent=agent,\n",
    "                    horizon=T,\n",
    "                    n_iterations=n_iterations,\n",
    "                    epochs=epochs,\n",
    "                    mini_batch_size=mini_batch_size,\n",
    "                    epsilon=clip_range)\n",
    "    trainer.step()\n",
    "\n",
    "player = Play(env, agent, ENV_NAME)\n",
    "player.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ant (4 legs animal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"Ant\"\n",
    "TRAIN_FLAG = True\n",
    "test_env = gym.make(ENV_NAME + \"-v2\")\n",
    "\n",
    "n_states = test_env.observation_space.shape[0]\n",
    "action_bounds = [test_env.action_space.low[0], test_env.action_space.high[0]]\n",
    "n_actions = test_env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"number of states:{n_states}\\n\"\n",
    "      f\"action bounds:{action_bounds}\\n\"\n",
    "      f\"number of actions:{n_actions}\")\n",
    "\n",
    "if not os.path.exists(ENV_NAME):\n",
    "    os.mkdir(ENV_NAME)\n",
    "    os.mkdir(ENV_NAME + \"/logs\")\n",
    "\n",
    "env = gym.make(ENV_NAME + \"-v2\")\n",
    "\n",
    "agent = Agent(n_states=n_states,\n",
    "              n_iter=n_iterations,\n",
    "              env_name=ENV_NAME,\n",
    "              action_bounds=action_bounds,\n",
    "              n_actions=n_actions,\n",
    "              lr=lr)\n",
    "if TRAIN_FLAG:\n",
    "    trainer = Train(env=env,\n",
    "                    test_env=test_env,\n",
    "                    env_name=ENV_NAME,\n",
    "                    agent=agent,\n",
    "                    horizon=T,\n",
    "                    n_iterations=n_iterations,\n",
    "                    epochs=epochs,\n",
    "                    mini_batch_size=mini_batch_size,\n",
    "                    epsilon=clip_range)\n",
    "    trainer.step()\n",
    "\n",
    "player = Play(env, agent, ENV_NAME)\n",
    "player.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FetchPickAndPlace\n",
    "\n",
    "The FetchPickAndPlace has some state output which different from the other environment.\n",
    "It contains dictionary of environment in the form of:\n",
    "- observation: the joint degree position and speed\n",
    "- desired_goal: the target position that robot need to go\n",
    "- achieved_goal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "3\n",
      "[ 1.34193265e+00  7.49100375e-01  5.34722720e-01  1.19599421e+00\n",
      "  8.00557661e-01  4.24702091e-01 -1.45938432e-01  5.14572867e-02\n",
      " -1.10020629e-01  2.91834773e-06 -4.72661656e-08 -3.85214084e-07\n",
      "  5.92637053e-07  1.12208536e-13 -7.74656889e-06 -7.65027248e-08\n",
      "  4.92570535e-05  1.88857148e-07 -2.90549459e-07 -1.18156686e-18\n",
      "  7.73934983e-06  7.18103404e-08 -2.42928780e-06  4.93607091e-07\n",
      "  1.70999820e-07]\n",
      "[1.19599421 0.80055766 0.42470209]\n",
      "[1.45979337 0.7254639  0.60492436]\n"
     ]
    }
   ],
   "source": [
    "ENV_NAME = \"FetchPickAndPlace\"\n",
    "TRAIN_FLAG = True\n",
    "test_env = gym.make(ENV_NAME + \"-v1\")\n",
    "\n",
    "n_states = test_env.observation_space[\"observation\"].shape[0]\n",
    "n_achieveds = test_env.observation_space[\"achieved_goal\"].shape[0]\n",
    "n_goals = test_env.observation_space[\"desired_goal\"].shape[0]\n",
    "action_bounds = [test_env.action_space.low[0], test_env.action_space.high[0]]\n",
    "n_actions = test_env.action_space.shape[0]\n",
    "\n",
    "print(n_states)\n",
    "print(n_goals)\n",
    "\n",
    "env_dict = test_env.reset()\n",
    "\n",
    "state = env_dict[\"observation\"]\n",
    "achieved_goal = env_dict[\"achieved_goal\"]\n",
    "desired_goal = env_dict[\"desired_goal\"]\n",
    "\n",
    "print(state)\n",
    "print(achieved_goal)\n",
    "print(desired_goal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to modify the environment dict to be the 1d array of state which contatain sate observation and desired goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.3419e+00,  7.4910e-01,  5.3472e-01,  1.1960e+00,  8.0056e-01,\n",
      "         4.2470e-01, -1.4594e-01,  5.1457e-02, -1.1002e-01,  2.9183e-06,\n",
      "        -4.7266e-08, -3.8521e-07,  5.9264e-07,  1.1221e-13, -7.7466e-06,\n",
      "        -7.6503e-08,  4.9257e-05,  1.8886e-07, -2.9055e-07, -1.1816e-18,\n",
      "         7.7393e-06,  7.1810e-08, -2.4293e-06,  4.9361e-07,  1.7100e-07,\n",
      "         1.4598e+00,  7.2546e-01,  6.0492e-01])\n"
     ]
    }
   ],
   "source": [
    "state = env_dict[\"observation\"]\n",
    "achieved_goal = env_dict[\"achieved_goal\"]\n",
    "desired_goal = env_dict[\"desired_goal\"]\n",
    "\n",
    "#state = np.expand_dims(state, axis=0)\n",
    "#goal = np.expand_dims(desired_goal, axis=0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x = np.append(state, desired_goal)\n",
    "    x = from_numpy(x).float()\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_state(env_dict):\n",
    "    state = env_dict[\"observation\"]\n",
    "    achieved_goal = env_dict[\"achieved_goal\"]\n",
    "    desired_goal = env_dict[\"desired_goal\"]\n",
    "\n",
    "    return np.append(state, desired_goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(agent, env, state_rms, action_bounds):\n",
    "    total_rewards = 0\n",
    "    s = env.reset()\n",
    "    s = set_state(s)\n",
    "    done = False\n",
    "    while not done:\n",
    "        s = np.clip((s - state_rms.mean) / (state_rms.var ** 0.5 + 1e-8), -5.0, 5.0)\n",
    "        dist = agent.choose_dist(s)\n",
    "        action = dist.sample().cpu().numpy()[0]\n",
    "        # action = np.clip(action, action_bounds[0], action_bounds[1])\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = set_state(next_state)\n",
    "        # env.render()\n",
    "        s = next_state\n",
    "        total_rewards += reward\n",
    "    # env.close()\n",
    "    return total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train:\n",
    "    def __init__(self, env, test_env, env_name, n_iterations, agent, epochs, mini_batch_size, epsilon, horizon):\n",
    "        self.env = env\n",
    "        self.env_name = env_name\n",
    "        self.test_env = test_env\n",
    "        self.agent = agent\n",
    "        self.epsilon = epsilon\n",
    "        self.horizon = horizon\n",
    "        self.epochs = epochs\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.n_iterations = n_iterations\n",
    "\n",
    "        self.start_time = 0\n",
    "        self.state_rms = RunningMeanStd(shape=(self.agent.n_states,))\n",
    "\n",
    "        self.running_reward = 0\n",
    "\n",
    "    @staticmethod\n",
    "    def choose_mini_batch(mini_batch_size, states, actions, returns, advs, values, log_probs):\n",
    "        full_batch_size = len(states)\n",
    "        for _ in range(full_batch_size // mini_batch_size):\n",
    "            indices = np.random.randint(0, full_batch_size, mini_batch_size)\n",
    "            yield states[indices], actions[indices], returns[indices], advs[indices], values[indices],\\\n",
    "                  log_probs[indices]\n",
    "\n",
    "    def train(self, states, actions, advs, values, log_probs):\n",
    "\n",
    "        values = np.vstack(values[:-1])\n",
    "        log_probs = np.vstack(log_probs)\n",
    "        returns = advs + values\n",
    "        advs = (advs - advs.mean()) / (advs.std() + 1e-8)\n",
    "        actions = np.vstack(actions)\n",
    "        for epoch in range(self.epochs):\n",
    "            for state, action, return_, adv, old_value, old_log_prob in self.choose_mini_batch(self.mini_batch_size,\n",
    "                                                                                               states, actions, returns,\n",
    "                                                                                               advs, values, log_probs):\n",
    "                state = torch.Tensor(state).to(self.agent.device)\n",
    "                action = torch.Tensor(action).to(self.agent.device)\n",
    "                return_ = torch.Tensor(return_).to(self.agent.device)\n",
    "                adv = torch.Tensor(adv).to(self.agent.device)\n",
    "                old_value = torch.Tensor(old_value).to(self.agent.device)\n",
    "                old_log_prob = torch.Tensor(old_log_prob).to(self.agent.device)\n",
    "\n",
    "                value = self.agent.critic(state)\n",
    "                # clipped_value = old_value + torch.clamp(value - old_value, -self.epsilon, self.epsilon)\n",
    "                # clipped_v_loss = (clipped_value - return_).pow(2)\n",
    "                # unclipped_v_loss = (value - return_).pow(2)\n",
    "                # critic_loss = 0.5 * torch.max(clipped_v_loss, unclipped_v_loss).mean()\n",
    "                critic_loss = self.agent.critic_loss(value, return_)\n",
    "\n",
    "                new_log_prob = self.calculate_log_probs(self.agent.current_policy, state, action)\n",
    "\n",
    "                ratio = (new_log_prob - old_log_prob).exp()\n",
    "                actor_loss = self.compute_actor_loss(ratio, adv)\n",
    "\n",
    "                self.agent.optimize(actor_loss, critic_loss)\n",
    "\n",
    "        return actor_loss, critic_loss\n",
    "\n",
    "    def step(self):\n",
    "        state = self.env.reset()\n",
    "        state = set_state(state)\n",
    "        for iteration in range(1, 1 + self.n_iterations):\n",
    "            states = []\n",
    "            actions = []\n",
    "            rewards = []\n",
    "            values = []\n",
    "            log_probs = []\n",
    "            dones = []\n",
    "\n",
    "            self.start_time = time.time()\n",
    "            for t in range(self.horizon):\n",
    "                # self.state_rms.update(state)\n",
    "                state = np.clip((state - self.state_rms.mean) / (self.state_rms.var ** 0.5 + 1e-8), -5, 5)\n",
    "                dist = self.agent.choose_dist(state)\n",
    "                action = dist.sample()\n",
    "                # action = np.clip(action, self.agent.action_bounds[0], self.agent.action_bounds[1])\n",
    "                log_prob = dist.log_prob(action).cpu()\n",
    "                action = action.cpu().numpy()[0]\n",
    "                value = self.agent.get_value(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                next_state = set_state(next_state)\n",
    "\n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                values.append(value)\n",
    "                log_probs.append(log_prob)\n",
    "                dones.append(done)\n",
    "\n",
    "                if done:\n",
    "                    state = self.env.reset()\n",
    "                    state = set_state(state)\n",
    "                else:\n",
    "                    state = next_state\n",
    "            # self.state_rms.update(next_state)\n",
    "            next_state = np.clip((next_state - self.state_rms.mean) / (self.state_rms.var ** 0.5 + 1e-8), -5, 5)\n",
    "            next_value = self.agent.get_value(next_state) * (1 - done)\n",
    "            values.append(next_value)\n",
    "\n",
    "            advs = self.get_gae(rewards, values, dones)\n",
    "            states = np.vstack(states)\n",
    "            actor_loss, critic_loss = self.train(states, actions, advs, values, log_probs)\n",
    "            # self.agent.set_weights()\n",
    "            self.agent.schedule_lr()\n",
    "            eval_rewards = evaluate_model(self.agent, self.test_env, self.state_rms, self.agent.action_bounds)\n",
    "            self.state_rms.update(states)\n",
    "            self.print_logs(iteration, actor_loss, critic_loss, eval_rewards)\n",
    "            print(\"iteration: \", iteration, \"\\teval_rewards: \", eval_rewards)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_gae(rewards, values, dones, gamma=0.99, lam=0.95):\n",
    "\n",
    "        advs = []\n",
    "        gae = 0\n",
    "\n",
    "        dones.append(0)\n",
    "        for step in reversed(range(len(rewards))):\n",
    "            delta = rewards[step] + gamma * (values[step + 1]) * (1 - dones[step]) - values[step]\n",
    "            gae = delta + gamma * lam * (1 - dones[step]) * gae\n",
    "            advs.append(gae)\n",
    "\n",
    "        advs.reverse()\n",
    "        return np.vstack(advs)\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_log_probs(model, states, actions):\n",
    "        policy_distribution = model(states)\n",
    "        return policy_distribution.log_prob(actions)\n",
    "\n",
    "    def compute_actor_loss(self, ratio, adv):\n",
    "        pg_loss1 = adv * ratio\n",
    "        pg_loss2 = adv * torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon)\n",
    "        loss = -torch.min(pg_loss1, pg_loss2).mean()\n",
    "        return loss\n",
    "\n",
    "    def print_logs(self, iteration, actor_loss, critic_loss, eval_rewards):\n",
    "        if iteration == 1:\n",
    "            self.running_reward = eval_rewards\n",
    "        else:\n",
    "            self.running_reward = self.running_reward * 0.99 + eval_rewards * 0.01\n",
    "\n",
    "        if iteration % 100 == 0:\n",
    "            print(f\"Iter:{iteration}| \"\n",
    "                  f\"Ep_Reward:{eval_rewards:.3f}| \"\n",
    "                  f\"Running_reward:{self.running_reward:.3f}| \"\n",
    "                  f\"Actor_Loss:{actor_loss:.3f}| \"\n",
    "                  f\"Critic_Loss:{critic_loss:.3f}| \"\n",
    "                  f\"Iter_duration:{time.time() - self.start_time:.3f}| \"\n",
    "                  f\"lr:{self.agent.actor_scheduler.get_last_lr()}\")\n",
    "            self.agent.save_weights(iteration, self.state_rms)\n",
    "\n",
    "        with SummaryWriter(self.env_name + \"/logs\") as writer:\n",
    "            writer.add_scalar(\"Episode running reward\", self.running_reward, iteration)\n",
    "            writer.add_scalar(\"Episode reward\", eval_rewards, iteration)\n",
    "            writer.add_scalar(\"Actor loss\", actor_loss, iteration)\n",
    "            writer.add_scalar(\"Critic loss\", critic_loss, iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating offscreen glfw\n"
     ]
    }
   ],
   "source": [
    "GlfwContext(offscreen=True)\n",
    "\n",
    "\n",
    "class Play:\n",
    "    def __init__(self, env, agent, env_name, max_episode=1):\n",
    "        self.env = env\n",
    "        self.max_episode = max_episode\n",
    "        self.agent = agent\n",
    "        _, self.state_rms_mean, self.state_rms_var = self.agent.load_weights()\n",
    "        self.agent.set_to_eval_mode()\n",
    "        self.device = torch.device(process_device)\n",
    "        self.fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "        self.VideoWriter = cv2.VideoWriter(env_name + \".avi\", self.fourcc, 50.0, (250, 250))\n",
    "\n",
    "    def evaluate(self):\n",
    "\n",
    "        for _ in range(self.max_episode):\n",
    "            s = self.env.reset()\n",
    "            s = set_state(s)\n",
    "            episode_reward = 0\n",
    "            for _ in range(self.env._max_episode_steps):\n",
    "                s = np.clip((s - self.state_rms_mean) / (self.state_rms_var ** 0.5 + 1e-8), -5.0, 5.0)\n",
    "                dist = self.agent.choose_dist(s)\n",
    "                action = dist.sample().cpu().numpy()[0]\n",
    "                s_, r, done, _ = self.env.step(action)\n",
    "                s_ = set_state(s_)\n",
    "                episode_reward += r\n",
    "                if done:\n",
    "                    break\n",
    "                s = s_\n",
    "                # self.env.render(mode=\"human\")\n",
    "                # self.env.viewer.cam.type = const.CAMERA_FIXED\n",
    "                # self.env.viewer.cam.fixedcamid = 0\n",
    "                # time.sleep(0.03)\n",
    "                I = self.env.render(mode='rgb_array')\n",
    "                I = cv2.cvtColor(I, cv2.COLOR_RGB2BGR)\n",
    "                I = cv2.resize(I, (250, 250))\n",
    "                self.VideoWriter.write(I)\n",
    "                # cv2.imshow(\"env\", I)\n",
    "                # cv2.waitKey(10)\n",
    "            print(f\"episode reward:{episode_reward:3.3f}\")\n",
    "        self.env.close()\n",
    "        self.VideoWriter.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"FetchPickAndPlace\"\n",
    "TRAIN_FLAG = True\n",
    "test_env = gym.make(ENV_NAME + \"-v1\")\n",
    "\n",
    "n_states = test_env.observation_space[\"observation\"].shape[0]\n",
    "n_achieveds = test_env.observation_space[\"achieved_goal\"].shape[0]\n",
    "n_goals = test_env.observation_space[\"desired_goal\"].shape[0]\n",
    "action_bounds = [test_env.action_space.low[0], test_env.action_space.high[0]]\n",
    "n_actions = test_env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried to use the environment without changing loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"number of states:{n_states}\\n\"\n",
    "      f\"action bounds:{action_bounds}\\n\"\n",
    "      f\"number of actions:{n_actions}\")\n",
    "\n",
    "if not os.path.exists(ENV_NAME):\n",
    "    os.mkdir(ENV_NAME)\n",
    "    os.mkdir(ENV_NAME + \"/logs\")\n",
    "\n",
    "env = gym.make(ENV_NAME + \"-v1\")\n",
    "\n",
    "agent = Agent(n_states=n_states + n_goals,\n",
    "              n_iter=n_iterations,\n",
    "              env_name=ENV_NAME,\n",
    "              action_bounds=action_bounds,\n",
    "              n_actions=n_actions,\n",
    "              lr=lr)\n",
    "if TRAIN_FLAG:\n",
    "    trainer = Train(env=env,\n",
    "                    test_env=test_env,\n",
    "                    env_name=ENV_NAME,\n",
    "                    agent=agent,\n",
    "                    horizon=T,\n",
    "                    n_iterations=n_iterations,\n",
    "                    epochs=epochs,\n",
    "                    mini_batch_size=mini_batch_size,\n",
    "                    epsilon=clip_range)\n",
    "    trainer.step()\n",
    "\n",
    "player = Play(env, agent, ENV_NAME)\n",
    "player.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As from the result, the output looks not so good. This may from the normalization technic or loss function and the way of the PPO may not good enough for the FetchPickAndPlace.\n",
    "\n",
    "Let's see another algorithm **DDPG**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Deterministic Policy Gradients (DDPG)\n",
    "\n",
    "DDPG is proposed by Deepmind in the paper [Continuous Control With Deep Reinforcement Learning](https://arxiv.org/abs/1509.02971).\n",
    "\n",
    "### DDPG models\n",
    "\n",
    "DDPG uses four neural networks: a Q network, a deterministic policy network, a target Q network, and a target policy network.\n",
    "\n",
    "- $\\theta^Q$: Q network\n",
    "- $\\theta^{\\mu}$: Deterministic policy function\n",
    "- $\\theta^{Q'}$: target Q network\n",
    "- $\\theta^{\\mu'}$: target policy network\n",
    "\n",
    "The Q network and policy network are similar to those in A2C, but in DDPG,\n",
    "the actor directly maps states to actions (the output of the network directly the output) instead of outputting the probability distribution across a discrete action space.\n",
    "\n",
    "The target networks are time-delayed copies of the original networks, slowly tracking the learned networks.\n",
    "Using target value networks greatly improves the stability of learning by avoiding interdependent\n",
    "updates to the value network and resulting divergence.\n",
    "\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha[R(s,a) + \\gamma \\max Q'(s',a') - Q(s,a)]$$\n",
    "\n",
    "We can use the same actor and critic models for the deterministic policy network and the $Q$ network as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def init_weights_biases(size):\n",
    "    v = 1.0 / np.sqrt(size[0])\n",
    "    return torch.FloatTensor(size).uniform_(-v, v)\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, n_states, n_actions, n_goals, n_hidden1=256, n_hidden2=256, n_hidden3=256, initial_w=3e-3):\n",
    "        self.n_states = n_states[0]\n",
    "        self.n_actions = n_actions\n",
    "        self.n_goals = n_goals\n",
    "        self.n_hidden1 = n_hidden1\n",
    "        self.n_hidden2 = n_hidden2\n",
    "        self.n_hidden3 = n_hidden3\n",
    "        self.initial_w = initial_w\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=self.n_states + self.n_goals, out_features=self.n_hidden1)\n",
    "        self.fc2 = nn.Linear(in_features=self.n_hidden1, out_features=self.n_hidden2)\n",
    "        self.fc3 = nn.Linear(in_features=self.n_hidden2, out_features=self.n_hidden3)\n",
    "        self.output = nn.Linear(in_features=self.n_hidden3, out_features=self.n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        output = torch.tanh(self.output(x))  # TODO add scale of the action\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, n_states, n_goals, n_hidden1=256, n_hidden2=256, n_hidden3=256, initial_w=3e-3, action_size=1):\n",
    "        self.n_states = n_states[0]\n",
    "        self.n_goals = n_goals\n",
    "        self.n_hidden1 = n_hidden1\n",
    "        self.n_hidden2 = n_hidden2\n",
    "        self.n_hidden3 = n_hidden3\n",
    "        self.initial_w = initial_w\n",
    "        self.action_size = action_size\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=self.n_states + self.n_goals + self.action_size, out_features=self.n_hidden1)\n",
    "        self.fc2 = nn.Linear(in_features=self.n_hidden1, out_features=self.n_hidden2)\n",
    "        self.fc3 = nn.Linear(in_features=self.n_hidden2, out_features=self.n_hidden3)\n",
    "        self.output = nn.Linear(in_features=self.n_hidden3, out_features=1)\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        x = F.relu(self.fc1(torch.cat([x, a], dim=-1)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        output = self.output(x)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO vs DDPG\n",
    "\n",
    "\n",
    "PPO actor-critic objective functions are based on a set of trajectories obtained by running the current policy over T timesteps. After the policy is updated, trajectories generated from old/stale policies are no longer applicable. i.e. it needs to be trained \"on-policy\".\n",
    "\n",
    "(Why? Because PPO uses a stochastic policy (i.e. a conditional probability distribution of actions given states) and the policy's objective function is based on sampling from trajectories from a probability distribution that depends the current policy's probability distribution (i.e. you need to use the current policy to generate the trajectories). NOTE that this is true for any policy gradients approach using a stochastic policy, not just PPO.)\n",
    "\n",
    "DDPG/TD3 only needs a single timestep for each actor / critic update (via Bellman equation) and it is straightforward to apply the current deterministic policy to old data tuples $(s_t, a_t, r_t, s_{t+1})$. i.e. it is trained \"off-policy\".\n",
    "\n",
    "(WHY? Because DDPG/TD3 use a deterministic policy and Silver, David, et al. \"Deterministic policy gradient algorithms.\" 2014. proved that the policy's objective function is an expectation value of state trajectories from the Markov Decision Process state transition function...but does not depend on the probability distribution induced by the policy, which after all is deterministic not stochastic.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting PPO or DDPG\n",
    "\n",
    "If the environment is expensive to sample from, use DDPG or SAC, since they're more sample efficient. If it's cheap to sample from, using PPO or a REINFORCE-based algorithm, since they're straightforward to implement, robust to hyperparameters, and easy to get working. You'll spend less wall-clock time training a PPO-like algorithm in a cheap environment.\n",
    "\n",
    "If you need to decide between DDPG and SAC, choose TD3. The performance of SAC and DDPG is nearly identical when you compare on the basis of whether or not a twin delayed update is used. SAC can be troublesome to get working, and the temperature parameter controls the stochasticity of your final policy -- effectively, it means your reward scheme can give you a policy that is too random to be useful, and picking a temperature parameter isn't necessarily straightforward. TD3 is almost the same as SAC, but noise injection is often easier to visualize and tune than setting the right temperature parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning DDPG\n",
    "\n",
    "Here is the pseudo code for DDPG.\n",
    "\n",
    "<img src=\"img/ddpg_pseudo.png\" title=\"The A2C architecture\" style=\"width: 600px;\" />\n",
    "\n",
    "The important aspects of DDPG are\n",
    "1. Experience replay\n",
    "2. Actor and critic network updates\n",
    "3. Target network updates\n",
    "4. Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Buffer\n",
    "As used in Deep Q learning (and many other RL algorithms), DDPG also uses a replay buffer to sample experience to update neural network parameters. During each trajectory roll-out, we save all the experience tuples (state, action, reward, next_state) and store them in a finite-sized cache  a replay buffer. Then, we sample random mini-batches of experience from the replay buffer when we update the value and policy networks.\n",
    "\n",
    "In optimization tasks, we want the data to be independently distributed. This fails to be the case when we optimize a sequential decision process in an on-policy way, because the data then would not be independent of each other. When we store them in a replay buffer and take random batches for training, we overcome this issue.\n",
    "\n",
    "### Hindsight Experience Replay (HER)\n",
    "\n",
    "Reference: https://arxiv.org/abs/1707.01495\n",
    "\n",
    "One ability humans have is to learn from our mistakes and adjust next time to avoid making the same mistake. We can apply the same concept to our reinforcement learning algorithm.\n",
    "\n",
    "Now this time, instead of concluding that the course of action you took was useless because you didnt score a goal, what if you say that maybe it didnt teach you how to score a goal, but it certainly taught you how NOT to shoot the puck. Or more precisely, what if you say it taught you how to shoot the puck slightly to the right side of the net? Now you can learn not only how to shoot towards the right, but you learn something new that might help you achieve the final goal every run. This is the idea behind Hindsight Experience Replay (HER).\n",
    "\n",
    "The HER procedure looks like this:\n",
    "1. Run your policy and store everything you observe (state and goal, action, reward, next state and goal) tuple into an experience buffer.\n",
    "2. Sample K goals from the states visited in the episode obtained at step 1, and for each goal store (state and sampled goal, action, reward with respect to the sampled goal, next state and sampled goal) tuple into the buffer.\n",
    "3. Repeat step 12 N times.\n",
    "4. Sample M number of experience tuples (batch) from the buffer and train the network with the said batch experience. (Do this B times)\n",
    "\n",
    "How does this help? By adding goal into the input space we are stating that there are multiple goals for our agent to observe. The new Q-function indicates how good taking each action is, given the current state, to achieving the current goal. And because we are sampling the new goals from the episode, the goals are now nodes that have been visited by our agent. So even if our agent fails to achieve the main goal in this episode, it has reached some states. By using those states as the new goal, now the agent can be trained with positive (or non negative) rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy as dc\n",
    "import random\n",
    "\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self, capacity, k_future, env):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.memory_counter = 0\n",
    "        self.memory_length = 0\n",
    "        self.env = env\n",
    "\n",
    "        self.future_p = 1 - (1. / (1 + k_future))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "\n",
    "        ep_indices = np.random.randint(0, len(self.memory), batch_size)\n",
    "        time_indices = np.random.randint(0, len(self.memory[0][\"next_state\"]), batch_size)\n",
    "        states = []\n",
    "        actions = []\n",
    "        desired_goals = []\n",
    "        next_states = []\n",
    "        next_achieved_goals = []\n",
    "\n",
    "        for episode, timestep in zip(ep_indices, time_indices):\n",
    "            states.append(dc(self.memory[episode][\"state\"][timestep]))\n",
    "            actions.append(dc(self.memory[episode][\"action\"][timestep]))\n",
    "            desired_goals.append(dc(self.memory[episode][\"desired_goal\"][timestep]))\n",
    "            next_achieved_goals.append(dc(self.memory[episode][\"next_achieved_goal\"][timestep]))\n",
    "            next_states.append(dc(self.memory[episode][\"next_state\"][timestep]))\n",
    "\n",
    "        states = np.vstack(states)\n",
    "        actions = np.vstack(actions)\n",
    "        desired_goals = np.vstack(desired_goals)\n",
    "        next_achieved_goals = np.vstack(next_achieved_goals)\n",
    "        next_states = np.vstack(next_states)\n",
    "\n",
    "        her_indices = np.where(np.random.uniform(size=batch_size) < self.future_p)\n",
    "        future_offset = np.random.uniform(size=batch_size) * (len(self.memory[0][\"next_state\"]) - time_indices)\n",
    "        future_offset = future_offset.astype(int)\n",
    "        future_t = (time_indices + 1 + future_offset)[her_indices]\n",
    "\n",
    "        future_ag = []\n",
    "        for episode, f_offset in zip(ep_indices[her_indices], future_t):\n",
    "            future_ag.append(dc(self.memory[episode][\"achieved_goal\"][f_offset]))\n",
    "        future_ag = np.vstack(future_ag)\n",
    "\n",
    "        desired_goals[her_indices] = future_ag\n",
    "        rewards = np.expand_dims(self.env.compute_reward(next_achieved_goals, desired_goals, None), 1)\n",
    "\n",
    "        return self.clip_obs(states), actions, rewards, self.clip_obs(next_states), self.clip_obs(desired_goals)\n",
    "\n",
    "    def add(self, transition):\n",
    "        self.memory.append(transition)\n",
    "        if len(self.memory) > self.capacity:\n",
    "            self.memory.pop(0)\n",
    "        assert len(self.memory) <= self.capacity\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    @staticmethod\n",
    "    def clip_obs(x):\n",
    "        return np.clip(x, -200, 200)\n",
    "\n",
    "    def sample_for_normalization(self, batch):\n",
    "        size = len(batch[0][\"next_state\"])\n",
    "        ep_indices = np.random.randint(0, len(batch), size)\n",
    "        time_indices = np.random.randint(0, len(batch[0][\"next_state\"]), size)\n",
    "        states = []\n",
    "        desired_goals = []\n",
    "\n",
    "        for episode, timestep in zip(ep_indices, time_indices):\n",
    "            states.append(dc(batch[episode][\"state\"][timestep]))\n",
    "            desired_goals.append(dc(batch[episode][\"desired_goal\"][timestep]))\n",
    "\n",
    "        states = np.vstack(states)\n",
    "        desired_goals = np.vstack(desired_goals)\n",
    "\n",
    "        her_indices = np.where(np.random.uniform(size=size) < self.future_p)\n",
    "        future_offset = np.random.uniform(size=size) * (len(batch[0][\"next_state\"]) - time_indices)\n",
    "        future_offset = future_offset.astype(int)\n",
    "        future_t = (time_indices + 1 + future_offset)[her_indices]\n",
    "\n",
    "        future_ag = []\n",
    "        for episode, f_offset in zip(ep_indices[her_indices], future_t):\n",
    "            future_ag.append(dc(batch[episode][\"achieved_goal\"][f_offset]))\n",
    "        future_ag = np.vstack(future_ag)\n",
    "\n",
    "        desired_goals[her_indices] = future_ag\n",
    "\n",
    "        return self.clip_obs(states), self.clip_obs(desired_goals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor (Policy) & Critic (Value) Network Updates\n",
    "The value network is updated similarly as is done in Q-learning. The updated Q value is obtained by the Bellman equation:\n",
    "\n",
    "$$y_i=r_i + \\gamma Q'(s_{i+1}, \\mu'(s_{i+1}|\\theta^{\\mu'})|\\theta^{Q'})$$\n",
    "\n",
    "However, in DDPG, the next-state Q values are calculated with the target value network and target policy network. Then, we minimize the mean-squared loss between the updated Q value and the original Q value:\n",
    "\n",
    "$$\\mathcal{L} = \\frac{1}{N}\\sum(y_i-Q(s_i,a_i|\\theta^Q))^2$$\n",
    "\n",
    "Note that the original Q value is calculated with the value network, not the target value network.\n",
    "\n",
    "For the policy function, our objective is to maximize the expected return:\n",
    "\n",
    "$$J(\\theta)=\\mathbb{E}[Q(s,a)|_{s=s_t, a_t=\\mu(s_t)}]$$\n",
    "\n",
    "To calculate the policy loss, we take the derivative of the objective function with respect to the policy parameter. Keep in mind that the actor (policy) function is differentiable, so we have to apply the chain rule.\n",
    "\n",
    "$$\\nabla_{\\theta^\\mu}J(\\theta) \\approx \\nabla_a Q(s,a) \\nabla_{\\theta^\\mu}\\mu(s|\\theta^\\mu)$$\n",
    "\n",
    "But since we are updating the policy in an off-policy way with batches of experience, we take the mean of the sum of gradients calculated from the mini-batch:\n",
    "\n",
    "$$\\nabla_{\\theta^\\mu}J(\\theta) \\approx \\frac{1}{N}\\sum[\\nabla_a Q(s,a|\\theta^Q)|_{s=s_i,a=\\mu(s_i)} \\nabla_{\\theta^\\mu}\\mu(s|\\theta^\\mu)|_{s=s_i}]$$\n",
    "\n",
    "### Target Network Updates\n",
    "We make a copy of the target network parameters and have them slowly track those of the learned networks via soft updates, as illustrated below:\n",
    "\n",
    "$$\\theta^{Q'} \\leftarrow \\tau \\theta^Q + (1-\\tau)\\theta^{Q'}$$\n",
    "\n",
    "$$\\theta^{\\mu'} \\leftarrow \\tau \\theta^\\mu + (1-\\tau)\\theta^{\\mu'}$$\n",
    "\n",
    "Where $\\tau < 1$\n",
    "\n",
    "### Exploration\n",
    "In Reinforcement learning for discrete action spaces, exploration is done via probabilistically selecting a random action (such as epsilon-greedy or Boltzmann exploration). For continuous action spaces, exploration is done via adding noise to the action itself (there is also the parameter space noise but we will skip that for now). In the DDPG paper, the authors use Ornstein-Uhlenbeck Process to add noise to the action output:\n",
    "\n",
    "$$\\mu'(s_t)=\\mu(s_t|\\theta_t^\\mu)+\\mathcal{N}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that <code>mpi4py</code> can be installed using pip:\n",
    "\n",
    "<code>pip install mpi4py</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's make normalizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "class Normalizer:\n",
    "    def __init__(self, size, eps=1e-2, default_clip_range=np.inf):\n",
    "        self.size = size\n",
    "        self.eps = eps\n",
    "        self.default_clip_range = default_clip_range\n",
    "        # some local information\n",
    "        self.local_sum = np.zeros(self.size, np.float32)\n",
    "        self.local_sumsq = np.zeros(self.size, np.float32)\n",
    "        self.local_count = np.zeros(1, np.float32)\n",
    "        # get the total sum sumsq and sum count\n",
    "        self.total_sum = np.zeros(self.size, np.float32)\n",
    "        self.total_sumsq = np.zeros(self.size, np.float32)\n",
    "        self.total_count = np.ones(1, np.float32)\n",
    "        # get the mean and std\n",
    "        self.mean = np.zeros(self.size, np.float32)\n",
    "        self.std = np.ones(self.size, np.float32)\n",
    "        # thread locker\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    # update the parameters of the normalizer\n",
    "    def update(self, v):\n",
    "        v = v.reshape(-1, self.size)\n",
    "        # do the computing\n",
    "        with self.lock:\n",
    "            self.local_sum += v.sum(axis=0)\n",
    "            self.local_sumsq += (np.square(v)).sum(axis=0)\n",
    "            self.local_count[0] += v.shape[0]\n",
    "\n",
    "    # sync the parameters across the cpus\n",
    "    def sync(self, local_sum, local_sumsq, local_count):\n",
    "        local_sum[...] = self._mpi_average(local_sum)\n",
    "        local_sumsq[...] = self._mpi_average(local_sumsq)\n",
    "        local_count[...] = self._mpi_average(local_count)\n",
    "        return local_sum, local_sumsq, local_count\n",
    "\n",
    "    def recompute_stats(self):\n",
    "        with self.lock:\n",
    "            local_count = self.local_count.copy()\n",
    "            local_sum = self.local_sum.copy()\n",
    "            local_sumsq = self.local_sumsq.copy()\n",
    "            # reset\n",
    "            self.local_count[...] = 0\n",
    "            self.local_sum[...] = 0\n",
    "            self.local_sumsq[...] = 0\n",
    "        # sync the stats\n",
    "        sync_sum, sync_sumsq, sync_count = self.sync(local_sum, local_sumsq, local_count)\n",
    "        # update the total stuff\n",
    "        self.total_sum += sync_sum\n",
    "        self.total_sumsq += sync_sumsq\n",
    "        self.total_count += sync_count\n",
    "        # calculate the new mean and std\n",
    "        self.mean = self.total_sum / self.total_count\n",
    "        self.std = np.sqrt(np.maximum(np.square(self.eps), (self.total_sumsq / self.total_count) - np.square(\n",
    "            self.total_sum / self.total_count)))\n",
    "\n",
    "    # average across the cpu's data\n",
    "    def _mpi_average(self, x):\n",
    "        buf = np.zeros_like(x)\n",
    "        MPI.COMM_WORLD.Allreduce(x, buf, op=MPI.SUM)\n",
    "        buf /= MPI.COMM_WORLD.Get_size()\n",
    "        return buf\n",
    "\n",
    "    # normalize the observation\n",
    "    def normalize(self, v, clip_range=None):\n",
    "        if clip_range is None:\n",
    "            clip_range = self.default_clip_range\n",
    "        return np.clip((v - self.mean) / self.std, -clip_range, clip_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent class\n",
    "\n",
    "The core of DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import from_numpy, device\n",
    "import numpy as np\n",
    "# from models import Actor, Critic\n",
    "# from memory import Memory\n",
    "from torch.optim import Adam\n",
    "from mpi4py import MPI\n",
    "# from normalizer import Normalizer\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, n_states, n_actions, n_goals, action_bounds, capacity, env,\n",
    "                 k_future,\n",
    "                 batch_size,\n",
    "                 action_size=1,\n",
    "                 tau=0.05,\n",
    "                 actor_lr=1e-3,\n",
    "                 critic_lr=1e-3,\n",
    "                 gamma=0.98):\n",
    "        self.device = device(\"cpu\")\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.n_goals = n_goals\n",
    "        self.k_future = k_future\n",
    "        self.action_bounds = action_bounds\n",
    "        self.action_size = action_size\n",
    "        self.env = env\n",
    "\n",
    "        self.actor = Actor(self.n_states, n_actions=self.n_actions, n_goals=self.n_goals).to(self.device)\n",
    "        self.critic = Critic(self.n_states, action_size=self.action_size, n_goals=self.n_goals).to(self.device)\n",
    "        self.sync_networks(self.actor)\n",
    "        self.sync_networks(self.critic)\n",
    "        self.actor_target = Actor(self.n_states, n_actions=self.n_actions, n_goals=self.n_goals).to(self.device)\n",
    "        self.critic_target = Critic(self.n_states, action_size=self.action_size, n_goals=self.n_goals).to(self.device)\n",
    "        self.init_target_networks()\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.capacity = capacity\n",
    "        self.memory = Memory(self.capacity, self.k_future, self.env)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.actor_lr = actor_lr\n",
    "        self.critic_lr = critic_lr\n",
    "        self.actor_optim = Adam(self.actor.parameters(), self.actor_lr)\n",
    "        self.critic_optim = Adam(self.critic.parameters(), self.critic_lr)\n",
    "\n",
    "        self.state_normalizer = Normalizer(self.n_states[0], default_clip_range=5)\n",
    "        self.goal_normalizer = Normalizer(self.n_goals, default_clip_range=5)\n",
    "\n",
    "    def choose_action(self, state, goal, train_mode=True):\n",
    "        state = self.state_normalizer.normalize(state)\n",
    "        goal = self.goal_normalizer.normalize(goal)\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        goal = np.expand_dims(goal, axis=0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = np.concatenate([state, goal], axis=1)\n",
    "            x = from_numpy(x).float().to(self.device)\n",
    "            action = self.actor(x)[0].cpu().data.numpy()\n",
    "\n",
    "        if train_mode:\n",
    "            action += 0.2 * np.random.randn(self.n_actions)\n",
    "            action = np.clip(action, self.action_bounds[0], self.action_bounds[1])\n",
    "\n",
    "            random_actions = np.random.uniform(low=self.action_bounds[0], high=self.action_bounds[1],\n",
    "                                               size=self.n_actions)\n",
    "            action += np.random.binomial(1, 0.3, 1)[0] * (random_actions - action)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def store(self, mini_batch):\n",
    "        for batch in mini_batch:\n",
    "            self.memory.add(batch)\n",
    "        self._update_normalizer(mini_batch)\n",
    "\n",
    "    def init_target_networks(self):\n",
    "        self.hard_update_networks(self.actor, self.actor_target)\n",
    "        self.hard_update_networks(self.critic, self.critic_target)\n",
    "\n",
    "    @staticmethod\n",
    "    def hard_update_networks(local_model, target_model):\n",
    "        target_model.load_state_dict(local_model.state_dict())\n",
    "\n",
    "    @staticmethod\n",
    "    def soft_update_networks(local_model, target_model, tau=0.05):\n",
    "        for t_params, e_params in zip(target_model.parameters(), local_model.parameters()):\n",
    "            t_params.data.copy_(tau * e_params.data + (1 - tau) * t_params.data)\n",
    "\n",
    "    def train(self):\n",
    "        states, actions, rewards, next_states, goals = self.memory.sample(self.batch_size)\n",
    "\n",
    "        states = self.state_normalizer.normalize(states)\n",
    "        next_states = self.state_normalizer.normalize(next_states)\n",
    "        goals = self.goal_normalizer.normalize(goals)\n",
    "        inputs = np.concatenate([states, goals], axis=1)\n",
    "        next_inputs = np.concatenate([next_states, goals], axis=1)\n",
    "\n",
    "        inputs = torch.Tensor(inputs).to(self.device)\n",
    "        rewards = torch.Tensor(rewards).to(self.device)\n",
    "        next_inputs = torch.Tensor(next_inputs).to(self.device)\n",
    "        actions = torch.Tensor(actions).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            target_q = self.critic_target(next_inputs, self.actor_target(next_inputs))\n",
    "            target_returns = rewards + self.gamma * target_q.detach()\n",
    "            target_returns = torch.clamp(target_returns, -1 / (1 - self.gamma), 0)\n",
    "\n",
    "        q_eval = self.critic(inputs, actions)\n",
    "        critic_loss = (target_returns - q_eval).pow(2).mean()\n",
    "\n",
    "        a = self.actor(inputs)\n",
    "        actor_loss = -self.critic(inputs, a).mean()\n",
    "        actor_loss += a.pow(2).mean()\n",
    "\n",
    "        self.actor_optim.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.sync_grads(self.actor)\n",
    "        self.actor_optim.step()\n",
    "\n",
    "        self.critic_optim.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.sync_grads(self.critic)\n",
    "        self.critic_optim.step()\n",
    "\n",
    "        return actor_loss.item(), critic_loss.item()\n",
    "\n",
    "    def save_weights(self):\n",
    "        torch.save({\"actor_state_dict\": self.actor.state_dict(),\n",
    "                    \"state_normalizer_mean\": self.state_normalizer.mean,\n",
    "                    \"state_normalizer_std\": self.state_normalizer.std,\n",
    "                    \"goal_normalizer_mean\": self.goal_normalizer.mean,\n",
    "                    \"goal_normalizer_std\": self.goal_normalizer.std}, \"FetchPickAndPlace.pth\")\n",
    "\n",
    "    def load_weights(self):\n",
    "\n",
    "        checkpoint = torch.load(\"FetchPickAndPlace.pth\")\n",
    "        actor_state_dict = checkpoint[\"actor_state_dict\"]\n",
    "        self.actor.load_state_dict(actor_state_dict)\n",
    "        state_normalizer_mean = checkpoint[\"state_normalizer_mean\"]\n",
    "        self.state_normalizer.mean = state_normalizer_mean\n",
    "        state_normalizer_std = checkpoint[\"state_normalizer_std\"]\n",
    "        self.state_normalizer.std = state_normalizer_std\n",
    "        goal_normalizer_mean = checkpoint[\"goal_normalizer_mean\"]\n",
    "        self.goal_normalizer.mean = goal_normalizer_mean\n",
    "        goal_normalizer_std = checkpoint[\"goal_normalizer_std\"]\n",
    "        self.goal_normalizer.std = goal_normalizer_std\n",
    "\n",
    "    def set_to_eval_mode(self):\n",
    "        self.actor.eval()\n",
    "        # self.critic.eval()\n",
    "\n",
    "    def update_networks(self):\n",
    "        self.soft_update_networks(self.actor, self.actor_target, self.tau)\n",
    "        self.soft_update_networks(self.critic, self.critic_target, self.tau)\n",
    "\n",
    "    def _update_normalizer(self, mini_batch):\n",
    "        states, goals = self.memory.sample_for_normalization(mini_batch)\n",
    "\n",
    "        self.state_normalizer.update(states)\n",
    "        self.goal_normalizer.update(goals)\n",
    "        self.state_normalizer.recompute_stats()\n",
    "        self.goal_normalizer.recompute_stats()\n",
    "\n",
    "    @staticmethod\n",
    "    def sync_networks(network):\n",
    "        comm = MPI.COMM_WORLD\n",
    "        flat_params = _get_flat_params_or_grads(network, mode='params')\n",
    "        comm.Bcast(flat_params, root=0)\n",
    "        _set_flat_params_or_grads(network, flat_params, mode='params')\n",
    "\n",
    "    @staticmethod\n",
    "    def sync_grads(network):\n",
    "        flat_grads = _get_flat_params_or_grads(network, mode='grads')\n",
    "        comm = MPI.COMM_WORLD\n",
    "        global_grads = np.zeros_like(flat_grads)\n",
    "        comm.Allreduce(flat_grads, global_grads, op=MPI.SUM)\n",
    "        _set_flat_params_or_grads(network, global_grads, mode='grads')\n",
    "\n",
    "\n",
    "def _get_flat_params_or_grads(network, mode='params'):\n",
    "    attr = 'data' if mode == 'params' else 'grad'\n",
    "    return np.concatenate([getattr(param, attr).cpu().numpy().flatten() for param in network.parameters()])\n",
    "\n",
    "\n",
    "def _set_flat_params_or_grads(network, flat_params, mode='params'):\n",
    "    attr = 'data' if mode == 'params' else 'grad'\n",
    "    pointer = 0\n",
    "    for param in network.parameters():\n",
    "        getattr(param, attr).copy_(\n",
    "            torch.tensor(flat_params[pointer:pointer + param.data.numel()]).view_as(param.data))\n",
    "        pointer += param.data.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play class (run and record the vdo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating offscreen glfw\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import device\n",
    "import numpy as np\n",
    "import cv2\n",
    "from gym import wrappers\n",
    "from mujoco_py import GlfwContext\n",
    "\n",
    "GlfwContext(offscreen=True)\n",
    "\n",
    "from mujoco_py.generated import const\n",
    "\n",
    "\n",
    "class Play:\n",
    "    def __init__(self, env, agent, max_episode=4):\n",
    "        self.env = env\n",
    "        self.env = wrappers.Monitor(env, \"./videos\", video_callable=lambda episode_id: True, force=True)\n",
    "        self.max_episode = max_episode\n",
    "        self.agent = agent\n",
    "        self.agent.load_weights()\n",
    "        self.agent.set_to_eval_mode()\n",
    "        self.device = device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def evaluate(self):\n",
    "\n",
    "        for _ in range(self.max_episode):\n",
    "            env_dict = self.env.reset()\n",
    "            state = env_dict[\"observation\"]\n",
    "            achieved_goal = env_dict[\"achieved_goal\"]\n",
    "            desired_goal = env_dict[\"desired_goal\"]\n",
    "            while np.linalg.norm(achieved_goal - desired_goal) <= 0.05:\n",
    "                env_dict = self.env.reset()\n",
    "                state = env_dict[\"observation\"]\n",
    "                achieved_goal = env_dict[\"achieved_goal\"]\n",
    "                desired_goal = env_dict[\"desired_goal\"]\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            while not done:\n",
    "                action = self.agent.choose_action(state, desired_goal, train_mode=False)\n",
    "                next_env_dict, r, done, _ = self.env.step(action)\n",
    "                next_state = next_env_dict[\"observation\"]\n",
    "                next_desired_goal = next_env_dict[\"desired_goal\"]\n",
    "                episode_reward += r\n",
    "                state = next_state.copy()\n",
    "                desired_goal = next_desired_goal.copy()\n",
    "                I = self.env.render(mode=\"rgb_array\")  # mode = \"rgb_array\n",
    "                self.env.viewer.cam.type = const.CAMERA_FREE\n",
    "                self.env.viewer.cam.fixedcamid = 0\n",
    "                # I = cv2.cvtColor(I, cv2.COLOR_RGB2BGR)\n",
    "                # cv2.imshow(\"I\", I)\n",
    "                # cv2.waitKey(2)\n",
    "            print(f\"episode_reward:{episode_reward:3.3f}\")\n",
    "\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "# from agent import Agent\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "# from play import Play\n",
    "import mujoco_py\n",
    "import random\n",
    "from mpi4py import MPI\n",
    "import psutil\n",
    "import time\n",
    "from copy import deepcopy as dc\n",
    "import os\n",
    "import torch\n",
    "\n",
    "ENV_NAME = \"FetchPickAndPlace-v1\"\n",
    "INTRO = False\n",
    "Train = True\n",
    "Play_FLAG = False\n",
    "MAX_EPOCHS = 250\n",
    "MAX_CYCLES = 50\n",
    "num_updates = 40\n",
    "MAX_EPISODES = 2\n",
    "memory_size = 7e+5 // 50\n",
    "batch_size = 256\n",
    "actor_lr = 1e-3\n",
    "critic_lr = 1e-3\n",
    "gamma = 0.98\n",
    "tau = 0.05\n",
    "k_future = 4\n",
    "\n",
    "test_env = gym.make(ENV_NAME)\n",
    "state_shape = test_env.observation_space.spaces[\"observation\"].shape\n",
    "n_actions = test_env.action_space.shape[0]\n",
    "n_goals = test_env.observation_space.spaces[\"desired_goal\"].shape[0]\n",
    "action_bounds = [test_env.action_space.low[0], test_env.action_space.high[0]]\n",
    "to_gb = lambda in_bytes: in_bytes / 1024 / 1024 / 1024\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ['IN_MPI'] = '1'\n",
    "\n",
    "\n",
    "def eval_agent(env_, agent_):\n",
    "    total_success_rate = []\n",
    "    running_r = []\n",
    "    for ep in range(10):\n",
    "        per_success_rate = []\n",
    "        env_dictionary = env_.reset()\n",
    "        s = env_dictionary[\"observation\"]\n",
    "        ag = env_dictionary[\"achieved_goal\"]\n",
    "        g = env_dictionary[\"desired_goal\"]\n",
    "        while np.linalg.norm(ag - g) <= 0.05:\n",
    "            env_dictionary = env_.reset()\n",
    "            s = env_dictionary[\"observation\"]\n",
    "            ag = env_dictionary[\"achieved_goal\"]\n",
    "            g = env_dictionary[\"desired_goal\"]\n",
    "        ep_r = 0\n",
    "        for t in range(50):\n",
    "            with torch.no_grad():\n",
    "                a = agent_.choose_action(s, g, train_mode=False)\n",
    "            observation_new, r, _, info_ = env_.step(a)\n",
    "            s = observation_new['observation']\n",
    "            g = observation_new['desired_goal']\n",
    "            per_success_rate.append(info_['is_success'])\n",
    "            ep_r += r\n",
    "        total_success_rate.append(per_success_rate)\n",
    "        if ep == 0:\n",
    "            running_r.append(ep_r)\n",
    "        else:\n",
    "            running_r.append(running_r[-1] * 0.99 + 0.01 * ep_r)\n",
    "    total_success_rate = np.array(total_success_rate)\n",
    "    local_success_rate = np.mean(total_success_rate[:, -1])\n",
    "    global_success_rate = MPI.COMM_WORLD.allreduce(local_success_rate, op=MPI.SUM)\n",
    "    return global_success_rate / MPI.COMM_WORLD.Get_size(), running_r, ep_r\n",
    "\n",
    "\n",
    "if INTRO:\n",
    "    print(f\"state_shape:{state_shape[0]}\\n\"\n",
    "          f\"number of actions:{n_actions}\\n\"\n",
    "          f\"action boundaries:{action_bounds}\\n\"\n",
    "          f\"max timesteps:{test_env._max_episode_steps}\")\n",
    "    for _ in range(3):\n",
    "        done = False\n",
    "        test_env.reset()\n",
    "        while not done:\n",
    "            action = test_env.action_space.sample()\n",
    "            test_state, test_reward, test_done, test_info = test_env.step(action)\n",
    "            # substitute_goal = test_state[\"achieved_goal\"].copy()\n",
    "            # substitute_reward = test_env.compute_reward(\n",
    "            #     test_state[\"achieved_goal\"], substitute_goal, test_info)\n",
    "            # print(\"r is {}, substitute_reward is {}\".format(r, substitute_reward))\n",
    "            test_env.render()\n",
    "    exit(0)\n",
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "env.seed(MPI.COMM_WORLD.Get_rank())\n",
    "random.seed(MPI.COMM_WORLD.Get_rank())\n",
    "np.random.seed(MPI.COMM_WORLD.Get_rank())\n",
    "torch.manual_seed(MPI.COMM_WORLD.Get_rank())\n",
    "agent = Agent(n_states=state_shape,\n",
    "              n_actions=n_actions,\n",
    "              n_goals=n_goals,\n",
    "              action_bounds=action_bounds,\n",
    "              capacity=memory_size,\n",
    "              action_size=n_actions,\n",
    "              batch_size=batch_size,\n",
    "              actor_lr=actor_lr,\n",
    "              critic_lr=critic_lr,\n",
    "              gamma=gamma,\n",
    "              tau=tau,\n",
    "              k_future=k_future,\n",
    "              env=dc(env))\n",
    "if Train:\n",
    "\n",
    "    t_success_rate = []\n",
    "    total_ac_loss = []\n",
    "    total_cr_loss = []\n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        start_time = time.time()\n",
    "        epoch_actor_loss = 0\n",
    "        epoch_critic_loss = 0\n",
    "        for cycle in range(0, MAX_CYCLES):\n",
    "            mb = []\n",
    "            cycle_actor_loss = 0\n",
    "            cycle_critic_loss = 0\n",
    "            for episode in range(MAX_EPISODES):\n",
    "                episode_dict = {\n",
    "                    \"state\": [],\n",
    "                    \"action\": [],\n",
    "                    \"info\": [],\n",
    "                    \"achieved_goal\": [],\n",
    "                    \"desired_goal\": [],\n",
    "                    \"next_state\": [],\n",
    "                    \"next_achieved_goal\": []}\n",
    "                env_dict = env.reset()\n",
    "                state = env_dict[\"observation\"]\n",
    "                achieved_goal = env_dict[\"achieved_goal\"]\n",
    "                desired_goal = env_dict[\"desired_goal\"]\n",
    "                while np.linalg.norm(achieved_goal - desired_goal) <= 0.05:\n",
    "                    env_dict = env.reset()\n",
    "                    state = env_dict[\"observation\"]\n",
    "                    achieved_goal = env_dict[\"achieved_goal\"]\n",
    "                    desired_goal = env_dict[\"desired_goal\"]\n",
    "                for t in range(500):\n",
    "                    action = agent.choose_action(state, desired_goal)\n",
    "                    next_env_dict, reward, done, info = env.step(action)\n",
    "\n",
    "                    next_state = next_env_dict[\"observation\"]\n",
    "                    next_achieved_goal = next_env_dict[\"achieved_goal\"]\n",
    "                    next_desired_goal = next_env_dict[\"desired_goal\"]\n",
    "\n",
    "                    episode_dict[\"state\"].append(state.copy())\n",
    "                    episode_dict[\"action\"].append(action.copy())\n",
    "                    episode_dict[\"achieved_goal\"].append(achieved_goal.copy())\n",
    "                    episode_dict[\"desired_goal\"].append(desired_goal.copy())\n",
    "\n",
    "                    state = next_state.copy()\n",
    "                    achieved_goal = next_achieved_goal.copy()\n",
    "                    desired_goal = next_desired_goal.copy()\n",
    "                    \n",
    "                    if done:\n",
    "                        break\n",
    "\n",
    "                episode_dict[\"state\"].append(state.copy())\n",
    "                episode_dict[\"achieved_goal\"].append(achieved_goal.copy())\n",
    "                episode_dict[\"desired_goal\"].append(desired_goal.copy())\n",
    "                episode_dict[\"next_state\"] = episode_dict[\"state\"][1:]\n",
    "                episode_dict[\"next_achieved_goal\"] = episode_dict[\"achieved_goal\"][1:]\n",
    "                mb.append(dc(episode_dict))\n",
    "\n",
    "            agent.store(mb)\n",
    "            for n_update in range(num_updates):\n",
    "                actor_loss, critic_loss = agent.train()\n",
    "                cycle_actor_loss += actor_loss\n",
    "                cycle_critic_loss += critic_loss\n",
    "\n",
    "            epoch_actor_loss += cycle_actor_loss / num_updates\n",
    "            epoch_critic_loss += cycle_critic_loss /num_updates\n",
    "            agent.update_networks()\n",
    "\n",
    "        ram = psutil.virtual_memory()\n",
    "        success_rate, running_reward, episode_reward = eval_agent(env, agent)\n",
    "        total_ac_loss.append(epoch_actor_loss)\n",
    "        total_cr_loss.append(epoch_critic_loss)\n",
    "        if MPI.COMM_WORLD.Get_rank() == 0:\n",
    "            t_success_rate.append(success_rate)\n",
    "            print(f\"Epoch:{epoch}| \"\n",
    "                  f\"Running_reward:{running_reward[-1]:.3f}| \"\n",
    "                  f\"EP_reward:{episode_reward:.3f}| \"\n",
    "                  f\"Memory_length:{len(agent.memory)}| \"\n",
    "                  f\"Duration:{time.time() - start_time:.3f}| \"\n",
    "                  f\"Actor_Loss:{actor_loss:.3f}| \"\n",
    "                  f\"Critic_Loss:{critic_loss:.3f}| \"\n",
    "                  f\"Success rate:{success_rate:.3f}| \"\n",
    "                  f\"{to_gb(ram.used):.1f}/{to_gb(ram.total):.1f} GB RAM\")\n",
    "            agent.save_weights()\n",
    "\n",
    "    if MPI.COMM_WORLD.Get_rank() == 0:\n",
    "\n",
    "        with SummaryWriter(\"logs\") as writer:\n",
    "            for i, success_rate in enumerate(t_success_rate):\n",
    "                writer.add_scalar(\"Success_rate\", success_rate, i)\n",
    "\n",
    "        plt.style.use('ggplot')\n",
    "        plt.figure()\n",
    "        plt.plot(np.arange(0, MAX_EPOCHS), t_success_rate)\n",
    "        plt.title(\"Success rate\")\n",
    "        plt.savefig(\"success_rate.png\")\n",
    "        plt.show()\n",
    "\n",
    "elif Play_FLAG:\n",
    "    player = Play(env, agent, max_episode=100)\n",
    "    player.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player = Play(env, agent, max_episode=100)\n",
    "player.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take-home exercise\n",
    "\n",
    "Apply PPO and DDPG to two different environments not already set up for you in this lab.\n",
    "In your report, write what you did and submit a video of the agent behaving in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
