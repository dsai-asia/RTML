{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "short-conversion",
   "metadata": {},
   "source": [
    "# RTML Lab 01: Setup\n",
    "\n",
    "In this lab, we'll set up the Python and CUDA environment that we'll use for the rest of the semester.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "The recommended environment for your work is Ubuntu Linux 22.04 LTS.\n",
    "\n",
    "You will most likely have the best experience with Ubuntu by installing it natively on your development workstation.\n",
    "\n",
    "However, an alternative path is to run Ubuntu side-by-side with Microsoft Windows using the Windows Subsystem for Linux (WSL).\n",
    "\n",
    "According to our surveys, most DS&AI students are not yet ready to switch to Ubuntu for 100% of their work and would like a more gentle introduction\n",
    "to the use of Linux for software development. Therefore, in this tutorial, we assume you are running Windows.\n",
    "\n",
    "### WSL\n",
    "\n",
    "The first step is to install WSL according to [Microsoft's WSL installation instructions](https://docs.microsoft.com/en-us/windows/wsl/install-win10).\n",
    "\n",
    "When it's time to download a Linux distribution from the Windows store, choose \"Ubuntu 22.04.\"\n",
    "\n",
    "### OpenSSH feature in Windows\n",
    "\n",
    "If OpenSSH is not already enabled, go to the \"Manage Optional Features\" settings panel in Windows (\"Apps -> Optional features\" in Windows 11),\n",
    "select \"OpenSSH client\", and click \"Install\".\n",
    "\n",
    "More detail is available at [Microsoft's documentation page for OpenSSH installation](https://docs.microsoft.com/en-us/windows-server/administration/openssh/openssh_install_firstuse#installing-openssh-from-the-settings-ui-on-windows-server-2019-or-windows-10-1809).\n",
    "\n",
    "### VSCode\n",
    "\n",
    "Visual Studio Code is a lightweight yet full featured cross platform IDE for software development that has recently caught up\n",
    "in terms of capabilities and popularity with other popular IDEs for Python such as PyCharm. It is somewhat easier to configure and use, also.\n",
    "Download and install VSCode from [the Visual Studio downloads page](https://code.visualstudio.com/download)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-albania",
   "metadata": {},
   "source": [
    "## WSL Python environment\n",
    "\n",
    "As a starting point, let's set up Python under WSL.\n",
    "\n",
    "In the lab manuals for RTML, when you see commands beginning with '\\$' like this: \n",
    "\n",
    "    $ ls\n",
    "    \n",
    "it means that you should run the command `ls` at your local WSL prompt.\n",
    "\n",
    "As a first step, you'll need to update Ubuntu's package lists, upgrade any that need upgrading, and then install Python 3 and the PIP package manager:\n",
    "\n",
    "     $ sudo apt-get update\n",
    "     $ sudo apt-get upgrade\n",
    "     $ sudo apt-get install python3-pip\n",
    "\n",
    "Then you should be able to run Python code:\n",
    "\n",
    "    $ python3\n",
    "    ...\n",
    "    >>> print('Hello, world!')\n",
    "    Hello, world!\n",
    "    >>>\n",
    "    \n",
    "(Here the `>>>` prompt means the Python interpreter prompt.)\n",
    "\n",
    "To install Python packages, use PIP.\n",
    "\n",
    "    $ pip3 install torch\n",
    "    ...\n",
    "    $ python3\n",
    "    ...\n",
    "    >>> import torch\n",
    "    >>> torch.__version__\n",
    "    '1.10.1'\n",
    "    \n",
    "To upgrade an existing package to the latest version, use the `-U` flag:\n",
    "\n",
    "    $ pip3 install -U torch\n",
    "    ...\n",
    "    $ python3\n",
    "    ...\n",
    "    >>> import torch\n",
    "    >>> torch.__version__\n",
    "    '1.10.1+cu102'\n",
    "   \n",
    "That was easy, right?\n",
    "\n",
    "To work at a professional level with Python, you'll probably want to look at `virtualenv`, which allows you to\n",
    "maintain separate Python environments with all dependencies for each of your projects separately.\n",
    "If you like, take a look at\n",
    "[the Python docs on virtualenv](https://docs.python-guide.org/dev/virtualenvs/#lower-level-virtualenv) for more information.\n",
    "Note that in Ubuntu, the commands are `python3` and `pip3`, not `python` and `pip`.\n",
    "\n",
    "In any case, in RTML, we will do most of our work remotely with another technology,\n",
    "Docker, to isolate our projects, so we don't need to get into virtualenv right now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-bread",
   "metadata": {},
   "source": [
    "## SSH setup for remote access to GPU server in WSL\n",
    "\n",
    "Next, we will set up our environment for access to a remote GPU server via SSH (the Secure Shell protocol).\n",
    "\n",
    "We'll begin with WSL then use the same configuration for our Windows SSH configuration.\n",
    "\n",
    "We'll assume that your GPU server is behind a gateway.\n",
    "\n",
    "At AIT, the gateway is `bazooka.cs.ait.ac.th`, and the GPU server is `puffer.cs.ait.ac.th`. I'll assume your username on both the\n",
    "gateway and GPU server is `st123456`. If you are using a different gateway or server, replace these names with your\n",
    "specific ones in the rest of the tutorial, and obviously, replace `st123456` with your username.\n",
    "\n",
    "First, let's try connecting to the gateway:\n",
    "\n",
    "    $ ssh st123456@bazooka.cs.ait.ac.th\n",
    "    Password for st123456@bazooka.cs.ait.ac.th:\n",
    "    ...\n",
    "    st123456@bazooka:~$ [Control-D or \"exit\" to exit]\n",
    "\n",
    "Next, we want to avoid having to type a password every time we log in to the remote server.\n",
    "We will generate an RSA public/private keypair for SSH to allow login without a password.\n",
    "If you already have an RSA public/private keypair for SSH, you can skip this step.\n",
    "\n",
    "    $ ssh-keygen -t rsa\n",
    "    Generating public/private rsa key pair.\n",
    "    Enter file in which to save the key (/home/mdailey/.ssh/id_rsa) [ENTER]\n",
    "    Enter passphrase (empty for no passphrase): [USE A PASSPHRASE YOU'LL NEVER FORGET]\n",
    "    Enter same passphrase again:\n",
    "    Your identification has been saved in /home/mdailey/.ssh/id_rsa\n",
    "    Your public key has been saved in /home/mdailey/.ssh/id_rsa.pub\n",
    "    The key fingerprint is:\n",
    "    SHA256:AgTtgfplWmns7Z0bQBOuYOawrKff0zZiI4rOVOVLHww mdailey@LAPTOP-NE58KA3C\n",
    "    The key's randomart image is:\n",
    "    +---[RSA 3072]----+\n",
    "    |  .+. .          |\n",
    "    |  ..o. .         |\n",
    "    |..+o.E+          |\n",
    "    |o* .@+o.         |\n",
    "    |.o.O.+ooS        |\n",
    "    |. + o +o.        |\n",
    "    |...  + o..       |\n",
    "    |+o..= = o.       |\n",
    "    |==.o.= ...       |\n",
    "    +----[SHA256]-----+\n",
    "    $\n",
    "\n",
    "Next, we copy the PUBLIC key to the server and tell the server to accept our login using the corresponding private key:\n",
    "\n",
    "    $ scp .ssh/id_rsa.pub st123456@bazooka.cs.ait.ac.th:\n",
    "    Password for st123456@bazooka.cs.ait.ac.th:\n",
    "    ...\n",
    "    $ ssh st123456@bazooka.cs.ait.ac.th\n",
    "    Password for st123456@bazooka.cs.ait.ac.th:\n",
    "    ...\n",
    "    bazooka$ mkdir -p .ssh\n",
    "    bazooka$ cat id_rsa.pub >> .ssh/authorized_keys\n",
    "    bazooka$ exit\n",
    "    $ ssh st123456@bazooka.cs.ait.ac.th\n",
    "    Enter passphrase for key '/home/mdailey/.ssh/id_rsa': [USE THAT PASSPHRASE YOU'LL NEVER FORGET]\n",
    "    ...\n",
    "    bazooka$ exit\n",
    "    $ \n",
    "\n",
    "We don't have to enter our password for the remote server anymore, but we still have to enter our passphrase for the key file. To fix that, we need something called the SSH Agent!\n",
    "It's a little program that runs in the background, reads your private keys into memory (if you ask it to), and then later supplies your keys to SSH every time authentication is needed.\n",
    "\n",
    "    $ eval `ssh-agent`\n",
    "    $ ssh-add ~/.ssh/id_rsa\n",
    "    Enter passphrase for /home/mdailey/.ssh/id_rsa:\n",
    "    Identity added: /home/mdailey/.ssh/id_rsa (mdailey@LAPTOP-NE58KA3C)\n",
    "    $ ssh st123456@bazooka.cs.ait.ac.th\n",
    "    ...\n",
    "    bazooka$ exit\n",
    "\n",
    "OK! Now that we can jump to the gateway without a password, let's use it to jump to the GPU server. For this, you need a file `~/.ssh/config` with contents\n",
    "\n",
    "    Host puffer\n",
    "      Hostname puffer.cs.ait.ac.th\n",
    "      ProxyCommand ssh st123456@bazooka.cs.ait.ac.th -W %h:%p\n",
    "      User st123456\n",
    "      ForwardAgent yes\n",
    "  \n",
    "If everything is OK, you should now be able to SSH directly to the GPU server without using passwords or passphrases:\n",
    "\n",
    "    $ ssh puffer\n",
    "    puffer$ exit\n",
    "\n",
    "That was a lot of steps, but not too bad, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flying-spare",
   "metadata": {},
   "source": [
    "## SSH setup for remote access to GPU server in Windows\n",
    "\n",
    "Now that everything is set up in WSL, it's easy to get it working in Windows directly. VSCode will use Windows' SSH client to connect to our GPU server, so that's why we need to set up\n",
    "both.\n",
    "\n",
    "Within WSL, copy your RSA key files and remote host configuration to your Windows home directory:\n",
    "\n",
    "    $ mkdir -p /mnt/c/Users/Matthew\\ Dailey/.ssh\n",
    "    $ cp ~/.ssh/config /mnt/c/Users/Matthew\\ Dailey/.ssh/\n",
    "    $ cp ~/.ssh/id_rsa* /mnt/c/Users/Matthew\\ Dailey/.ssh/\n",
    "\n",
    "In the Windows Powershell RUNNING AS ADMINISTRATOR, tell Windows to always start the SSH Agent:\n",
    "\n",
    "    PS C:\\WINDOWS/system32> Set-Service ssh-agent -StartupType Automatic\n",
    "    PS C:\\WINDOWS/system32> Start-Service ssh-agent\n",
    "    PS C:\\WINDOWS/system32> Get-Service ssh-agent\n",
    "    \n",
    "    Status   Name               DisplayName\n",
    "    ------   ----               -----------\n",
    "    Running  ssh-agent          OpenSSH Authentication Agent\n",
    "\n",
    "    PS C:\\WINDOWS/system32> exit\n",
    "\n",
    "Now, in an ordinary Powershell, you should be able to log in to the GPU server:\n",
    "\n",
    "    PS C:\\Users\\Matthew Dailey> ssh puffer\n",
    "    ...\n",
    "    puffer$\n",
    "\n",
    "To make sure these changes stay persistent, you may wish to reboot at this point and check that the SSH is enabled and the SSH Agent services\n",
    "are running properly on startup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-chamber",
   "metadata": {},
   "source": [
    "## Testing NVIDIA/Docker integration\n",
    "\n",
    "If you have docker version 19.03 or later installed on the server, you don't need to use a special program to access NVIDIA GPUs within docker. Try the following:\n",
    "\n",
    "    puffer$ docker run -it --rm --gpus all ubuntu nvidia-smi\n",
    "    Fri Jan 15 00:58:18 2021\n",
    "    +-----------------------------------------------------------------------------+\n",
    "    | NVIDIA-SMI 460.91.03   Driver Version: 460.91.03   CUDA Version: 11.2       |\n",
    "    |-------------------------------+----------------------+----------------------+\n",
    "    | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "    | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
    "    |                               |                      |               MIG M. |\n",
    "    |===============================+======================+======================|\n",
    "    |   0  GeForce RTX 208...  Off  | 00000000:84:00.0 Off |                  N/A |\n",
    "    | 24%   38C    P0    56W / 250W |      0MiB / 11019MiB |      0%      Default |\n",
    "    |                               |                      |                  N/A |\n",
    "    +-------------------------------+----------------------+----------------------+\n",
    "    |   1  GeForce RTX 208...  Off  | 00000000:85:00.0 Off |                  N/A |\n",
    "    | 22%   40C    P0    47W / 250W |      0MiB / 11019MiB |      1%      Default |\n",
    "    |                               |                      |                  N/A |\n",
    "    +-------------------------------+----------------------+----------------------+\n",
    "    |   2  GeForce RTX 208...  Off  | 00000000:88:00.0 Off |                  N/A |\n",
    "    | 23%   36C    P0    51W / 250W |      0MiB / 11019MiB |      0%      Default |\n",
    "    |                               |                      |                  N/A |\n",
    "    +-------------------------------+----------------------+----------------------+\n",
    "    |   3  GeForce RTX 208...  Off  | 00000000:89:00.0 Off |                  N/A |\n",
    "    | 31%   36C    P0    25W / 250W |      0MiB / 11019MiB |      0%      Default |\n",
    "    |                               |                      |                  N/A |\n",
    "    +-------------------------------+----------------------+----------------------+\n",
    "    \n",
    "    +-----------------------------------------------------------------------------+\n",
    "    | Processes:                                                                  |\n",
    "    |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
    "    |        ID   ID                                                   Usage      |\n",
    "    |=============================================================================|\n",
    "    |  No running processes found                                                 |\n",
    "    +-----------------------------------------------------------------------------+\n",
    "    puffer$\n",
    "\n",
    "Once the docker images download, you should see some information about the GPU environment on the system. Try the following for a detailed list of GPUs you have access to:\n",
    "\n",
    "    puffer$ docker run -it --rm --gpus all ubuntu nvidia-smi -L\n",
    "\n",
    "One fun trick is to use the DIGITS web application for a deep learning experiment:\n",
    "\n",
    "    puffer$ docker run -itd --gpus all -p 5123:5000 nvidia/digits\n",
    "\n",
    "This connects host port 5123 to the docker process' port 5000.\n",
    "You'll have to pick an unused port instead of 5123.\n",
    "Once running, check that the server is actually up:\n",
    "\n",
    "    puffer$ wget http://localhost:5123\n",
    "    \n",
    "Be careful with wget to localhost if you have a proxy set up -- wget might try to go through the proxy even though the port is local.\n",
    "In that case you would instead do\n",
    "\n",
    "    puffer$ http_proxy='' wget http://localhost:5123\n",
    "\n",
    "If successful, you're up. To forward local port 5000 to port 5123 on the GPU server, you need to modify the command you use to ssh to the server:\n",
    "\n",
    "    $ ssh -L 3000:localhost:5123 puffer\n",
    "    puffer$\n",
    "\n",
    "See if you can connect to your NVIDIA digits process from your local browser once this is done.\n",
    "You should be able to access http://localhost:3000 in your Web browser and access the DIGITS application running on the server.\n",
    "Some useful commands: `docker ps` shows running containers:\n",
    "\n",
    "    puffer$ docker ps\n",
    "    CONTAINER ID  IMAGE          COMMAND             CREATED        STATUS       PORTS                                                NAMES\n",
    "    ...\n",
    "    8acfadaf1a06  nvidia/digits  \"python -m digits\"  3 minutes ago  Up 2 minutes 6006/tcp, 0.0.0.0:5123->5000/tcp, :::5123->5000/tcp  relaxed_golick\n",
    "    ...\n",
    "    puffer$\n",
    "\n",
    "`docker logs` shows the logs generated by a container:\n",
    "\n",
    "    puffer$ docker logs 8acf\n",
    "    mdailey@puffer:~$ docker logs 8acf\n",
    "      ___ ___ ___ ___ _____ ___\n",
    "     |   \\_ _/ __|_ _|_   _/ __|\n",
    "     | |) | | (_ || |  | | \\__ \\\n",
    "     |___/___\\___|___| |_| |___/ 6.0.0\n",
    "\n",
    "    libdc1394 error: Failed to initialize libdc1394\n",
    "    /usr/local/lib/python2.7/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
    "      warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
    "    2022-01-14 00:26:49 [INFO ] Loaded 0 jobs.\n",
    "    puffer$\n",
    "\n",
    "`docker stop` stops a container:\n",
    "\n",
    "    puffer$ docker stop 8acf\n",
    "    8acf\n",
    "\n",
    "And finally, `docker rm` removes the stopped container:\n",
    "\n",
    "    puffer$ docker rm 8acf\n",
    "    8acf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-snake",
   "metadata": {},
   "source": [
    "## Build your docker image\n",
    "\n",
    "To build a Docker image, you'll need to access the Internet. Unfortunately, on the GPU server,\n",
    "we require a proxy server to access the Internet. For this part, you'll need to check what\n",
    "shell you are running (run `echo $SHELL` to find out).\n",
    "To set a proxy with CSH, edit `~/.cshrc` and\n",
    "put this at the end of the file:\n",
    "\n",
    "    setenv http_proxy http://192.41.170.23:3128\n",
    "    setenv https_proxy http://192.41.170.23:3128\n",
    "\n",
    "For BASH, you would edit `~/.bashrc` and place the following at the end of the file:\n",
    "\n",
    "    export http_proxy=http://192.41.170.23:3128\n",
    "    export https_proxy=http://192.41.170.23:3128\n",
    "\n",
    "Now, let's prepare the docker environment on the GPU server\n",
    "\n",
    "    $ scp ~/.ssh/id_rsa.pub puffer:\n",
    "    $ ssh puffer\n",
    "    puffer$ mkdir -p lab01\n",
    "    puffer$ mv id_rsa.pub lab01/\n",
    "    puffer$ cd lab01\n",
    "    puffer$ cat > Dockerfile <<EOF\n",
    "    FROM nvidia/cuda:11.1-base\n",
    "\n",
    "    ENV http_proxy http://192.41.170.23:3128\n",
    "    ENV https_proxy http://192.41.170.23:3128\n",
    "\n",
    "    RUN apt-get update && apt-get upgrade -y && apt-get install -y openssh-server\n",
    "    RUN mkdir /var/run/sshd\n",
    "    RUN mkdir /root/.ssh/\n",
    "    EXPOSE 22\n",
    "\n",
    "    # set the locale to en_US.UTF-8\n",
    "    RUN apt-get install -y locales\n",
    "    ENV DEBIAN_FRONTEND noninteractive\n",
    "    RUN echo \"en_US.UTF-8 UTF-8\" > /etc/locale.gen \\\n",
    "        && locale-gen en_US.UTF-8 \\\n",
    "        && dpkg-reconfigure locales \\\n",
    "        && /usr/sbin/update-locale LANG=en_US.UTF-8\n",
    "    ENV LC_ALL en_US.UTF-8\n",
    "\n",
    "    RUN apt-get install -y python3-pip\n",
    "    RUN pip3 install numpy torch torchvision\n",
    "\n",
    "    RUN echo \"export https_proxy=http://192.41.170.23:3128\" >> /root/.bashrc\n",
    "    RUN echo \"export http_proxy=http://192.41.170.23:3128\" >> /root/.bashrc\n",
    "    COPY id_rsa.pub /root/.ssh/authorized_keys\n",
    "\n",
    "    CMD [\"/usr/sbin/sshd\", \"-D\"]\n",
    "    EOF\n",
    "    puffer$ docker build . -t matt-lab1   # Use your own tag here!\n",
    "\n",
    "If the docker image builds successfully, you're ready to go! Run the image as\n",
    "\n",
    "    puffer$ docker run -p 2222:22 --gpus all matt-lab1\n",
    "\n",
    "You'll want to use a unique port name in place of 2222 if there are others using the GPU server. If that works, you should be able to run\n",
    "\n",
    "    puffer$ ssh root@localhost -p 2222\n",
    "\n",
    "(Use whatever port you selected above in place of 2222, of course.) If that worked, stop and remove your container:\n",
    "\n",
    "    puffer$ docker ps | grep matt-lab1\n",
    "    9d93e1eba494        matt-lab1             \"/usr/sbin/sshd -D\"   27 minutes ago      Up 27 minutes       0.0.0.0:2222->22/tcp                           matt-lab1\n",
    "    puffer$ docker stop 9d93e\n",
    "    puffer$ docker rm 9d93e\n",
    "    puffer$ exit\n",
    "\n",
    "Another way to do things is to ssh to the gpu server, start the container, and open a local port to the remote system, all in one command:\n",
    "\n",
    "    $ ssh -L 2222:localhost:3333 puffer \"docker run -p 3333:22 --gpus all matt-lab1\"\n",
    "\n",
    "The first 2222 is the port on the LOCAL machine. There won't be any conflict there, so you can use what you like. The second port (3333) is the port on the REMOTE machine.\n",
    "It should match the port used on the GPU machine for the docker image's SSH port (2222 in the previous example).\n",
    "\n",
    "If all that worked, now your Docker container is up and running your image with GPU support, and local port 2222 is forwarded to the SSH port of the new Docker container. Now you should be able to run (on your local system)\n",
    "\n",
    "    $ ssh root@localhost -p 2222\n",
    "    1f7a17f71d25# pip3 list | grep torch\n",
    "    ...\n",
    "    torch (1.10.1+cu102)\n",
    "    ...\n",
    "    1f7a17f71d25#\n",
    "\n",
    "If you get asked for a password, make sure you have your SSH Agent enabled as above with the right key added to the agent.\n",
    "You should see torch and numpy among the list of installed Python packages. All is good!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostic-holocaust",
   "metadata": {},
   "source": [
    "## Use a persistent directory for your work inside the container\n",
    "\n",
    "We have good access to our GPU server now, but there are a couple problems with the setup so far:\n",
    "\n",
    "1. Every time you restart your container, your work disappears and you have to upload it again. You may lose work.\n",
    "1. Every time you restart your container, for VSCode to work (see below), you have to reinstall the VSCode remote service and Python extension. This takes too much time (10 or 20 minutes, it seems).\n",
    "\n",
    "The solution is to mount a normal folder on puffer as `/root` inside the container:\n",
    "\n",
    "    puffer$ mv lab01 lab01-old                                        # Assuming you already have a lab01 directory\n",
    "    puffer$ wget https://bit.ly/3oTmtiv -O lab01.zip\n",
    "    puffer$ unzip lab01.zip\n",
    "    puffer$ nano lab01/run.sh                                         # Change your desired SSH port (2224 for Matt) and image tag (matt-lab1 for Matt)\n",
    "    puffer$ cp lab01-old/id_rsa.pub lab01/root/.ssh/authorized_keys   # Replace the authorized_keys file with the id_rsa.pub you created at the beginning of the lab)\n",
    "    puffer$ docker ps | grep matt-lab1\n",
    "    d3c8e1c27b01        matt-lab1                  \"/usr/sbin/sshd -D\"   19 minutes ago       Up 19 minutes       0.0.0.0:2224->22/tcp                           matt-lab1\n",
    "    puffer$ docker stop d3c8\n",
    "    puffer$ docker rm d3c8\n",
    "\n",
    "To start the container, just run the script you edited:\n",
    "\n",
    "    puffer$ cd lab01\n",
    "    puffer$ ./run.sh\n",
    "    af7db76ac58d32a7e403d889ba0b5474c88b3bdc10ab89a79a999d0c09e4c0e6  # This is the ID of the new container running my image \n",
    "    puffer$ docker ps | grep matt-lab1\n",
    "    af7db76ac58d        matt-lab1             \"/bin/sh -c 'chown -â€¦\"      About a minute ago   Up About a minute   0.0.0.0:2224->22/tcp                         matt-lab1\n",
    "\n",
    "On your Windows machine, you'll want a SSH config letting you connect directly to your container using the gateway and GPU server as intermediate hops (in `C:\\Users\\Username\\.ssh\\config`):\n",
    "\n",
    "    Host puffer\n",
    "      HostName puffer.cs.ait.ac.th\n",
    "      ProxyCommand ssh.exe st123456@bazooka.cs.ait.ac.th -W %h:%p\n",
    "      ForwardAgent yes\n",
    "      User st123456\n",
    "      ServerAliveInterval 300\n",
    "      ServerAliveCountMax 2\n",
    "    \n",
    "    Host matt-lab1\n",
    "      Hostname localhost\n",
    "      ProxyCommand ssh.exe st123456@puffer -W %h:%p\n",
    "      ForwardAgent yes\n",
    "      User root\n",
    "      ServerAliveInterval 300\n",
    "      ServerAliveCountMax 2\n",
    "      Port 2224\n",
    "\n",
    "Test that you can ssh to the container directly from your workstation:\n",
    "\n",
    "    windows$ ssh matt-lab1\n",
    "    root@de11ccd18306:~#\n",
    "\n",
    "## Connect VSCode to your container on the GPU server\n",
    "\n",
    "Now we are almost done. The last step is to tell VSCode to use the remote environment inside the new Docker container rather than the local system for running our Python code.\n",
    "\n",
    "Install the Remote SSH Extension in VSCode then use it to connect to your container on the server. Note that when you add a new SSH host in VSCode, you may have to go back to the configuration\n",
    "file and edit it with the correct settings. I also usually seem to have to make multiple attempts to get a successful connection the first time, but once the connection is good, VSCode connects reliably.\n",
    "\n",
    "Once VSCode connects to any remote server successfully, it will upload a small program to help it edit and run code on the remote side of the SSH connection. This takes a little while. Once that's\n",
    "done, you'll want to install the Python Extension for VSCode. This takes quite some time, for some reason, but it only needs to be done once if you followed the instructions to mount `/root` above.\n",
    " \n",
    "Finally, open the `/root` directory or make a subdirectory in the container and create a file for your project code, e.g., `alexnet.py`. Start with a simple test like this:\n",
    "\n",
    "    import torch\n",
    "    print(torch.has_cuda)\n",
    "\n",
    "On puffer, check that your `lab01/root` directory now contains the code you created in VSCode.\n",
    " \n",
    "Finally, if you click on the green \"Run\" triangle and get the result True, you're successful!\n",
    "\n",
    "## Try out an AlexNet model\n",
    "\n",
    "Try some sample code executing a pretrained AlexNet model in PyTorch on a famous dog image:\n",
    "\n",
    "    import torch\n",
    "    import urllib\n",
    "    import os\n",
    "\n",
    "    os.environ['http_proxy'] = 'http://192.41.170.23:3128'\n",
    "    os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "\n",
    "    model = torch.hub.load('pytorch/vision:v0.11.2', 'alexnet', pretrained=True)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Download an example image from the pytorch website\n",
    "\n",
    "    filename = 'dog.jpg'\n",
    "    if not os.path.isfile(filename):\n",
    "        with urllib.request.urlopen('https://github.com/pytorch/hub/raw/master/images/dog.jpg') as url:\n",
    "            with open(filename, 'wb') as f:\n",
    "                f.write(url.read())\n",
    "\n",
    "    from PIL import Image\n",
    "    from torchvision import transforms\n",
    "    input_image = Image.open(filename)\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    input_tensor = preprocess(input_image)\n",
    "    input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "\n",
    "    # move the input and model to GPU for speed if available\n",
    "    if torch.cuda.is_available():\n",
    "        input_batch = input_batch.to('cuda')\n",
    "        model.to('cuda')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_batch)\n",
    "\n",
    "    # Tensor of shape 1000, with confidence scores over Imagenet's 1000 classes\n",
    "    #print(output[0])\n",
    "\n",
    "    # The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
    "    softmax_scores = torch.nn.functional.softmax(output[0], dim=0)\n",
    "\n",
    "    maxval, maxindex = output.max(1)\n",
    "    print('Maximum value', maxval, 'at index', maxindex)\n",
    "\n",
    "You should get output such as\n",
    "\n",
    "    Maximum value tensor([16.8252], device='cuda:0') at index tensor([258], device='cuda:0')\n",
    "\n",
    "You can find the meaning of each output [at this gist](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a), for example. For index 258, we see:\n",
    "\n",
    "    258: 'Samoyed, Samoyede'\n",
    "\n",
    "which is the closest class for this input, a good result!\n",
    "\n",
    "Figure out how to time code in Python. Run `model(input_batch)` right before the code you time to warm up caches and so on. \n",
    "I got an evaluation time of approximately 30 ms for CPU and approximately 1.3 ms for CUDA on puffer.\n",
    "\n",
    "## Tensorboard coolness\n",
    "\n",
    "How can we plot things like loss during training? It's possible to run an X server on Windows and have the Python process running in the container\n",
    "display ordinary Matplotlib plots through the SSH tunnel.\n",
    "\n",
    "However, for deep learning work, most of the things we want to plot are handled by a very nice tool, Tensorboard. This is part of the TensorFlow\n",
    "project but it also interfaces to PyTorch nicely.\n",
    "\n",
    "To install tensorboard in the container:\n",
    "\n",
    "    0a9b00a3f526$ pip3 install tensorboard\n",
    "\n",
    "(You can also add `RUN pip3 install tensorboard` to the `Dockerfile`, rebuild your image, and restart the container.)\n",
    "\n",
    "In your program, do something like what's shown at [the PyTorch Tensoboard documentation](https://pytorch.org/docs/stable/tensorboard.html). For CIFAR-10 you might have something like\n",
    "\n",
    "\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    ...  # Unpickle training batches into train_batches[] array per https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "    writer = SummaryWriter()\n",
    "    data = torch.tensor(train_batches[0][b'data']).view([-1, 3, 32, 32])\n",
    "    grid = torchvision.utils.make_grid(data)\n",
    "    writer.add_image('images', grid, 0)\n",
    "    writer.close()\n",
    "\n",
    "Verify that after running this code, you have some data logged to the `./runs` directory.\n",
    "\n",
    "In the container, run `tensorboard --logdir ./runs &` to start Tensorboard in the background.\n",
    "\n",
    "If you're running VSCode SSH Remote, it should automatically detect that you've got Tensorboard running and automatically map container port 6006 to your Windows machine's port 6006. Then you\n",
    "can go to `http://localhost:6006` in the browser and see the training batch's images in the dashboard:\n",
    "\n",
    "![Tensorboard example output](https://raw.githubusercontent.com/dsai-asia/RTML/main/Labs/01-Setup/tensorboard.png \"Tensorboard example output\")\n",
    "\n",
    "If you aren't connecting to the container with VSCode, you'd have to manually map the remote port to a local port, e.g.,\n",
    "\n",
    "    ssh -L 6006:localhost:6006 matt-lab1\n",
    "\n",
    "## The independent part\n",
    "\n",
    "OK, so now your goal is to try to train and evaluate the PyTorch AlexNet model on the CIFAR-10 dataset. You can use Torch's torchvision module to load the data into PyTorch tensors.\n",
    "\n",
    "## The report\n",
    "\n",
    "Your lab report should have the following sections:\n",
    "\n",
    "1. Introduction: the background and goals of the lab\n",
    "1. Methods: what you did, what parameters you tried, and so on\n",
    "1. Results: what were the results\n",
    "1. Conclusion: what did you learn from the lab, and what might be the next steps\n",
    "\n",
    "In the results section, be sure to show training and validation loss as a function of training epochs. You'll also want to show results on a separate test set and give some analysis of the errors the classifier makes on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-pursuit",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
