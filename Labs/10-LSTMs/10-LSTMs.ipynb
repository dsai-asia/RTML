{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 10: Long Short Term Memory (LSTM) Models\n",
    "\n",
    "Today we will build a chatbot using the PyTorch LSTM cell.\n",
    "\n",
    "The material in this lab comes from several sources:\n",
    "- Hands-On Natural Language Processing with Pytorch 1.x, PacktPub\n",
    "- https://d2l.ai/chapter_recurrent-modern/beam-search.html\n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.LSTMCell.html\n",
    "- https://towardsdatascience.com/building-a-lstm-by-hand-on-pytorch-59c02a4ec091\n",
    "- https://github.com/usmanali414/Chatbot-using-Pytorch-LSTM (source code)\n",
    "\n",
    "## What is the LSTM?\n",
    "\n",
    "Simple RNNs (SRNs) like we used in Lab 09\n",
    "have difficulty retaining information over a long period of time. LSTM addresses some of the issues arising\n",
    "in SRNs. LSTM adds two gates to the typical SRN: an *update gate* and a *forget gate*. These two gates help a network\n",
    "learn long-term dependencies, for example, so that it can remember the relevant words within a sentence while ignoring\n",
    "all the irrelevant information.\n",
    "\n",
    "At a high level,\n",
    "LSTMs are similar in structure to SRNs. Like SRNs, LSTMs propagate a hidden state from step to step.\n",
    "However, the inner details of an LSTM cell are quite different from the SRN.\n",
    "\n",
    "<img src=\"img/LSTM2.png\" title=\"LSTM\" style=\"width: 100px;\" />\n",
    "\n",
    "## LSTM cell\n",
    "\n",
    "The inner workings of an LSTM cell are significantly more complicated. At time step\n",
    "$t$, the processing looks like this:\n",
    "\n",
    "<img src=\"img/LSTMcell2.png\" title=\"LSTM cell\" style=\"width: 800px;\" />\n",
    "\n",
    "The hidden state $h_t$ is the actual output of the layer that is passed\n",
    "on to any later layers. It is obtained from\n",
    "the internal cell state $c_t$ after it is modulated by the output gate.\n",
    "The cell state $c_t$ is cell's internal state. It may be partially or\n",
    "completely cleared by the forget gate, then the new input may be written\n",
    "to it, under the control of the input gate.\n",
    "\n",
    "### Forget gate\n",
    "\n",
    "The forget gate can preserve or attenutate the previous cell state.\n",
    "It is highlighted in the bold rectangle:\n",
    "\n",
    "<img src=\"img/lstm_forgetgate2.png\" title=\"Forget gate\" style=\"width: 480px;\" />\n",
    "\n",
    "Forget gate processing:\n",
    "\n",
    "$f_t = \\sigma(W_{if}x_t + b_{if} + W_{hf}h_{t-1} + b_{hf})$\n",
    "\n",
    "$c'_t = f_t \\odot c_{t-1}$\n",
    "\n",
    "Note that $\\odot$ here means pointwise multiplication of two equal-length vectors and\n",
    "that the attenuation of the previous cell state is dependent on both the current input\n",
    "and the previous hidden state.\n",
    "\n",
    "### Input gate\n",
    "\n",
    "The input gate allows a value depending on the current input and previous hidden state\n",
    "to be added to the partly forgotten previous cell state $c'_t$ to obtain the new cell state\n",
    "$c_t$. However, the addition of the new information to the cell state is controlled by\n",
    "a multiplicative gate just like the forget gate that also depends on the input and previous\n",
    "hidden state. The input gate is highlighted in the bold rectangle here:\n",
    "\n",
    "<img src=\"img/lstm_inputgate2.jpg\" title=\"Input gate\" style=\"width: 480px;\" />\n",
    "\n",
    "Input gate processing:\n",
    "\n",
    "$i_t = \\sigma(W_{ii}x_t + b_{ii} + W_{hi}h_{t-1} + b_{hi})$\n",
    "\n",
    "$c^+_t = \\tanh(W_{ic}x_t + b_{ic} + W_{hc}h_{t-1} + b_{hc})$\n",
    "\n",
    "$c_t = c'_t + i_t \\odot c^+_t$\n",
    "\n",
    "### Output gate\n",
    "\n",
    "The hidden state $h_t$, already explained above, is a value output by the\n",
    "cell that is used by any later layers at the current\n",
    "time step. The output gate allows some or all of the current\n",
    "cell state to be propagated to the next step as the hidden state.\n",
    "We can think of the output $h_t$ as a (possibly attentuated) copy of\n",
    "the current cell state. The output gate controls this\n",
    "attentuation. It is highlighted in the bold rectangle:\n",
    "\n",
    "<img src=\"img/lstm_outputgate2.png\" title=\"Output gate\" style=\"width: 480px;\" />\n",
    "\n",
    "Output gate processing:\n",
    "\n",
    "$o_t=\\sigma(W_{io}x_t + b_{io} + W_{ho}h_{t-1} + b_{ho})$\n",
    "\n",
    "$h_t=o_t \\cdot \\tanh(c_t)$\n",
    "\n",
    "### Summary\n",
    "\n",
    "In summary, The LSTM cell performs the following computations:\n",
    "\n",
    "$i_t=\\sigma(W_{ii}x_t + b_{ii} + W_{hi}h_{t-1} + b_{hi})$\n",
    "\n",
    "$f_t=\\sigma(W_{if}x_t + b_{if} + W_{hf}h_{t-1} + b_{hf})$\n",
    "\n",
    "$c^+_t = \\tanh(W_{ic}x_t + b_{ic} + W_{hc}h_{t-1} + b_{hc})$\n",
    "\n",
    "$o_t=\\sigma(W_{io}x_t + b_{io} + W_{ho}h_{t-1} + b_{ho})$\n",
    "\n",
    "$c_t=f_t \\odot c_{t-1} + i_t \\odot c^+_t$\n",
    "\n",
    "$h_t=o_t \\odot \\tanh(c_t)$\n",
    "\n",
    "## LSTM code example\n",
    "\n",
    "OK, now let's look at an example of how we might make our own LSTM\n",
    "cell in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class NaiveCustomLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_sz: int, hidden_sz: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_sz\n",
    "        self.hidden_size = hidden_sz\n",
    "        \n",
    "        # Parameters for computing i_t\n",
    "        self.U_i = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.V_i = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.b_i = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        # Parameters for computing f_t\n",
    "        self.U_f = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.V_f = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.b_f = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        # Parameters for computing c_t\n",
    "        self.U_c = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.V_c = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.b_c = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        # Parameters for computing o_t\n",
    "        self.U_o = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.V_o = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.b_o = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        self.init_weights()\n",
    "                \n",
    "    def init_weights(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.normal_(mean=0, std=stdv)\n",
    "         \n",
    "    def forward(self, x, init_states=None):\n",
    "        \"\"\"\n",
    "        forward: Run input x through the cell. Assumes x.shape is (batch_size, sequence_length, input_size)\n",
    "        \"\"\"\n",
    "        bs, seq_sz, _ = x.size()\n",
    "        hidden_seq = []\n",
    "        \n",
    "        if init_states is None:\n",
    "            h_t, c_t = (\n",
    "                torch.zeros(bs, self.hidden_size).to(x.device),\n",
    "                torch.zeros(bs, self.hidden_size).to(x.device),\n",
    "            )\n",
    "        else:\n",
    "            h_t, c_t = init_states\n",
    "            \n",
    "        for t in range(seq_sz):\n",
    "            x_t = x[:, t, :]\n",
    "            \n",
    "            i_t = torch.sigmoid(x_t @ self.U_i + h_t @ self.V_i + self.b_i)\n",
    "            f_t = torch.sigmoid(x_t @ self.U_f + h_t @ self.V_f + self.b_f)\n",
    "            g_t = torch.tanh(x_t @ self.U_c + h_t @ self.V_c + self.b_c)\n",
    "            o_t = torch.sigmoid(x_t @ self.U_o + h_t @ self.V_o + self.b_o)\n",
    "            c_t = f_t * c_t + i_t * g_t\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "            \n",
    "            hidden_seq.append(h_t.unsqueeze(0))\n",
    "        \n",
    "        # Reshape hidden_seq tensor to (batch size, sequence length, hidden_size)\n",
    "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
    "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
    "\n",
    "        return hidden_seq, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our chatbot\n",
    "\n",
    "A chatbot, given input text, should output an appropriate sequence of tokens. One reasonable strategy would\n",
    "be to output, given input sequence $x$, the sequence maximizing $P(y \\mid x)$ using a generative model with\n",
    "parameters learned from a dataset using maximum likelihood.\n",
    "\n",
    "We know that $$P(y \\mid x) = \\prod_{t = 1}^T P(y_t | y_1, \\ldots, y_{t-1}, x)$$.\n",
    "\n",
    "We'll use an LSTM cell as our model of $P(y_t | y_1, \\ldots, y_{t-1}, x)$.\n",
    "One (bad) strategy to find the optimal output for $x$ would be to randomly sample ouput token sequences $y$, calculate\n",
    "$P(y \\mid x)$, and repeat, finally returning the sequence with the highest probability.\n",
    "\n",
    "A better strategy, however, would be a smarter search procedure. \n",
    "We'll go through two search methods: *greedy search* and *beam search*.\n",
    "\n",
    "### Greedy search\n",
    "\n",
    "Greedy search is a simple strategy. At time step $t'$ of the output sequence, given the previous tokens and a representation\n",
    "$c$ of the input, the token with the highest predicted probability from the set of possible tokens $\\gamma$ is selected:\n",
    "\n",
    "$$y_{t'}=\\underset{y\\in\\gamma}{\\textrm{argmax}} P(y \\mid y_1,...,y_{t'-1},c).$$\n",
    "\n",
    "We'll allow two strategies to end the sequence: if the token\n",
    "`<eos>` is output, we stop immediately; otherwise, we keep generating outputs until a maximum output sentence length $T'$ is reached.\n",
    "\n",
    "The disadvantage of this greedy search method is that the actual optimal sequence $y^*$ will contain some locally suboptimal steps.\n",
    "To give some intuition, consider a game like chess. We have to sacrifice some of our own pieces (short term suboptimality) in order to obtain a long\n",
    "term advantage and win the game. Similarly, when selecting an output sentence for our chatbot, some of the tokens in the optimal sequence may have a\n",
    "relatively low probability given the previous sequence.\n",
    "\n",
    "Here's how greedy search works:\n",
    "\n",
    "<img src=\"img/lstm_greedysearch.PNG\" title=\"At each time step, greedy search selects the token with the highest conditional probability\" style=\"width: 240px;\" />\n",
    "\n",
    "We have four tokens `A`, `B`, `C`, and `<eos>` in the output dictionary. The four numbers under each time step represent conditional probabilities of generating\n",
    "`A`, `B`, `C`, and `<eos>` at that time step, given the blue highlighted token was selected for the previous time step, respectively.\n",
    "\n",
    "At each time step, greedy search selects the token with the highest conditional probability. Therefore, the sequence\n",
    "`A`, `B`, `C`, and `<eos>` will be ouptut. The conditional probability of this output sequence is $0.5 \\times 0.4 \\times 0.4 \\times 0.6 = 0.048$.\n",
    "\n",
    "In another example, suppose that at time step 2, we select the token `C`, which has the second highest conditional probability:\n",
    "When `C` is selected at step 2, the conditional probabilities at step 3 will change. Suppose we obtain the following probabilities\n",
    "and then continue selecting the greedy optimal tokens on step 3 and 4:\n",
    "\n",
    "<img src=\"img/lstm_greedysearch2.PNG\" title=\"At time step 2, the token “C”, which has the second highest conditional probability, is selected\" style=\"width: 240px;\" />\n",
    "\n",
    "In this case, we obtain a conditional probability $0.5 \\times 0.3 \\times 0.6 \\times 0.6 = 0.054$, which is larger than what we got from the greedy search. In this example, greedy search is clearly suboptimal.\n",
    "\n",
    "OK, we'll begin with greedy search then introduce a more sophisticated search procedure, beam\n",
    "search. For now, let's start implementing our chatbot. First, we pull in some imports and find the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.jit import script, trace\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import codecs\n",
    "from io import open\n",
    "import itertools\n",
    "import math\n",
    "import pickle\n",
    "from torch.autograd import Variable\n",
    "\n",
    "CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess the data\n",
    "\n",
    "The [Cornell Movie-Dialogs\n",
    "Corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html)\n",
    "is a rich dataset of movie character dialogs with the following statistics:\n",
    "-  220,579 conversational exchanges between 10,292 pairs of movie\n",
    "   characters\n",
    "-  9035 characters from 617 movies\n",
    "-  304,713 total utterances\n",
    "\n",
    "The dataset is large and diverse, with great variation in\n",
    "language formality, time periods, sentiment, and so on. This\n",
    "diversity will hopefully let us build a model that can handle\n",
    "many different contexts.\n",
    "\n",
    "The [PyTorch chatbot tutorial](https://pytorch.org/tutorials/beginner/chatbot_tutorial.html)\n",
    "shows how to preprocess the raw Movie Dialogs dataset. We've done this for you already.\n",
    "\n",
    "Next, note that we are dealing with sequences of words, which cannot be directly\n",
    "mapped to a continuous vector space as we need for LSTMs. We will therefore create a mapping\n",
    "from each unique word in the dataset to an index value. (Note that in the readings in class we've seen\n",
    "better ways to handle natural language vocabularies!)\n",
    "To perform our simple version of the mapping, we define a class `Voc`. It\n",
    "creates both the forward mapping from words to indices and the reverse mapping from\n",
    "indices back to words, as well as a count of each word and a total word count.\n",
    "The class's behavior includes a method to add a word to the vocabulary (`addWord`), a\n",
    "method to add all words in a sentence at once\n",
    "(`addSentence`), and a method to trim infrequently seen words (`trim`). We'll discuss\n",
    "trimming more later. Here it is:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reserved word tokens\n",
    "\n",
    "PAD_token = 0  # Used for padding short sentences\n",
    "SOS_token = 1  # Start-of-sentence token\n",
    "EOS_token = 2  # End-of-sentence token\n",
    "\n",
    "class Voc:\n",
    "\n",
    "    def __init__(self):        \n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3  # Count SOS, EOS, PAD\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    # Remove words below a certain count threshold\n",
    "\n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "\n",
    "        keep_words = []\n",
    "\n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "\n",
    "        print('keep_words {} / {} = {:.4f}'.format(\n",
    "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
    "        ))\n",
    "\n",
    "        # Reinitialize dictionaries\n",
    "\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3 # Count default tokens\n",
    "\n",
    "        for word in keep_words:\n",
    "            self.addWord(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run through the dataset and assemble our vocabulary,\n",
    "as well as the training query/response sentence pairs.\n",
    "Before we can use the data, we have to perform some\n",
    "preprocessing. First, we convert Unicode strings to ASCII using\n",
    "`unicodeToAscii`. Then we convert every letter to lowercase and\n",
    "trim all non-letter characters except for basic punctuation\n",
    "(function `normalizeString`). Finally, to improve training convergence,\n",
    "we filter out sentences with length greater than the `MAX_LENGTH`\n",
    "threshold (function `filterPairs`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10  # Maximum sentence length to consider\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# Read query/response pairs and return a Voc object\n",
    "\n",
    "def readVocs(datafile):\n",
    "    print(\"Reading lines...\")    \n",
    "    # Read the file and split into lines\n",
    "    lines = open(datafile, encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "    voc = Voc()\n",
    "    return voc, pairs\n",
    "\n",
    "# Boolean function returning True iff both sentences in a pair 'p' are under the MAX_LENGTH threshold\n",
    "\n",
    "def filterPair(p):\n",
    "    # Input sequences need to preserve the last word for EOS token\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "# Filter pairs using the filterPair predicate\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "# Using the functions defined above, return a populated voc object and pairs list\n",
    "\n",
    "def loadPrepareData(datafile):\n",
    "    print(\"Start preparing training data ...\")\n",
    "    voc, pairs = readVocs(datafile)\n",
    "    print(\"Read {!s} sentence pairs\".format(len(pairs)))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        voc.addSentence(pair[0])\n",
    "        voc.addSentence(pair[1])\n",
    "    print(\"Counted words:\", voc.num_words)\n",
    "    return voc, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With those routines specficied, now we read chat dataset file, preprocess it, and mold it into pairs of question-answer sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start preparing training data ...\n",
      "Reading lines...\n",
      "Read 221282 sentence pairs\n",
      "Trimmed to 64271 sentence pairs\n",
      "Counting words...\n",
      "Counted words: 18008\n",
      "\n",
      "pairs:\n",
      "['there .', 'where ?']\n",
      "['you have my word . as a gentleman', 'you re sweet .']\n",
      "['hi .', 'looks like things worked out tonight huh ?']\n",
      "['you know chastity ?', 'i believe we share an art instructor']\n",
      "['have fun tonight ?', 'tons']\n",
      "['well no . . .', 'then that s all you had to say .']\n",
      "['then that s all you had to say .', 'but']\n",
      "['but', 'you always been this selfish ?']\n",
      "['do you listen to this crap ?', 'what crap ?']\n",
      "['what good stuff ?', 'the real you .']\n"
     ]
    }
   ],
   "source": [
    "# wget https://github.com/dsai-asia/RTML/raw/main/Labs/11-LSTMs/chatDataset.txt\n",
    "\n",
    "# Load/Assemble Voc and pairs\n",
    "\n",
    "datafile = 'chatDataset.txt'\n",
    "voc, pairs = loadPrepareData(datafile)\n",
    "\n",
    "# Print some pairs to validate\n",
    "\n",
    "print(\"\\npairs:\")\n",
    "for pair in pairs[:10]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already mentioned, we will trim out rarely-used\n",
    "words from the vocabulary. This will help improve convergence during\n",
    "training, because with a lower-dimensional input feature space, it will be easier\n",
    "to estimate the probability model $P(y \\mid x)$. We trim as a two-step\n",
    "process:\n",
    "\n",
    "1. Trim words appearing fewer than `MIN_COUNT` times with the previously-given `Voc.trim`\n",
    "   method.\n",
    "\n",
    "2. Filter out all sentence pairs containing trimmed words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep_words 7823 / 18005 = 0.4345\n",
      "Trimmed from 64271 pairs to 53165, 0.8272 of total\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MIN_COUNT = 3    # Minimum word count threshold for trimming\n",
    "\n",
    "def trimRareWords(voc, pairs, MIN_COUNT):\n",
    "    # Trim words used under the MIN_COUNT from the voc\n",
    "    voc.trim(MIN_COUNT)\n",
    "    # Filter out pairs with trimmed words\n",
    "    keep_pairs = []\n",
    "    for pair in pairs:\n",
    "        input_sentence = pair[0]\n",
    "        output_sentence = pair[1]\n",
    "        keep_input = True\n",
    "        keep_output = True\n",
    "        # Check input sentence\n",
    "        for word in input_sentence.split(' '):\n",
    "            if word not in voc.word2index:\n",
    "                keep_input = False\n",
    "                break\n",
    "        # Check output sentence\n",
    "        for word in output_sentence.split(' '):\n",
    "            if word not in voc.word2index:\n",
    "                keep_output = False\n",
    "                break\n",
    "\n",
    "        # Only keep pairs that do not contain trimmed word(s) in their input or output sentence\n",
    "        if keep_input and keep_output:\n",
    "            keep_pairs.append(pair)\n",
    "\n",
    "    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n",
    "    return keep_pairs\n",
    "\n",
    "# Trim vocabulary and pairs\n",
    "\n",
    "pairs = trimRareWords(voc, pairs, MIN_COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset into testing and training pair sets\n",
    "\n",
    "Let's split the dataset into the first 45,000 pairs for training and the rest for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "testpairs = pairs[45000:]\n",
    "pairs  = pairs[:45000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert pairs to tensors\n",
    "\n",
    "First, let's make tensors representing sentences in which we encode\n",
    "each sequence as a sequence of indices. The sequences should all be\n",
    "padded to to a length of `MAX_LENGTH` so that they are all the same\n",
    "size. The transformation will look like this: \n",
    "\n",
    "![title](img/seq2seq_batches.png)\n",
    "\n",
    "`zeroPadding` does the padding.\n",
    "\n",
    "The ``inputVar`` function handles the process of converting sentences to\n",
    "tensor, ultimately creating a correctly shaped zero-padded tensor. It\n",
    "also returns a tensor of ``lengths`` for each of the sequences in the\n",
    "batch which will be passed to our decoder later.\n",
    "\n",
    "The ``outputVar`` function performs a similar function to ``inputVar``,\n",
    "but instead of returning a ``lengths`` tensor, it returns a binary mask\n",
    "tensor and a maximum target sentence length. The binary mask tensor has\n",
    "the same shape as the output target tensor, but every element that is a\n",
    "*PAD_token* is 0 and all others are 1.\n",
    "\n",
    "``batch2TrainData`` simply takes a bunch of pairs and returns the input\n",
    "and target tensors using the aforementioned functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(voc, sentence):\n",
    "    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n",
    "\n",
    "def zeroPadding(l, fillvalue=PAD_token):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
    "\n",
    "def binaryMatrix(l, value=PAD_token):\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == PAD_token:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "    return m\n",
    "\n",
    "# Return a padded input sequence tensor and the lengths of each original sequence\n",
    "\n",
    "def inputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, lengths\n",
    "\n",
    "# Return a padded target sequence tensor, a padding mask, and the max target length\n",
    "\n",
    "def outputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    mask = binaryMatrix(padList)\n",
    "    mask = torch.ByteTensor(mask)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, mask, max_target_len\n",
    "\n",
    "# Return all items for a given batch of pairs\n",
    "\n",
    "def batch2TrainData(voc, pair_batch):\n",
    "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "    input_batch, output_batch = [], []\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "    inp, lengths = inputVar(input_batch, voc)\n",
    "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
    "    return inp, lengths, output, mask, max_target_len\n",
    "\n",
    "# Example for validation\n",
    "\n",
    "small_batch_size = 5\n",
    "batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\n",
    "input_variable, lengths, target_variable, mask, max_target_len = batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As LSTM takes time series data so we need to convert our pairs of sentences into time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['there .', 'where ?'], ['you have my word . as a gentleman', 'you re sweet .'], ['hi .', 'looks like things worked out tonight huh ?'], ['have fun tonight ?', 'tons'], ['well no . . .', 'then that s all you had to say .']]\n",
      "[['you have my word . as a gentleman', 'you re sweet .'], ['well no . . .', 'then that s all you had to say .'], ['have fun tonight ?', 'tons'], ['there .', 'where ?'], ['hi .', 'looks like things worked out tonight huh ?']]\n",
      "tensor([[ 318, 3490,  101,  169,   51],\n",
      "        [   4,   25,  258,   53,    4],\n",
      "        [   2,   74,  147,  217,   33],\n",
      "        [   0,    4,    7,    4,    7],\n",
      "        [   0,    2,   92,    2,   89],\n",
      "        [   0,    0,    4,    0,  257],\n",
      "        [   0,    0,    2,    0,   53],\n",
      "        [   0,    0,    0,    0, 1386],\n",
      "        [   0,    0,    0,    0,    6],\n",
      "        [   0,    0,    0,    0,    2]])\n",
      "tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [0, 1, 1, 1, 1],\n",
      "        [0, 1, 1, 1, 1],\n",
      "        [0, 0, 1, 0, 1],\n",
      "        [0, 0, 1, 0, 1],\n",
      "        [0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 1]], dtype=torch.uint8)\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "pair_batch = pairs[:5]\n",
    "print(pair_batch)\n",
    "pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "print(pair_batch)\n",
    "print(target_variable)\n",
    "print(mask)\n",
    "print(max_target_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define models\n",
    "\n",
    "Seq2Seq Model\n",
    "\n",
    "The brains of our chatbot is a sequence-to-sequence (seq2seq) model. The\n",
    "goal of a seq2seq model is to take a variable-length sequence as an\n",
    "input, and return a variable-length sequence as an output using a\n",
    "fixed-sized model.\n",
    "\n",
    "`Sutskever et al. <https://arxiv.org/abs/1409.3215>`__ discovered that\n",
    "by using two separate recurrent neural nets together, we can accomplish\n",
    "this task. One RNN acts as an **encoder**, which encodes a variable\n",
    "length input sequence to a fixed-length context vector. In theory, this\n",
    "context vector (the final hidden layer of the RNN) will contain semantic\n",
    "information about the query sentence that is input to the bot. The\n",
    "second RNN is a **decoder**, which takes an input word and the context\n",
    "vector, and returns a guess for the next word in the sequence and a\n",
    "hidden state to use in the next iteration.\n",
    "\n",
    "![title](img/seq2seq_ts2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder\n",
    "\n",
    "The encoder RNN iterates through the input sentence one token\n",
    "(e.g. word) at a time, at each time step outputting an “output” vector\n",
    "and a “hidden state” vector. The hidden state vector is then passed to\n",
    "the next time step, while the output vector is recorded. The encoder\n",
    "transforms the context it saw at each point in the sequence into a set\n",
    "of points in a high-dimensional space, which the decoder will use to\n",
    "generate a meaningful output for the given task.\n",
    "\n",
    "At the heart of our encoder is a multi-layered LSTM.\n",
    "We will use a bidirectional variant of the LSTM, meaning that there\n",
    "are essentially two independent RNNs: one that is fed the input sequence\n",
    "in normal sequential order, and one that is fed the input sequence in\n",
    "reverse order. The outputs of each network are summed at each time step.\n",
    "Using a bidirectional LSTM will give us the advantage of encoding both\n",
    "past and future context.\n",
    "\n",
    "![title](img/RNN-bidirectional2.png)\n",
    "\n",
    "\n",
    "Note that an ``embedding`` layer is used to encode our word indices in\n",
    "an arbitrarily sized feature space. For our models, this layer will map\n",
    "each word to a feature space of size *hidden_size*. When trained, these\n",
    "values should encode semantic similarity between similar meaning words.\n",
    "\n",
    "Finally, if passing a padded batch of sequences to an RNN module, we\n",
    "must pack and unpack padding around the RNN pass using\n",
    "``nn.utils.rnn.pack_padded_sequence`` and\n",
    "``nn.utils.rnn.pad_packed_sequence`` respectively.\n",
    "\n",
    "**Computation Graph:**\n",
    "\n",
    "   1) Convert word indexes to embeddings.\n",
    "\n",
    "   2) Pack padded batch of sequences for RNN module.\n",
    "\n",
    "   3) Forward pass through LSTM.\n",
    "\n",
    "   4) Unpack padding.\n",
    "\n",
    "   5) Sum bidirectional LSTM outputs.\n",
    "   \n",
    "   6) Return output and final hidden state.\n",
    "\n",
    "**Inputs:**\n",
    "\n",
    "-  ``input_seq``: batch of input sentences; shape=\\ *(max_length,\n",
    "   batch_size)*\n",
    "-  ``input_lengths``: list of sentence lengths corresponding to each\n",
    "   sentence in the batch; shape=\\ *(batch_size)*\n",
    "-  ``hidden``: hidden state; shape=\\ *(n_layers x num_directions,\n",
    "   batch_size, hidden_size)*\n",
    "\n",
    "**Outputs:**\n",
    "\n",
    "-  ``outputs``: output features from the last hidden layer of the LSTM\n",
    "   (sum of bidirectional outputs); shape=\\ *(max_length, batch_size,\n",
    "   hidden_size)*\n",
    "-  ``hidden``: updated hidden state from LSTM; shape=\\ *(n_layers x\n",
    "   num_directions, batch_size, hidden_size)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder\n",
    "\n",
    "Now we declare our encoder which is consist of bidirectional LSTM units.It is vital to declare bidirectional LSTM as their results are better than unidirectional LSTM in some NLP problems.What it done is instead of learning embeddings of previous words it also considers the embeddings or features of next word suitable to predict target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers,\n",
    "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        \n",
    "        embedded = self.embedding(input_seq)\n",
    "\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths.cpu())\n",
    "        \n",
    "        outputs, hidden = self.lstm(packed, hidden)\n",
    "        \n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        \n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
    "        \n",
    "        return outputs, hidden\n",
    "    def init_hidden(self):\n",
    "        \n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder\n",
    "\n",
    "The decoder RNN generates the response sentence in a token-by-token\n",
    "fashion. It uses the encoder’s context vectors, and internal hidden\n",
    "states to generate the next word in the sequence. It continues\n",
    "generating words until it outputs an *EOS_token*, representing the end\n",
    "of the sentence. A common problem with a vanilla seq2seq decoder is that\n",
    "if we rely soley on the context vector to encode the entire input\n",
    "sequence’s meaning, it is likely that we will have information loss.\n",
    "This is especially the case when dealing with long input sequences,\n",
    "greatly limiting the capability of our decoder.\n",
    "\n",
    "To combat this, `Bahdanau et al. <https://arxiv.org/abs/1409.0473>`__\n",
    "created an “attention mechanism” that allows the decoder to pay\n",
    "attention to certain parts of the input sequence, rather than using the\n",
    "entire fixed context at every step.\n",
    "\n",
    "At a high level, attention is calculated using the decoder’s current\n",
    "hidden state and the encoder’s outputs. The output attention weights\n",
    "have the same shape as the input sequence, allowing us to multiply them\n",
    "by the encoder outputs, giving us a weighted sum which indicates the\n",
    "parts of encoder output to pay attention to. `Sean\n",
    "Robertson’s <https://github.com/spro>`__ figure describes this very\n",
    "well:\n",
    "![title](img/attn2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        if self.method not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \n",
    "        attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "        attn_energies = attn_energies.t()\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined our attention submodule, we can implement the\n",
    "actual decoder model. For the decoder, we will manually feed our batch\n",
    "one time step at a time. This means that our embedded word tensor and\n",
    "LSTM output will both have shape *(1, batch_size, hidden_size)*.\n",
    "\n",
    "**Computation Graph:**\n",
    "\n",
    "   1) Get embedding of current input word.\n",
    "\n",
    "   2) Forward through unidirectional LSTM.\n",
    "\n",
    "   3) Calculate attention weights from the current LSTM output from (2).\n",
    "\n",
    "   4) Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector.\n",
    "\n",
    "   5) Concatenate weighted context vector and LSTM output using Luong eq. 5.\n",
    "\n",
    "   6) Predict next word using Luong eq. 6 (without softmax).\n",
    "   \n",
    "   7) Return output and final hidden state.\n",
    "\n",
    "**Inputs:**\n",
    "\n",
    "-  ``input_step``: one time step (one word) of input sequence batch;\n",
    "   shape=\\ *(1, batch_size)*\n",
    "-  ``last_hidden``: final hidden layer of LSTM; shape=\\ *(n_layers x\n",
    "   num_directions, batch_size, hidden_size)*\n",
    "-  ``encoder_outputs``: encoder model’s output; shape=\\ *(max_length,\n",
    "   batch_size, hidden_size)*\n",
    "\n",
    "**Outputs:**\n",
    "\n",
    "-  ``output``: softmax normalized tensor giving probabilities of each\n",
    "   word being the correct next word in the decoded sequence;\n",
    "   shape=\\ *(batch_size, voc.num_words)*\n",
    "-  ``hidden``: final hidden state of LSTM; shape=\\ *(n_layers x\n",
    "   num_directions, batch_size, hidden_size)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "\n",
    "        \n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "       \n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))#, bidirectional=True)\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.attn = Attn(attn_model, hidden_size)\n",
    "    \n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        rnn_output, hidden = self.lstm(embedded, last_hidden)\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training procedure\n",
    "\n",
    "Masked loss\n",
    "\n",
    "Since we are dealing with batches of padded sequences, we cannot simply\n",
    "consider all elements of the tensor when calculating loss. We define\n",
    "``maskNLLLoss`` to calculate our loss based on our decoder’s output\n",
    "tensor, the target tensor, and a binary mask tensor describing the\n",
    "padding of the target tensor. This loss function calculates the average\n",
    "negative log likelihood of the elements that correspond to a *1* in the\n",
    "mask tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskNLLLoss(inp, target, mask):\n",
    "    nTotal = mask.sum()\n",
    "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
    "    loss = crossEntropy.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single training iteration\n",
    "\n",
    "The ``train`` function contains the algorithm for a single training\n",
    "iteration (a single batch of inputs).\n",
    "\n",
    "We will use a couple of clever tricks to aid in convergence:\n",
    "\n",
    "-  The first trick is using **teacher forcing**. This means that at some\n",
    "   probability, set by ``teacher_forcing_ratio``, we use the current\n",
    "   target word as the decoder’s next input rather than using the\n",
    "   decoder’s current guess. This technique acts as training wheels for\n",
    "   the decoder, aiding in more efficient training. However, teacher\n",
    "   forcing can lead to model instability during inference, as the\n",
    "   decoder may not have a sufficient chance to truly craft its own\n",
    "   output sequences during training. Thus, we must be mindful of how we\n",
    "   are setting the ``teacher_forcing_ratio``, and not be fooled by fast\n",
    "   convergence.\n",
    "\n",
    "-  The second trick that we implement is **gradient clipping**. This is\n",
    "   a commonly used technique for countering the “exploding gradient”\n",
    "   problem. In essence, by clipping or thresholding gradients to a\n",
    "   maximum value, we prevent the gradients from growing exponentially\n",
    "   and either overflow (NaN), or overshoot steep cliffs in the cost\n",
    "   function.\n",
    "\n",
    "\n",
    "\n",
    "**Sequence of Operations:**\n",
    "\n",
    "   1) Forward pass entire input batch through encoder.\n",
    "\n",
    "   2) Initialize decoder inputs as SOS_token, and hidden state as the encoder's final hidden state.\n",
    "   \n",
    "   3) Forward input batch sequence through decoder one time step at a time.\n",
    "\n",
    "   4) If teacher forcing: set next decoder input as the current target; else: set next decoder input as current decoder output.\n",
    "\n",
    "   5) Calculate and accumulate loss.\n",
    "\n",
    "   6) Perform backpropagation.\n",
    "\n",
    "   7) Clip gradients.\n",
    "   \n",
    "   8) Update encoder and decoder model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n",
    "          encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH):\n",
    "\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "  \n",
    "    input_variable = input_variable.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    \n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "\n",
    "    \n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "\n",
    "    \n",
    "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "\n",
    "    \n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            \n",
    "            decoder_input = target_variable[t].view(1, -1)\n",
    "            \n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "    else:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            \n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            \n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    \n",
    "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return sum(print_losses) / n_totals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full training procedure\n",
    "\n",
    "It is finally time to tie the full training procedure together with the\n",
    "data. The ``trainIters`` function is responsible for running\n",
    "``n_iterations`` of training given the passed models, optimizers, data,\n",
    "etc. This function is quite self explanatory, as we have done the heavy\n",
    "lifting with the ``train`` function.\n",
    "\n",
    "One thing to note is that when we save our model, we save a tarball\n",
    "containing the encoder and decoder state_dicts (parameters), the\n",
    "optimizers’ state_dicts, the loss, the iteration, etc. Saving the model\n",
    "in this way will give us the ultimate flexibility with the checkpoint.\n",
    "After loading a checkpoint, we will be able to use the model parameters\n",
    "to run inference, or we can continue training right where we left off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " max_target_len "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename):\n",
    "\n",
    "    # Load batches for each iteration\n",
    "    training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
    "                      for _ in range(n_iteration)]\n",
    "\n",
    "    # Initializations\n",
    "    print('Initializing ...')\n",
    "    start_iteration = 1\n",
    "    print_loss = 0\n",
    "    losslist = []\n",
    "    if loadFilename:\n",
    "        start_iteration = checkpoint['iteration'] + 1\n",
    "\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    for iteration in range(start_iteration, n_iteration + 1):\n",
    "        training_batch = training_batches[iteration - 1]\n",
    "        \n",
    "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
    "\n",
    "        \n",
    "        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n",
    "                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
    "        print_loss += loss\n",
    "\n",
    "        \n",
    "        if iteration % print_every == 0:\n",
    "            print_loss_avg = print_loss / print_every\n",
    "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
    "            print_loss = 0\n",
    "            losslist.append(print_loss_avg)\n",
    "        \n",
    "        if (iteration % save_every == 0):\n",
    "            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
    "            print(directory)\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            torch.save({\n",
    "                'iteration': iteration,\n",
    "                'en': encoder.state_dict(),\n",
    "                'de': decoder.state_dict(),\n",
    "                'en_opt': encoder_optimizer.state_dict(),\n",
    "                'de_opt': decoder_optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'voc_dict': voc.__dict__,\n",
    "                'embedding': embedding.state_dict()\n",
    "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))\n",
    "    return losslist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation function\n",
    "\n",
    "After training a model, we want to be able to talk to the bot ourselves.\n",
    "First, we must define how we want the model to decode the encoded input.\n",
    "\n",
    "#### Greedy decoding\n",
    "\n",
    "Greedy decoding is the decoding method that we use during training when\n",
    "we are **NOT** using teacher forcing. In other words, for each time\n",
    "step, we simply choose the word from ``decoder_output`` with the highest\n",
    "softmax value. This decoding method is optimal on a single time-step\n",
    "level.\n",
    "\n",
    "To facilite the greedy decoding operation, we define a\n",
    "``GreedySearchDecoder`` class. When run, an object of this class takes\n",
    "an input sequence (``input_seq``) of shape *(input_seq length, 1)*, a\n",
    "scalar input length (``input_length``) tensor, and a ``max_length`` to\n",
    "bound the response sentence length. The input sentence is evaluated\n",
    "using the following computational graph:\n",
    "\n",
    "**Computation Graph:**\n",
    "\n",
    "   1) Forward input through encoder model.\n",
    "\n",
    "   2) Prepare encoder's final hidden layer to be first hidden input to the decoder.\n",
    "\n",
    "   3) Initialize decoder's first input as SOS_token.\n",
    "\n",
    "   4) Initialize tensors to append decoded words to.\n",
    "\n",
    "   5) Iteratively decode one word token at a time:\n",
    "   \n",
    "       a) Forward pass through decoder.\n",
    "\n",
    "       b) Obtain most likely word token and its softmax score.\n",
    "\n",
    "       c) Record token and score.\n",
    "\n",
    "       d) Prepare current token to be next decoder input.\n",
    "\n",
    "   6) Return collections of word tokens and scores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedySearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(GreedySearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        \n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        \n",
    "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "        \n",
    "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
    "        \n",
    "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
    "        all_scores = torch.zeros([0], device=device)\n",
    "       \n",
    "        for _ in range(max_length):\n",
    "            \n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "            \n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "        \n",
    "        return all_tokens, all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating some text\n",
    "\n",
    "Now that we have our decoding method defined, we can write functions for\n",
    "evaluating a string input sentence. The ``evaluate`` function manages\n",
    "the low-level process of handling the input sentence. We first format\n",
    "the sentence as an input batch of word indexes with *batch_size==1*. We\n",
    "do this by converting the words of the sentence to their corresponding\n",
    "indexes, and transposing the dimensions to prepare the tensor for our\n",
    "models. We also create a ``lengths`` tensor which contains the length of\n",
    "our input sentence. In this case, ``lengths`` is scalar because we are\n",
    "only evaluating one sentence at a time (batch_size==1). Next, we obtain\n",
    "the decoded response sentence tensor using our ``GreedySearchDecoder``\n",
    "object (``searcher``). Finally, we convert the response’s indexes to\n",
    "words and return the list of decoded words.\n",
    "\n",
    "``evaluateInput`` acts as the user interface for our chatbot. When\n",
    "called, an input text field will spawn in which we can enter our query\n",
    "sentence. After typing our input sentence and pressing *Enter*, our text\n",
    "is normalized in the same way as our training data, and is ultimately\n",
    "fed to the ``evaluate`` function to obtain a decoded output sentence. We\n",
    "loop this process, so we can keep chatting with our bot until we enter\n",
    "either “q” or “quit”.\n",
    "\n",
    "Finally, if a sentence is entered that contains a word that is not in\n",
    "the vocabulary, we handle this gracefully by printing an error message\n",
    "and prompting the user to enter another sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
    "  \n",
    "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
    "    \n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    \n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "    \n",
    "    input_batch = input_batch.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    \n",
    "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
    "    \n",
    "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
    "    return decoded_words\n",
    "\n",
    "\n",
    "def evaluateInput(encoder, decoder, searcher, voc):\n",
    "    input_sentence = ''\n",
    "    while(1):\n",
    "        try:\n",
    "            \n",
    "            input_sentence = input('> ')\n",
    "            \n",
    "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
    "            \n",
    "            input_sentence = normalizeString(input_sentence)\n",
    "            \n",
    "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "            \n",
    "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "            print('Bot:', ' '.join(output_words))\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model\n",
    "\n",
    "\n",
    "Finally, it is time to run our model!\n",
    "\n",
    "Regardless of whether we want to train or test the chatbot model, we\n",
    "must initialize the individual encoder and decoder models. In the\n",
    "following block, we set our desired configurations, choose to start from\n",
    "scratch or set a checkpoint to load from, and build and initialize the\n",
    "models. Feel free to play with different model configurations to\n",
    "optimize performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'cb_model'\n",
    "attn_model = 'dot'\n",
    "\n",
    "hidden_size = 512\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 4\n",
    "dropout = 0.5\n",
    "batch_size = 256 \n",
    "loadFilename = None\n",
    "\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: 47.3%; Average loss: 1.8933\n",
      "Iteration: 2850; Percent complete: 47.5%; Average loss: 1.9112\n",
      "Iteration: 2860; Percent complete: 47.7%; Average loss: 1.8683\n",
      "Iteration: 2870; Percent complete: 47.8%; Average loss: 1.9026\n",
      "Iteration: 2880; Percent complete: 48.0%; Average loss: 1.8985\n",
      "Iteration: 2890; Percent complete: 48.2%; Average loss: 1.9089\n",
      "Iteration: 2900; Percent complete: 48.3%; Average loss: 1.8892\n",
      "Iteration: 2910; Percent complete: 48.5%; Average loss: 1.8484\n",
      "Iteration: 2920; Percent complete: 48.7%; Average loss: 1.9180\n",
      "Iteration: 2930; Percent complete: 48.8%; Average loss: 1.8815\n",
      "Iteration: 2940; Percent complete: 49.0%; Average loss: 1.8785\n",
      "Iteration: 2950; Percent complete: 49.2%; Average loss: 1.8748\n",
      "Iteration: 2960; Percent complete: 49.3%; Average loss: 1.8773\n",
      "Iteration: 2970; Percent complete: 49.5%; Average loss: 1.8974\n",
      "Iteration: 2980; Percent complete: 49.7%; Average loss: 1.8732\n",
      "Iteration: 2990; Percent complete: 49.8%; Average loss: 1.8666\n",
      "Iteration: 3000; Percent complete: 50.0%; Average loss: 1.8826\n",
      "Iteration: 3010; Percent complete: 50.2%; Average loss: 1.8355\n",
      "Iteration: 3020; Percent complete: 50.3%; Average loss: 1.8429\n",
      "Iteration: 3030; Percent complete: 50.5%; Average loss: 1.8157\n",
      "Iteration: 3040; Percent complete: 50.7%; Average loss: 1.8498\n",
      "Iteration: 3050; Percent complete: 50.8%; Average loss: 1.8329\n",
      "Iteration: 3060; Percent complete: 51.0%; Average loss: 1.8425\n",
      "Iteration: 3070; Percent complete: 51.2%; Average loss: 1.8540\n",
      "Iteration: 3080; Percent complete: 51.3%; Average loss: 1.8315\n",
      "Iteration: 3090; Percent complete: 51.5%; Average loss: 1.8494\n",
      "Iteration: 3100; Percent complete: 51.7%; Average loss: 1.8365\n",
      "Iteration: 3110; Percent complete: 51.8%; Average loss: 1.8352\n",
      "Iteration: 3120; Percent complete: 52.0%; Average loss: 1.8200\n",
      "Iteration: 3130; Percent complete: 52.2%; Average loss: 1.8398\n",
      "Iteration: 3140; Percent complete: 52.3%; Average loss: 1.8188\n",
      "Iteration: 3150; Percent complete: 52.5%; Average loss: 1.8031\n",
      "Iteration: 3160; Percent complete: 52.7%; Average loss: 1.8132\n",
      "Iteration: 3170; Percent complete: 52.8%; Average loss: 1.8066\n",
      "Iteration: 3180; Percent complete: 53.0%; Average loss: 1.8238\n",
      "Iteration: 3190; Percent complete: 53.2%; Average loss: 1.8315\n",
      "Iteration: 3200; Percent complete: 53.3%; Average loss: 1.8326\n",
      "Iteration: 3210; Percent complete: 53.5%; Average loss: 1.8067\n",
      "Iteration: 3220; Percent complete: 53.7%; Average loss: 1.8165\n",
      "Iteration: 3230; Percent complete: 53.8%; Average loss: 1.8269\n",
      "Iteration: 3240; Percent complete: 54.0%; Average loss: 1.7997\n",
      "Iteration: 3250; Percent complete: 54.2%; Average loss: 1.8118\n",
      "Iteration: 3260; Percent complete: 54.3%; Average loss: 1.8316\n",
      "Iteration: 3270; Percent complete: 54.5%; Average loss: 1.7675\n",
      "Iteration: 3280; Percent complete: 54.7%; Average loss: 1.7673\n",
      "Iteration: 3290; Percent complete: 54.8%; Average loss: 1.8176\n",
      "Iteration: 3300; Percent complete: 55.0%; Average loss: 1.8150\n",
      "Iteration: 3310; Percent complete: 55.2%; Average loss: 1.7572\n",
      "Iteration: 3320; Percent complete: 55.3%; Average loss: 1.7831\n",
      "Iteration: 3330; Percent complete: 55.5%; Average loss: 1.7943\n",
      "Iteration: 3340; Percent complete: 55.7%; Average loss: 1.7824\n",
      "Iteration: 3350; Percent complete: 55.8%; Average loss: 1.7470\n",
      "Iteration: 3360; Percent complete: 56.0%; Average loss: 1.7908\n",
      "Iteration: 3370; Percent complete: 56.2%; Average loss: 1.7651\n",
      "Iteration: 3380; Percent complete: 56.3%; Average loss: 1.7482\n",
      "Iteration: 3390; Percent complete: 56.5%; Average loss: 1.7977\n",
      "Iteration: 3400; Percent complete: 56.7%; Average loss: 1.7651\n",
      "Iteration: 3410; Percent complete: 56.8%; Average loss: 1.7965\n",
      "Iteration: 3420; Percent complete: 57.0%; Average loss: 1.7914\n",
      "Iteration: 3430; Percent complete: 57.2%; Average loss: 1.7927\n",
      "Iteration: 3440; Percent complete: 57.3%; Average loss: 1.7438\n",
      "Iteration: 3450; Percent complete: 57.5%; Average loss: 1.7420\n",
      "Iteration: 3460; Percent complete: 57.7%; Average loss: 1.7438\n",
      "Iteration: 3470; Percent complete: 57.8%; Average loss: 1.7579\n",
      "Iteration: 3480; Percent complete: 58.0%; Average loss: 1.7400\n",
      "Iteration: 3490; Percent complete: 58.2%; Average loss: 1.7358\n",
      "Iteration: 3500; Percent complete: 58.3%; Average loss: 1.7419\n",
      "Iteration: 3510; Percent complete: 58.5%; Average loss: 1.7864\n",
      "Iteration: 3520; Percent complete: 58.7%; Average loss: 1.7438\n",
      "Iteration: 3530; Percent complete: 58.8%; Average loss: 1.7329\n",
      "Iteration: 3540; Percent complete: 59.0%; Average loss: 1.7245\n",
      "Iteration: 3550; Percent complete: 59.2%; Average loss: 1.7381\n",
      "Iteration: 3560; Percent complete: 59.3%; Average loss: 1.7379\n",
      "Iteration: 3570; Percent complete: 59.5%; Average loss: 1.7419\n",
      "Iteration: 3580; Percent complete: 59.7%; Average loss: 1.6949\n",
      "Iteration: 3590; Percent complete: 59.8%; Average loss: 1.7106\n",
      "Iteration: 3600; Percent complete: 60.0%; Average loss: 1.6942\n",
      "Iteration: 3610; Percent complete: 60.2%; Average loss: 1.7057\n",
      "Iteration: 3620; Percent complete: 60.3%; Average loss: 1.7000\n",
      "Iteration: 3630; Percent complete: 60.5%; Average loss: 1.7350\n",
      "Iteration: 3640; Percent complete: 60.7%; Average loss: 1.6916\n",
      "Iteration: 3650; Percent complete: 60.8%; Average loss: 1.7058\n",
      "Iteration: 3660; Percent complete: 61.0%; Average loss: 1.7311\n",
      "Iteration: 3670; Percent complete: 61.2%; Average loss: 1.7124\n",
      "Iteration: 3680; Percent complete: 61.3%; Average loss: 1.7040\n",
      "Iteration: 3690; Percent complete: 61.5%; Average loss: 1.6875\n",
      "Iteration: 3700; Percent complete: 61.7%; Average loss: 1.7150\n",
      "Iteration: 3710; Percent complete: 61.8%; Average loss: 1.6691\n",
      "Iteration: 3720; Percent complete: 62.0%; Average loss: 1.6723\n",
      "Iteration: 3730; Percent complete: 62.2%; Average loss: 1.7080\n",
      "Iteration: 3740; Percent complete: 62.3%; Average loss: 1.6687\n",
      "Iteration: 3750; Percent complete: 62.5%; Average loss: 1.6674\n",
      "Iteration: 3760; Percent complete: 62.7%; Average loss: 1.6922\n",
      "Iteration: 3770; Percent complete: 62.8%; Average loss: 1.6959\n",
      "Iteration: 3780; Percent complete: 63.0%; Average loss: 1.6923\n",
      "Iteration: 3790; Percent complete: 63.2%; Average loss: 1.6625\n",
      "Iteration: 3800; Percent complete: 63.3%; Average loss: 1.6470\n",
      "Iteration: 3810; Percent complete: 63.5%; Average loss: 1.6689\n",
      "Iteration: 3820; Percent complete: 63.7%; Average loss: 1.6547\n",
      "Iteration: 3830; Percent complete: 63.8%; Average loss: 1.6703\n",
      "Iteration: 3840; Percent complete: 64.0%; Average loss: 1.6691\n",
      "Iteration: 3850; Percent complete: 64.2%; Average loss: 1.6498\n",
      "Iteration: 3860; Percent complete: 64.3%; Average loss: 1.6909\n",
      "Iteration: 3870; Percent complete: 64.5%; Average loss: 1.6593\n",
      "Iteration: 3880; Percent complete: 64.7%; Average loss: 1.6798\n",
      "Iteration: 3890; Percent complete: 64.8%; Average loss: 1.6280\n",
      "Iteration: 3900; Percent complete: 65.0%; Average loss: 1.6674\n",
      "Iteration: 3910; Percent complete: 65.2%; Average loss: 1.6492\n",
      "Iteration: 3920; Percent complete: 65.3%; Average loss: 1.6603\n",
      "Iteration: 3930; Percent complete: 65.5%; Average loss: 1.6423\n",
      "Iteration: 3940; Percent complete: 65.7%; Average loss: 1.6437\n",
      "Iteration: 3950; Percent complete: 65.8%; Average loss: 1.6370\n",
      "Iteration: 3960; Percent complete: 66.0%; Average loss: 1.5985\n",
      "Iteration: 3970; Percent complete: 66.2%; Average loss: 1.6325\n",
      "Iteration: 3980; Percent complete: 66.3%; Average loss: 1.6607\n",
      "Iteration: 3990; Percent complete: 66.5%; Average loss: 1.6396\n",
      "Iteration: 4000; Percent complete: 66.7%; Average loss: 1.6267\n",
      "content/cb_model/Chat/2-4_512\n",
      "Iteration: 4010; Percent complete: 66.8%; Average loss: 1.6525\n",
      "Iteration: 4020; Percent complete: 67.0%; Average loss: 1.6329\n",
      "Iteration: 4030; Percent complete: 67.2%; Average loss: 1.6019\n",
      "Iteration: 4040; Percent complete: 67.3%; Average loss: 1.5940\n",
      "Iteration: 4050; Percent complete: 67.5%; Average loss: 1.6258\n",
      "Iteration: 4060; Percent complete: 67.7%; Average loss: 1.6404\n",
      "Iteration: 4070; Percent complete: 67.8%; Average loss: 1.5843\n",
      "Iteration: 4080; Percent complete: 68.0%; Average loss: 1.6408\n",
      "Iteration: 4090; Percent complete: 68.2%; Average loss: 1.6211\n",
      "Iteration: 4100; Percent complete: 68.3%; Average loss: 1.5886\n",
      "Iteration: 4110; Percent complete: 68.5%; Average loss: 1.5866\n",
      "Iteration: 4120; Percent complete: 68.7%; Average loss: 1.6055\n",
      "Iteration: 4130; Percent complete: 68.8%; Average loss: 1.5960\n",
      "Iteration: 4140; Percent complete: 69.0%; Average loss: 1.6151\n",
      "Iteration: 4150; Percent complete: 69.2%; Average loss: 1.5614\n",
      "Iteration: 4160; Percent complete: 69.3%; Average loss: 1.6282\n",
      "Iteration: 4170; Percent complete: 69.5%; Average loss: 1.5970\n",
      "Iteration: 4180; Percent complete: 69.7%; Average loss: 1.5750\n",
      "Iteration: 4190; Percent complete: 69.8%; Average loss: 1.5686\n",
      "Iteration: 4200; Percent complete: 70.0%; Average loss: 1.5835\n",
      "Iteration: 4210; Percent complete: 70.2%; Average loss: 1.5781\n",
      "Iteration: 4220; Percent complete: 70.3%; Average loss: 1.5951\n",
      "Iteration: 4230; Percent complete: 70.5%; Average loss: 1.6094\n",
      "Iteration: 4240; Percent complete: 70.7%; Average loss: 1.5990\n",
      "Iteration: 4250; Percent complete: 70.8%; Average loss: 1.5688\n",
      "Iteration: 4260; Percent complete: 71.0%; Average loss: 1.5682\n",
      "Iteration: 4270; Percent complete: 71.2%; Average loss: 1.5490\n",
      "Iteration: 4280; Percent complete: 71.3%; Average loss: 1.5440\n",
      "Iteration: 4290; Percent complete: 71.5%; Average loss: 1.5569\n",
      "Iteration: 4300; Percent complete: 71.7%; Average loss: 1.5503\n",
      "Iteration: 4310; Percent complete: 71.8%; Average loss: 1.5782\n",
      "Iteration: 4320; Percent complete: 72.0%; Average loss: 1.5498\n",
      "Iteration: 4330; Percent complete: 72.2%; Average loss: 1.5598\n",
      "Iteration: 4340; Percent complete: 72.3%; Average loss: 1.5617\n",
      "Iteration: 4350; Percent complete: 72.5%; Average loss: 1.5331\n",
      "Iteration: 4360; Percent complete: 72.7%; Average loss: 1.5459\n",
      "Iteration: 4370; Percent complete: 72.8%; Average loss: 1.5262\n",
      "Iteration: 4380; Percent complete: 73.0%; Average loss: 1.5767\n",
      "Iteration: 4390; Percent complete: 73.2%; Average loss: 1.5180\n",
      "Iteration: 4400; Percent complete: 73.3%; Average loss: 1.5427\n",
      "Iteration: 4410; Percent complete: 73.5%; Average loss: 1.5437\n",
      "Iteration: 4420; Percent complete: 73.7%; Average loss: 1.5294\n",
      "Iteration: 4430; Percent complete: 73.8%; Average loss: 1.5411\n",
      "Iteration: 4440; Percent complete: 74.0%; Average loss: 1.5235\n",
      "Iteration: 4450; Percent complete: 74.2%; Average loss: 1.5221\n",
      "Iteration: 4460; Percent complete: 74.3%; Average loss: 1.5374\n",
      "Iteration: 4470; Percent complete: 74.5%; Average loss: 1.5069\n",
      "Iteration: 4480; Percent complete: 74.7%; Average loss: 1.4969\n",
      "Iteration: 4490; Percent complete: 74.8%; Average loss: 1.5087\n",
      "Iteration: 4500; Percent complete: 75.0%; Average loss: 1.4920\n",
      "Iteration: 4510; Percent complete: 75.2%; Average loss: 1.5103\n",
      "Iteration: 4520; Percent complete: 75.3%; Average loss: 1.4830\n",
      "Iteration: 4530; Percent complete: 75.5%; Average loss: 1.4989\n",
      "Iteration: 4540; Percent complete: 75.7%; Average loss: 1.5208\n",
      "Iteration: 4550; Percent complete: 75.8%; Average loss: 1.5002\n",
      "Iteration: 4560; Percent complete: 76.0%; Average loss: 1.4924\n",
      "Iteration: 4570; Percent complete: 76.2%; Average loss: 1.4807\n",
      "Iteration: 4580; Percent complete: 76.3%; Average loss: 1.4808\n",
      "Iteration: 4590; Percent complete: 76.5%; Average loss: 1.4912\n",
      "Iteration: 4600; Percent complete: 76.7%; Average loss: 1.4454\n",
      "Iteration: 4610; Percent complete: 76.8%; Average loss: 1.4757\n",
      "Iteration: 4620; Percent complete: 77.0%; Average loss: 1.4751\n",
      "Iteration: 4630; Percent complete: 77.2%; Average loss: 1.4779\n",
      "Iteration: 4640; Percent complete: 77.3%; Average loss: 1.5021\n",
      "Iteration: 4650; Percent complete: 77.5%; Average loss: 1.4882\n",
      "Iteration: 4660; Percent complete: 77.7%; Average loss: 1.4577\n",
      "Iteration: 4670; Percent complete: 77.8%; Average loss: 1.4769\n",
      "Iteration: 4680; Percent complete: 78.0%; Average loss: 1.4507\n",
      "Iteration: 4690; Percent complete: 78.2%; Average loss: 1.4562\n",
      "Iteration: 4700; Percent complete: 78.3%; Average loss: 1.4519\n",
      "Iteration: 4710; Percent complete: 78.5%; Average loss: 1.4562\n",
      "Iteration: 4720; Percent complete: 78.7%; Average loss: 1.4575\n",
      "Iteration: 4730; Percent complete: 78.8%; Average loss: 1.4781\n",
      "Iteration: 4740; Percent complete: 79.0%; Average loss: 1.4552\n",
      "Iteration: 4750; Percent complete: 79.2%; Average loss: 1.4531\n",
      "Iteration: 4760; Percent complete: 79.3%; Average loss: 1.4203\n",
      "Iteration: 4770; Percent complete: 79.5%; Average loss: 1.4641\n",
      "Iteration: 4780; Percent complete: 79.7%; Average loss: 1.4295\n",
      "Iteration: 4790; Percent complete: 79.8%; Average loss: 1.4336\n",
      "Iteration: 4800; Percent complete: 80.0%; Average loss: 1.4561\n",
      "Iteration: 4810; Percent complete: 80.2%; Average loss: 1.4127\n",
      "Iteration: 4820; Percent complete: 80.3%; Average loss: 1.4344\n",
      "Iteration: 4830; Percent complete: 80.5%; Average loss: 1.4576\n",
      "Iteration: 4840; Percent complete: 80.7%; Average loss: 1.4266\n",
      "Iteration: 4850; Percent complete: 80.8%; Average loss: 1.4455\n",
      "Iteration: 4860; Percent complete: 81.0%; Average loss: 1.4486\n",
      "Iteration: 4870; Percent complete: 81.2%; Average loss: 1.3911\n",
      "Iteration: 4880; Percent complete: 81.3%; Average loss: 1.4299\n",
      "Iteration: 4890; Percent complete: 81.5%; Average loss: 1.4479\n",
      "Iteration: 4900; Percent complete: 81.7%; Average loss: 1.4231\n",
      "Iteration: 4910; Percent complete: 81.8%; Average loss: 1.4386\n",
      "Iteration: 4920; Percent complete: 82.0%; Average loss: 1.4141\n",
      "Iteration: 4930; Percent complete: 82.2%; Average loss: 1.4157\n",
      "Iteration: 4940; Percent complete: 82.3%; Average loss: 1.4283\n",
      "Iteration: 4950; Percent complete: 82.5%; Average loss: 1.4111\n",
      "Iteration: 4960; Percent complete: 82.7%; Average loss: 1.4194\n",
      "Iteration: 4970; Percent complete: 82.8%; Average loss: 1.4048\n",
      "Iteration: 4980; Percent complete: 83.0%; Average loss: 1.4170\n",
      "Iteration: 4990; Percent complete: 83.2%; Average loss: 1.3918\n",
      "Iteration: 5000; Percent complete: 83.3%; Average loss: 1.4314\n",
      "Iteration: 5010; Percent complete: 83.5%; Average loss: 1.3998\n",
      "Iteration: 5020; Percent complete: 83.7%; Average loss: 1.3873\n",
      "Iteration: 5030; Percent complete: 83.8%; Average loss: 1.3784\n",
      "Iteration: 5040; Percent complete: 84.0%; Average loss: 1.3801\n",
      "Iteration: 5050; Percent complete: 84.2%; Average loss: 1.4248\n",
      "Iteration: 5060; Percent complete: 84.3%; Average loss: 1.3928\n",
      "Iteration: 5070; Percent complete: 84.5%; Average loss: 1.3864\n",
      "Iteration: 5080; Percent complete: 84.7%; Average loss: 1.3839\n",
      "Iteration: 5090; Percent complete: 84.8%; Average loss: 1.3828\n",
      "Iteration: 5100; Percent complete: 85.0%; Average loss: 1.4153\n",
      "Iteration: 5110; Percent complete: 85.2%; Average loss: 1.3639\n",
      "Iteration: 5120; Percent complete: 85.3%; Average loss: 1.3877\n",
      "Iteration: 5130; Percent complete: 85.5%; Average loss: 1.3645\n",
      "Iteration: 5140; Percent complete: 85.7%; Average loss: 1.3806\n",
      "Iteration: 5150; Percent complete: 85.8%; Average loss: 1.3896\n",
      "Iteration: 5160; Percent complete: 86.0%; Average loss: 1.3703\n",
      "Iteration: 5170; Percent complete: 86.2%; Average loss: 1.3711\n",
      "Iteration: 5180; Percent complete: 86.3%; Average loss: 1.3863\n",
      "Iteration: 5190; Percent complete: 86.5%; Average loss: 1.3733\n",
      "Iteration: 5200; Percent complete: 86.7%; Average loss: 1.3468\n",
      "Iteration: 5210; Percent complete: 86.8%; Average loss: 1.3595\n",
      "Iteration: 5220; Percent complete: 87.0%; Average loss: 1.3913\n",
      "Iteration: 5230; Percent complete: 87.2%; Average loss: 1.3478\n",
      "Iteration: 5240; Percent complete: 87.3%; Average loss: 1.3661\n",
      "Iteration: 5250; Percent complete: 87.5%; Average loss: 1.3275\n",
      "Iteration: 5260; Percent complete: 87.7%; Average loss: 1.3664\n",
      "Iteration: 5270; Percent complete: 87.8%; Average loss: 1.3395\n",
      "Iteration: 5280; Percent complete: 88.0%; Average loss: 1.3513\n",
      "Iteration: 5290; Percent complete: 88.2%; Average loss: 1.3329\n",
      "Iteration: 5300; Percent complete: 88.3%; Average loss: 1.3100\n",
      "Iteration: 5310; Percent complete: 88.5%; Average loss: 1.3730\n",
      "Iteration: 5320; Percent complete: 88.7%; Average loss: 1.3538\n",
      "Iteration: 5330; Percent complete: 88.8%; Average loss: 1.3389\n",
      "Iteration: 5340; Percent complete: 89.0%; Average loss: 1.3340\n",
      "Iteration: 5350; Percent complete: 89.2%; Average loss: 1.3378\n",
      "Iteration: 5360; Percent complete: 89.3%; Average loss: 1.3238\n",
      "Iteration: 5370; Percent complete: 89.5%; Average loss: 1.3143\n",
      "Iteration: 5380; Percent complete: 89.7%; Average loss: 1.3342\n",
      "Iteration: 5390; Percent complete: 89.8%; Average loss: 1.3373\n",
      "Iteration: 5400; Percent complete: 90.0%; Average loss: 1.3572\n",
      "Iteration: 5410; Percent complete: 90.2%; Average loss: 1.3278\n",
      "Iteration: 5420; Percent complete: 90.3%; Average loss: 1.3309\n",
      "Iteration: 5430; Percent complete: 90.5%; Average loss: 1.3379\n",
      "Iteration: 5440; Percent complete: 90.7%; Average loss: 1.3300\n",
      "Iteration: 5450; Percent complete: 90.8%; Average loss: 1.3208\n",
      "Iteration: 5460; Percent complete: 91.0%; Average loss: 1.3033\n",
      "Iteration: 5470; Percent complete: 91.2%; Average loss: 1.3099\n",
      "Iteration: 5480; Percent complete: 91.3%; Average loss: 1.3095\n",
      "Iteration: 5490; Percent complete: 91.5%; Average loss: 1.3150\n",
      "Iteration: 5500; Percent complete: 91.7%; Average loss: 1.2902\n",
      "Iteration: 5510; Percent complete: 91.8%; Average loss: 1.2978\n",
      "Iteration: 5520; Percent complete: 92.0%; Average loss: 1.2929\n",
      "Iteration: 5530; Percent complete: 92.2%; Average loss: 1.3093\n",
      "Iteration: 5540; Percent complete: 92.3%; Average loss: 1.2808\n",
      "Iteration: 5550; Percent complete: 92.5%; Average loss: 1.3442\n",
      "Iteration: 5560; Percent complete: 92.7%; Average loss: 1.2973\n",
      "Iteration: 5570; Percent complete: 92.8%; Average loss: 1.3091\n",
      "Iteration: 5580; Percent complete: 93.0%; Average loss: 1.2652\n",
      "Iteration: 5590; Percent complete: 93.2%; Average loss: 1.3028\n",
      "Iteration: 5600; Percent complete: 93.3%; Average loss: 1.2933\n",
      "Iteration: 5610; Percent complete: 93.5%; Average loss: 1.2767\n",
      "Iteration: 5620; Percent complete: 93.7%; Average loss: 1.2839\n",
      "Iteration: 5630; Percent complete: 93.8%; Average loss: 1.2634\n",
      "Iteration: 5640; Percent complete: 94.0%; Average loss: 1.2777\n",
      "Iteration: 5650; Percent complete: 94.2%; Average loss: 1.2706\n",
      "Iteration: 5660; Percent complete: 94.3%; Average loss: 1.2561\n",
      "Iteration: 5670; Percent complete: 94.5%; Average loss: 1.2742\n",
      "Iteration: 5680; Percent complete: 94.7%; Average loss: 1.2869\n",
      "Iteration: 5690; Percent complete: 94.8%; Average loss: 1.2898\n",
      "Iteration: 5700; Percent complete: 95.0%; Average loss: 1.2610\n",
      "Iteration: 5710; Percent complete: 95.2%; Average loss: 1.2760\n",
      "Iteration: 5720; Percent complete: 95.3%; Average loss: 1.2329\n",
      "Iteration: 5730; Percent complete: 95.5%; Average loss: 1.2634\n",
      "Iteration: 5740; Percent complete: 95.7%; Average loss: 1.2332\n",
      "Iteration: 5750; Percent complete: 95.8%; Average loss: 1.2671\n",
      "Iteration: 5760; Percent complete: 96.0%; Average loss: 1.2491\n",
      "Iteration: 5770; Percent complete: 96.2%; Average loss: 1.2511\n",
      "Iteration: 5780; Percent complete: 96.3%; Average loss: 1.2356\n",
      "Iteration: 5790; Percent complete: 96.5%; Average loss: 1.2559\n",
      "Iteration: 5800; Percent complete: 96.7%; Average loss: 1.2237\n",
      "Iteration: 5810; Percent complete: 96.8%; Average loss: 1.2481\n",
      "Iteration: 5820; Percent complete: 97.0%; Average loss: 1.2536\n",
      "Iteration: 5830; Percent complete: 97.2%; Average loss: 1.2100\n",
      "Iteration: 5840; Percent complete: 97.3%; Average loss: 1.2649\n",
      "Iteration: 5850; Percent complete: 97.5%; Average loss: 1.2280\n",
      "Iteration: 5860; Percent complete: 97.7%; Average loss: 1.2590\n",
      "Iteration: 5870; Percent complete: 97.8%; Average loss: 1.2562\n",
      "Iteration: 5880; Percent complete: 98.0%; Average loss: 1.2443\n",
      "Iteration: 5890; Percent complete: 98.2%; Average loss: 1.2266\n",
      "Iteration: 5900; Percent complete: 98.3%; Average loss: 1.2284\n",
      "Iteration: 5910; Percent complete: 98.5%; Average loss: 1.2286\n",
      "Iteration: 5920; Percent complete: 98.7%; Average loss: 1.2475\n",
      "Iteration: 5930; Percent complete: 98.8%; Average loss: 1.2139\n",
      "Iteration: 5940; Percent complete: 99.0%; Average loss: 1.2186\n",
      "Iteration: 5950; Percent complete: 99.2%; Average loss: 1.2312\n",
      "Iteration: 5960; Percent complete: 99.3%; Average loss: 1.2183\n",
      "Iteration: 5970; Percent complete: 99.5%; Average loss: 1.1805\n",
      "Iteration: 5980; Percent complete: 99.7%; Average loss: 1.2035\n",
      "Iteration: 5990; Percent complete: 99.8%; Average loss: 1.1993\n",
      "Iteration: 6000; Percent complete: 100.0%; Average loss: 1.2007\n",
      "content/cb_model/Chat/2-4_512\n"
     ]
    }
   ],
   "source": [
    "save_dir = 'content/'\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_iteration = 6000\n",
    "print_every = 10\n",
    "save_every = 2000\n",
    "loadFilename = None\n",
    "corpus_name=\"Chat\"\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "print(\"Starting Training!\")\n",
    "lossvalues = trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
    "           print_every, save_every, clip, corpus_name, loadFilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Created with matplotlib (https://matplotlib.org/) -->\n",
       "<svg height=\"248.95077pt\" version=\"1.1\" viewBox=\"0 0 372.103125 248.95077\" width=\"372.103125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2021-03-30T22:49:26.419134</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 248.95077 \n",
       "L 372.103125 248.95077 \n",
       "L 372.103125 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill:none;\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 30.103125 225.072645 \n",
       "L 364.903125 225.072645 \n",
       "L 364.903125 7.632645 \n",
       "L 30.103125 7.632645 \n",
       "z\n",
       "\" style=\"fill:#ffffff;\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" id=\"mca95cafdb3\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.321307\" xlink:href=\"#mca95cafdb3\" y=\"225.072645\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(42.140057 239.671082)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path d=\"M 31.78125 66.40625 \n",
       "Q 24.171875 66.40625 20.328125 58.90625 \n",
       "Q 16.5 51.421875 16.5 36.375 \n",
       "Q 16.5 21.390625 20.328125 13.890625 \n",
       "Q 24.171875 6.390625 31.78125 6.390625 \n",
       "Q 39.453125 6.390625 43.28125 13.890625 \n",
       "Q 47.125 21.390625 47.125 36.375 \n",
       "Q 47.125 51.421875 43.28125 58.90625 \n",
       "Q 39.453125 66.40625 31.78125 66.40625 \n",
       "z\n",
       "M 31.78125 74.21875 \n",
       "Q 44.046875 74.21875 50.515625 64.515625 \n",
       "Q 56.984375 54.828125 56.984375 36.375 \n",
       "Q 56.984375 17.96875 50.515625 8.265625 \n",
       "Q 44.046875 -1.421875 31.78125 -1.421875 \n",
       "Q 19.53125 -1.421875 13.0625 8.265625 \n",
       "Q 6.59375 17.96875 6.59375 36.375 \n",
       "Q 6.59375 54.828125 13.0625 64.515625 \n",
       "Q 19.53125 74.21875 31.78125 74.21875 \n",
       "z\n",
       "\" id=\"DejaVuSans-48\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"96.133266\" xlink:href=\"#mca95cafdb3\" y=\"225.072645\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 100 -->\n",
       "      <g transform=\"translate(86.589516 239.671082)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path d=\"M 12.40625 8.296875 \n",
       "L 28.515625 8.296875 \n",
       "L 28.515625 63.921875 \n",
       "L 10.984375 60.40625 \n",
       "L 10.984375 69.390625 \n",
       "L 28.421875 72.90625 \n",
       "L 38.28125 72.90625 \n",
       "L 38.28125 8.296875 \n",
       "L 54.390625 8.296875 \n",
       "L 54.390625 0 \n",
       "L 12.40625 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-49\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"146.945225\" xlink:href=\"#mca95cafdb3\" y=\"225.072645\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 200 -->\n",
       "      <g transform=\"translate(137.401475 239.671082)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path d=\"M 19.1875 8.296875 \n",
       "L 53.609375 8.296875 \n",
       "L 53.609375 0 \n",
       "L 7.328125 0 \n",
       "L 7.328125 8.296875 \n",
       "Q 12.9375 14.109375 22.625 23.890625 \n",
       "Q 32.328125 33.6875 34.8125 36.53125 \n",
       "Q 39.546875 41.84375 41.421875 45.53125 \n",
       "Q 43.3125 49.21875 43.3125 52.78125 \n",
       "Q 43.3125 58.59375 39.234375 62.25 \n",
       "Q 35.15625 65.921875 28.609375 65.921875 \n",
       "Q 23.96875 65.921875 18.8125 64.3125 \n",
       "Q 13.671875 62.703125 7.8125 59.421875 \n",
       "L 7.8125 69.390625 \n",
       "Q 13.765625 71.78125 18.9375 73 \n",
       "Q 24.125 74.21875 28.421875 74.21875 \n",
       "Q 39.75 74.21875 46.484375 68.546875 \n",
       "Q 53.21875 62.890625 53.21875 53.421875 \n",
       "Q 53.21875 48.921875 51.53125 44.890625 \n",
       "Q 49.859375 40.875 45.40625 35.40625 \n",
       "Q 44.1875 33.984375 37.640625 27.21875 \n",
       "Q 31.109375 20.453125 19.1875 8.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-50\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"197.757185\" xlink:href=\"#mca95cafdb3\" y=\"225.072645\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 300 -->\n",
       "      <g transform=\"translate(188.213435 239.671082)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path d=\"M 40.578125 39.3125 \n",
       "Q 47.65625 37.796875 51.625 33 \n",
       "Q 55.609375 28.21875 55.609375 21.1875 \n",
       "Q 55.609375 10.40625 48.1875 4.484375 \n",
       "Q 40.765625 -1.421875 27.09375 -1.421875 \n",
       "Q 22.515625 -1.421875 17.65625 -0.515625 \n",
       "Q 12.796875 0.390625 7.625 2.203125 \n",
       "L 7.625 11.71875 \n",
       "Q 11.71875 9.328125 16.59375 8.109375 \n",
       "Q 21.484375 6.890625 26.8125 6.890625 \n",
       "Q 36.078125 6.890625 40.9375 10.546875 \n",
       "Q 45.796875 14.203125 45.796875 21.1875 \n",
       "Q 45.796875 27.640625 41.28125 31.265625 \n",
       "Q 36.765625 34.90625 28.71875 34.90625 \n",
       "L 20.21875 34.90625 \n",
       "L 20.21875 43.015625 \n",
       "L 29.109375 43.015625 \n",
       "Q 36.375 43.015625 40.234375 45.921875 \n",
       "Q 44.09375 48.828125 44.09375 54.296875 \n",
       "Q 44.09375 59.90625 40.109375 62.90625 \n",
       "Q 36.140625 65.921875 28.71875 65.921875 \n",
       "Q 24.65625 65.921875 20.015625 65.03125 \n",
       "Q 15.375 64.15625 9.8125 62.3125 \n",
       "L 9.8125 71.09375 \n",
       "Q 15.4375 72.65625 20.34375 73.4375 \n",
       "Q 25.25 74.21875 29.59375 74.21875 \n",
       "Q 40.828125 74.21875 47.359375 69.109375 \n",
       "Q 53.90625 64.015625 53.90625 55.328125 \n",
       "Q 53.90625 49.265625 50.4375 45.09375 \n",
       "Q 46.96875 40.921875 40.578125 39.3125 \n",
       "z\n",
       "\" id=\"DejaVuSans-51\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-51\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"248.569144\" xlink:href=\"#mca95cafdb3\" y=\"225.072645\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 400 -->\n",
       "      <g transform=\"translate(239.025394 239.671082)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path d=\"M 37.796875 64.3125 \n",
       "L 12.890625 25.390625 \n",
       "L 37.796875 25.390625 \n",
       "z\n",
       "M 35.203125 72.90625 \n",
       "L 47.609375 72.90625 \n",
       "L 47.609375 25.390625 \n",
       "L 58.015625 25.390625 \n",
       "L 58.015625 17.1875 \n",
       "L 47.609375 17.1875 \n",
       "L 47.609375 0 \n",
       "L 37.796875 0 \n",
       "L 37.796875 17.1875 \n",
       "L 4.890625 17.1875 \n",
       "L 4.890625 26.703125 \n",
       "z\n",
       "\" id=\"DejaVuSans-52\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-52\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"299.381103\" xlink:href=\"#mca95cafdb3\" y=\"225.072645\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 500 -->\n",
       "      <g transform=\"translate(289.837353 239.671082)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path d=\"M 10.796875 72.90625 \n",
       "L 49.515625 72.90625 \n",
       "L 49.515625 64.59375 \n",
       "L 19.828125 64.59375 \n",
       "L 19.828125 46.734375 \n",
       "Q 21.96875 47.46875 24.109375 47.828125 \n",
       "Q 26.265625 48.1875 28.421875 48.1875 \n",
       "Q 40.625 48.1875 47.75 41.5 \n",
       "Q 54.890625 34.8125 54.890625 23.390625 \n",
       "Q 54.890625 11.625 47.5625 5.09375 \n",
       "Q 40.234375 -1.421875 26.90625 -1.421875 \n",
       "Q 22.3125 -1.421875 17.546875 -0.640625 \n",
       "Q 12.796875 0.140625 7.71875 1.703125 \n",
       "L 7.71875 11.625 \n",
       "Q 12.109375 9.234375 16.796875 8.0625 \n",
       "Q 21.484375 6.890625 26.703125 6.890625 \n",
       "Q 35.15625 6.890625 40.078125 11.328125 \n",
       "Q 45.015625 15.765625 45.015625 23.390625 \n",
       "Q 45.015625 31 40.078125 35.4375 \n",
       "Q 35.15625 39.890625 26.703125 39.890625 \n",
       "Q 22.75 39.890625 18.8125 39.015625 \n",
       "Q 14.890625 38.140625 10.796875 36.28125 \n",
       "z\n",
       "\" id=\"DejaVuSans-53\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-53\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"350.193063\" xlink:href=\"#mca95cafdb3\" y=\"225.072645\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 600 -->\n",
       "      <g transform=\"translate(340.649313 239.671082)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path d=\"M 33.015625 40.375 \n",
       "Q 26.375 40.375 22.484375 35.828125 \n",
       "Q 18.609375 31.296875 18.609375 23.390625 \n",
       "Q 18.609375 15.53125 22.484375 10.953125 \n",
       "Q 26.375 6.390625 33.015625 6.390625 \n",
       "Q 39.65625 6.390625 43.53125 10.953125 \n",
       "Q 47.40625 15.53125 47.40625 23.390625 \n",
       "Q 47.40625 31.296875 43.53125 35.828125 \n",
       "Q 39.65625 40.375 33.015625 40.375 \n",
       "z\n",
       "M 52.59375 71.296875 \n",
       "L 52.59375 62.3125 \n",
       "Q 48.875 64.0625 45.09375 64.984375 \n",
       "Q 41.3125 65.921875 37.59375 65.921875 \n",
       "Q 27.828125 65.921875 22.671875 59.328125 \n",
       "Q 17.53125 52.734375 16.796875 39.40625 \n",
       "Q 19.671875 43.65625 24.015625 45.921875 \n",
       "Q 28.375 48.1875 33.59375 48.1875 \n",
       "Q 44.578125 48.1875 50.953125 41.515625 \n",
       "Q 57.328125 34.859375 57.328125 23.390625 \n",
       "Q 57.328125 12.15625 50.6875 5.359375 \n",
       "Q 44.046875 -1.421875 33.015625 -1.421875 \n",
       "Q 20.359375 -1.421875 13.671875 8.265625 \n",
       "Q 6.984375 17.96875 6.984375 36.375 \n",
       "Q 6.984375 53.65625 15.1875 63.9375 \n",
       "Q 23.390625 74.21875 37.203125 74.21875 \n",
       "Q 40.921875 74.21875 44.703125 73.484375 \n",
       "Q 48.484375 72.75 52.59375 71.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-54\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-54\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" id=\"m13b3d94bf9\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m13b3d94bf9\" y=\"212.734134\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 1.2 -->\n",
       "      <g transform=\"translate(7.2 216.533352)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path d=\"M 10.6875 12.40625 \n",
       "L 21 12.40625 \n",
       "L 21 0 \n",
       "L 10.6875 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-46\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m13b3d94bf9\" y=\"187.517269\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 1.4 -->\n",
       "      <g transform=\"translate(7.2 191.316488)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m13b3d94bf9\" y=\"162.300405\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 1.6 -->\n",
       "      <g transform=\"translate(7.2 166.099624)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m13b3d94bf9\" y=\"137.083541\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 1.8 -->\n",
       "      <g transform=\"translate(7.2 140.882759)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path d=\"M 31.78125 34.625 \n",
       "Q 24.75 34.625 20.71875 30.859375 \n",
       "Q 16.703125 27.09375 16.703125 20.515625 \n",
       "Q 16.703125 13.921875 20.71875 10.15625 \n",
       "Q 24.75 6.390625 31.78125 6.390625 \n",
       "Q 38.8125 6.390625 42.859375 10.171875 \n",
       "Q 46.921875 13.96875 46.921875 20.515625 \n",
       "Q 46.921875 27.09375 42.890625 30.859375 \n",
       "Q 38.875 34.625 31.78125 34.625 \n",
       "z\n",
       "M 21.921875 38.8125 \n",
       "Q 15.578125 40.375 12.03125 44.71875 \n",
       "Q 8.5 49.078125 8.5 55.328125 \n",
       "Q 8.5 64.0625 14.71875 69.140625 \n",
       "Q 20.953125 74.21875 31.78125 74.21875 \n",
       "Q 42.671875 74.21875 48.875 69.140625 \n",
       "Q 55.078125 64.0625 55.078125 55.328125 \n",
       "Q 55.078125 49.078125 51.53125 44.71875 \n",
       "Q 48 40.375 41.703125 38.8125 \n",
       "Q 48.828125 37.15625 52.796875 32.3125 \n",
       "Q 56.78125 27.484375 56.78125 20.515625 \n",
       "Q 56.78125 9.90625 50.3125 4.234375 \n",
       "Q 43.84375 -1.421875 31.78125 -1.421875 \n",
       "Q 19.734375 -1.421875 13.25 4.234375 \n",
       "Q 6.78125 9.90625 6.78125 20.515625 \n",
       "Q 6.78125 27.484375 10.78125 32.3125 \n",
       "Q 14.796875 37.15625 21.921875 38.8125 \n",
       "z\n",
       "M 18.3125 54.390625 \n",
       "Q 18.3125 48.734375 21.84375 45.5625 \n",
       "Q 25.390625 42.390625 31.78125 42.390625 \n",
       "Q 38.140625 42.390625 41.71875 45.5625 \n",
       "Q 45.3125 48.734375 45.3125 54.390625 \n",
       "Q 45.3125 60.0625 41.71875 63.234375 \n",
       "Q 38.140625 66.40625 31.78125 66.40625 \n",
       "Q 25.390625 66.40625 21.84375 63.234375 \n",
       "Q 18.3125 60.0625 18.3125 54.390625 \n",
       "z\n",
       "\" id=\"DejaVuSans-56\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m13b3d94bf9\" y=\"111.866676\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 2.0 -->\n",
       "      <g transform=\"translate(7.2 115.665895)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m13b3d94bf9\" y=\"86.649812\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 2.2 -->\n",
       "      <g transform=\"translate(7.2 90.449031)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_7\">\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m13b3d94bf9\" y=\"61.432947\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_14\">\n",
       "      <!-- 2.4 -->\n",
       "      <g transform=\"translate(7.2 65.232166)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_8\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m13b3d94bf9\" y=\"36.216083\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_15\">\n",
       "      <!-- 2.6 -->\n",
       "      <g transform=\"translate(7.2 40.015302)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_9\">\n",
       "     <g id=\"line2d_16\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m13b3d94bf9\" y=\"10.999219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_16\">\n",
       "      <!-- 2.8 -->\n",
       "      <g transform=\"translate(7.2 14.798437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_17\">\n",
       "    <path clip-path=\"url(#p7683c4ee05)\" d=\"M 45.321307 28.636792 \n",
       "L 45.829426 19.636651 \n",
       "L 46.337546 17.516281 \n",
       "L 47.353785 20.714778 \n",
       "L 47.861905 19.35032 \n",
       "L 48.370024 18.605585 \n",
       "L 48.878144 18.541308 \n",
       "L 49.386264 23.32612 \n",
       "L 49.894383 19.594622 \n",
       "L 50.402503 25.201862 \n",
       "L 50.910622 21.065034 \n",
       "L 51.418742 21.116396 \n",
       "L 51.926862 24.746172 \n",
       "L 52.434981 20.483783 \n",
       "L 52.943101 23.420457 \n",
       "L 53.45122 23.594563 \n",
       "L 53.95934 25.196845 \n",
       "L 54.467459 21.815978 \n",
       "L 54.975579 27.815152 \n",
       "L 55.483699 26.823385 \n",
       "L 55.991818 27.119046 \n",
       "L 56.499938 26.28464 \n",
       "L 57.008057 30.278107 \n",
       "L 57.516177 25.916137 \n",
       "L 58.024297 32.795233 \n",
       "L 58.532416 33.03606 \n",
       "L 59.040536 28.381779 \n",
       "L 59.548655 29.299491 \n",
       "L 60.056775 29.742471 \n",
       "L 60.564895 27.547348 \n",
       "L 61.073014 27.761456 \n",
       "L 61.581134 31.11188 \n",
       "L 62.597373 34.231296 \n",
       "L 63.105493 33.977403 \n",
       "L 63.613612 28.544587 \n",
       "L 64.121732 30.43011 \n",
       "L 64.629851 35.691038 \n",
       "L 65.137971 36.404235 \n",
       "L 65.646091 32.183007 \n",
       "L 66.15421 34.856074 \n",
       "L 66.66233 30.934459 \n",
       "L 67.170449 37.689172 \n",
       "L 67.678569 39.632247 \n",
       "L 68.186689 35.703714 \n",
       "L 68.694808 36.643467 \n",
       "L 69.202928 38.933904 \n",
       "L 69.711047 37.533613 \n",
       "L 70.219167 34.749955 \n",
       "L 70.727286 41.14313 \n",
       "L 71.235406 38.785873 \n",
       "L 71.743526 40.258561 \n",
       "L 72.251645 42.943629 \n",
       "L 72.759765 43.831123 \n",
       "L 73.267884 40.15794 \n",
       "L 73.776004 42.188859 \n",
       "L 74.284124 41.33535 \n",
       "L 75.300363 43.365158 \n",
       "L 75.808482 43.759911 \n",
       "L 76.316602 43.353893 \n",
       "L 76.824722 42.461951 \n",
       "L 77.332841 48.296978 \n",
       "L 77.840961 44.696084 \n",
       "L 78.34908 45.79087 \n",
       "L 78.8572 48.164513 \n",
       "L 79.36532 44.586721 \n",
       "L 79.873439 50.530013 \n",
       "L 80.381559 48.254275 \n",
       "L 80.889678 54.707101 \n",
       "L 81.397798 49.271045 \n",
       "L 81.905918 42.03669 \n",
       "L 82.414037 49.328051 \n",
       "L 82.922157 51.905658 \n",
       "L 83.430276 49.580413 \n",
       "L 83.938396 50.32478 \n",
       "L 84.446515 48.847077 \n",
       "L 84.954635 49.966707 \n",
       "L 85.462755 55.021654 \n",
       "L 85.970874 52.708505 \n",
       "L 86.478994 53.813122 \n",
       "L 86.987113 52.583264 \n",
       "L 87.495233 50.012676 \n",
       "L 88.003353 50.690622 \n",
       "L 88.511472 56.153375 \n",
       "L 89.019592 55.10727 \n",
       "L 89.527711 56.56029 \n",
       "L 90.035831 55.167632 \n",
       "L 90.543951 54.715298 \n",
       "L 91.05207 56.873255 \n",
       "L 91.56019 61.072072 \n",
       "L 92.068309 56.098966 \n",
       "L 92.576429 56.077093 \n",
       "L 93.084549 61.979465 \n",
       "L 93.592668 58.904164 \n",
       "L 94.100788 60.760548 \n",
       "L 94.608907 55.145431 \n",
       "L 95.117027 63.464392 \n",
       "L 95.625147 58.663745 \n",
       "L 96.133266 55.003343 \n",
       "L 96.641386 64.109604 \n",
       "L 97.149505 58.367082 \n",
       "L 97.657625 64.072315 \n",
       "L 98.165745 61.984919 \n",
       "L 98.673864 58.461048 \n",
       "L 99.181984 60.674621 \n",
       "L 99.690103 60.770896 \n",
       "L 100.198223 65.761666 \n",
       "L 100.706342 65.373171 \n",
       "L 101.214462 64.017566 \n",
       "L 101.722582 65.386941 \n",
       "L 102.230701 64.395557 \n",
       "L 102.738821 64.157568 \n",
       "L 103.24694 62.610008 \n",
       "L 103.75506 66.029194 \n",
       "L 104.26318 66.98166 \n",
       "L 104.771299 63.916893 \n",
       "L 105.279419 67.619199 \n",
       "L 105.787538 66.486267 \n",
       "L 106.803778 66.517795 \n",
       "L 107.311897 69.630234 \n",
       "L 107.820017 68.316342 \n",
       "L 108.328136 65.185819 \n",
       "L 108.836256 68.323936 \n",
       "L 109.344376 73.63643 \n",
       "L 109.852495 67.913362 \n",
       "L 110.360615 71.672631 \n",
       "L 110.868734 69.956684 \n",
       "L 111.884974 72.901686 \n",
       "L 112.393093 69.592232 \n",
       "L 112.901213 72.89162 \n",
       "L 113.409332 71.093542 \n",
       "L 113.917452 73.561394 \n",
       "L 114.425572 77.146762 \n",
       "L 114.933691 74.090269 \n",
       "L 115.94993 72.444074 \n",
       "L 116.45805 75.17202 \n",
       "L 116.966169 75.416472 \n",
       "L 117.474289 74.422967 \n",
       "L 117.982409 78.711136 \n",
       "L 118.490528 80.585452 \n",
       "L 118.998648 73.528419 \n",
       "L 119.506767 77.276143 \n",
       "L 120.014887 76.341406 \n",
       "L 120.523007 77.440578 \n",
       "L 121.031126 79.298191 \n",
       "L 121.539246 77.312764 \n",
       "L 122.555485 76.672005 \n",
       "L 123.571724 85.521571 \n",
       "L 124.079844 80.302124 \n",
       "L 124.587963 84.759644 \n",
       "L 125.096083 81.16582 \n",
       "L 125.604203 80.692309 \n",
       "L 126.112322 82.53853 \n",
       "L 126.620442 83.449439 \n",
       "L 127.128561 85.254861 \n",
       "L 127.636681 83.627731 \n",
       "L 128.144801 85.736129 \n",
       "L 128.65292 82.980062 \n",
       "L 129.16104 81.567255 \n",
       "L 129.669159 83.32758 \n",
       "L 130.177279 83.468457 \n",
       "L 130.685398 85.996578 \n",
       "L 131.193518 84.733415 \n",
       "L 131.701638 82.907606 \n",
       "L 132.209757 87.179589 \n",
       "L 132.717877 84.307044 \n",
       "L 133.225996 86.480777 \n",
       "L 133.734116 85.444566 \n",
       "L 134.242236 83.864546 \n",
       "L 135.258475 89.590197 \n",
       "L 135.766594 83.980442 \n",
       "L 136.274714 83.443025 \n",
       "L 137.290953 87.634847 \n",
       "L 137.799073 88.765184 \n",
       "L 138.307192 92.818047 \n",
       "L 138.815312 92.723873 \n",
       "L 139.323432 92.85063 \n",
       "L 139.831551 95.809933 \n",
       "L 141.35591 91.884321 \n",
       "L 142.372149 95.391908 \n",
       "L 142.880269 89.082052 \n",
       "L 143.388388 91.329342 \n",
       "L 143.896508 94.178936 \n",
       "L 144.404628 95.446572 \n",
       "L 144.912747 94.017443 \n",
       "L 145.420867 97.279403 \n",
       "L 145.928986 95.487155 \n",
       "L 146.437106 92.306056 \n",
       "L 146.945225 99.574576 \n",
       "L 147.453345 95.643643 \n",
       "L 147.961465 95.258551 \n",
       "L 148.469584 97.707129 \n",
       "L 148.977704 97.978609 \n",
       "L 149.485823 94.17633 \n",
       "L 149.993943 98.799862 \n",
       "L 150.502063 97.460398 \n",
       "L 151.010182 103.832859 \n",
       "L 151.518302 101.188022 \n",
       "L 152.026421 99.88135 \n",
       "L 152.534541 100.465286 \n",
       "L 153.042661 99.351179 \n",
       "L 153.55078 96.620481 \n",
       "L 154.567019 100.450053 \n",
       "L 155.075139 106.071172 \n",
       "L 155.583259 97.752822 \n",
       "L 156.091378 103.794743 \n",
       "L 156.599498 102.459789 \n",
       "L 157.107617 100.000019 \n",
       "L 157.615737 105.174661 \n",
       "L 158.123857 104.046079 \n",
       "L 158.631976 99.694477 \n",
       "L 159.140096 99.417391 \n",
       "L 159.648215 101.052395 \n",
       "L 160.156335 106.980424 \n",
       "L 160.664454 100.501931 \n",
       "L 161.172574 106.148652 \n",
       "L 161.680694 107.462251 \n",
       "L 162.188813 103.920195 \n",
       "L 162.696933 106.264604 \n",
       "L 163.205052 106.430842 \n",
       "L 163.713172 104.490377 \n",
       "L 164.221292 105.710196 \n",
       "L 164.729411 111.415913 \n",
       "L 165.237531 109.981384 \n",
       "L 165.74565 110.089553 \n",
       "L 166.25377 108.895942 \n",
       "L 166.76189 109.661877 \n",
       "L 167.270009 112.852212 \n",
       "L 167.778129 111.584642 \n",
       "L 168.286248 112.466333 \n",
       "L 169.302488 114.665231 \n",
       "L 169.810607 111.781694 \n",
       "L 170.318727 111.521254 \n",
       "L 170.826846 114.286647 \n",
       "L 171.334966 112.73994 \n",
       "L 171.843086 110.541881 \n",
       "L 172.351205 112.438154 \n",
       "L 172.859325 110.314148 \n",
       "L 173.367444 111.948555 \n",
       "L 173.875564 113.16463 \n",
       "L 174.891803 117.004476 \n",
       "L 175.399923 111.456462 \n",
       "L 175.908042 110.695609 \n",
       "L 176.416162 115.617491 \n",
       "L 176.924281 116.752301 \n",
       "L 177.432401 119.533016 \n",
       "L 178.44864 115.275947 \n",
       "L 178.95676 117.74675 \n",
       "L 179.464879 114.334376 \n",
       "L 179.972999 116.569168 \n",
       "L 180.481119 118.103619 \n",
       "L 180.989238 116.300294 \n",
       "L 181.497358 118.977301 \n",
       "L 182.005477 119.460232 \n",
       "L 182.513597 119.18949 \n",
       "L 183.021717 116.446778 \n",
       "L 183.529836 121.026893 \n",
       "L 184.037956 123.381803 \n",
       "L 184.546075 121.497981 \n",
       "L 185.054195 122.136751 \n",
       "L 185.562315 121.69732 \n",
       "L 186.070434 124.854755 \n",
       "L 186.578554 124.485091 \n",
       "L 187.086673 121.71486 \n",
       "L 187.594793 120.992849 \n",
       "L 188.102913 124.78734 \n",
       "L 188.611032 126.113392 \n",
       "L 189.119152 125.31472 \n",
       "L 189.627271 123.065146 \n",
       "L 190.135391 128.476631 \n",
       "L 190.64351 124.149463 \n",
       "L 191.15163 124.659614 \n",
       "L 191.65975 123.357188 \n",
       "L 192.167869 125.835614 \n",
       "L 192.675989 130.982705 \n",
       "L 193.184108 122.206461 \n",
       "L 193.692228 126.807055 \n",
       "L 194.708467 127.657916 \n",
       "L 195.216587 127.337859 \n",
       "L 195.724706 124.808884 \n",
       "L 196.232826 127.856042 \n",
       "L 196.740946 128.684892 \n",
       "L 197.249065 126.668162 \n",
       "L 197.757185 132.605326 \n",
       "L 198.265304 131.672326 \n",
       "L 198.773424 135.101166 \n",
       "L 199.281544 130.803729 \n",
       "L 199.789663 132.940217 \n",
       "L 200.805902 130.271777 \n",
       "L 201.314022 133.110732 \n",
       "L 201.822142 130.854725 \n",
       "L 202.330261 132.484666 \n",
       "L 202.838381 132.641298 \n",
       "L 203.3465 134.561093 \n",
       "L 203.85462 132.065459 \n",
       "L 204.36274 134.717275 \n",
       "L 204.870859 136.688826 \n",
       "L 205.378979 135.414083 \n",
       "L 205.887098 136.249311 \n",
       "L 206.395218 134.078615 \n",
       "L 206.903337 133.113041 \n",
       "L 207.411457 132.969913 \n",
       "L 207.919577 136.235318 \n",
       "L 208.935816 133.692068 \n",
       "L 209.443935 137.119812 \n",
       "L 209.952055 135.590191 \n",
       "L 210.460175 133.094681 \n",
       "L 210.968294 141.18342 \n",
       "L 211.476414 141.204691 \n",
       "L 211.984533 134.865426 \n",
       "L 212.492653 135.187554 \n",
       "L 213.000773 142.478497 \n",
       "L 213.508892 139.217173 \n",
       "L 214.017012 137.796589 \n",
       "L 214.525131 139.306863 \n",
       "L 215.033251 143.767023 \n",
       "L 215.541371 138.245066 \n",
       "L 216.04949 141.482952 \n",
       "L 216.55761 143.614626 \n",
       "L 217.065729 137.374527 \n",
       "L 217.573849 141.480038 \n",
       "L 218.081969 137.522089 \n",
       "L 218.590088 138.165228 \n",
       "L 219.098208 137.99879 \n",
       "L 219.606327 144.168626 \n",
       "L 220.114447 144.394264 \n",
       "L 220.622566 144.165133 \n",
       "L 221.130686 142.392499 \n",
       "L 221.638806 144.654252 \n",
       "L 222.146925 145.176135 \n",
       "L 222.655045 144.411532 \n",
       "L 223.163164 138.794976 \n",
       "L 223.671284 144.175542 \n",
       "L 224.687523 146.603354 \n",
       "L 225.195643 144.890982 \n",
       "L 225.703762 144.911505 \n",
       "L 226.211882 144.407335 \n",
       "L 226.720002 150.330051 \n",
       "L 227.228121 148.357311 \n",
       "L 227.736241 150.422854 \n",
       "L 228.24436 148.967319 \n",
       "L 228.75248 149.68576 \n",
       "L 229.2606 145.282207 \n",
       "L 229.768719 150.75261 \n",
       "L 230.276839 148.961233 \n",
       "L 230.784958 145.769169 \n",
       "L 231.293078 148.124368 \n",
       "L 231.801198 149.188485 \n",
       "L 232.309317 151.265198 \n",
       "L 232.817437 147.800124 \n",
       "L 233.325556 153.586875 \n",
       "L 233.833676 153.180519 \n",
       "L 234.341796 148.682218 \n",
       "L 234.849915 153.642436 \n",
       "L 235.358035 153.796065 \n",
       "L 235.866154 150.669237 \n",
       "L 236.374274 150.208246 \n",
       "L 236.882393 150.669054 \n",
       "L 237.390513 154.42205 \n",
       "L 237.898633 156.37896 \n",
       "L 238.406752 153.614269 \n",
       "L 238.914872 155.397836 \n",
       "L 239.422991 153.439161 \n",
       "L 239.931111 153.584687 \n",
       "L 240.439231 156.02575 \n",
       "L 240.94735 150.842922 \n",
       "L 241.45547 154.818743 \n",
       "L 241.963589 152.233026 \n",
       "L 242.471709 158.775894 \n",
       "L 242.979829 153.796117 \n",
       "L 243.487948 156.094256 \n",
       "L 243.996068 154.698559 \n",
       "L 244.504187 156.962577 \n",
       "L 245.012307 156.791459 \n",
       "L 245.520427 157.633843 \n",
       "L 246.028546 162.484055 \n",
       "L 247.044785 154.644418 \n",
       "L 247.552905 157.303379 \n",
       "L 248.061025 158.937989 \n",
       "L 248.569144 155.68328 \n",
       "L 249.077264 158.154608 \n",
       "L 249.585383 162.062352 \n",
       "L 250.093503 163.058288 \n",
       "L 250.601622 159.051208 \n",
       "L 251.109742 157.210845 \n",
       "L 251.617862 164.283457 \n",
       "L 252.125981 157.159992 \n",
       "L 252.634101 159.637834 \n",
       "L 253.14222 163.735615 \n",
       "L 253.65034 163.986298 \n",
       "L 254.15846 161.606139 \n",
       "L 254.666579 162.80444 \n",
       "L 255.174699 160.395924 \n",
       "L 255.682818 167.161357 \n",
       "L 256.190938 158.740488 \n",
       "L 256.699058 162.673223 \n",
       "L 257.207177 165.451518 \n",
       "L 257.715297 166.254449 \n",
       "L 258.223416 164.38372 \n",
       "L 258.731536 165.056502 \n",
       "L 259.747775 161.119808 \n",
       "L 260.255895 162.420236 \n",
       "L 260.764014 166.228467 \n",
       "L 261.272134 166.306816 \n",
       "L 261.780254 168.725565 \n",
       "L 262.288373 169.36589 \n",
       "L 262.796493 167.739929 \n",
       "L 263.304612 168.570137 \n",
       "L 263.812732 165.049447 \n",
       "L 264.320852 168.628443 \n",
       "L 264.828971 167.368668 \n",
       "L 265.337091 167.124451 \n",
       "L 265.84521 170.732913 \n",
       "L 266.35333 169.12625 \n",
       "L 266.861449 171.606685 \n",
       "L 267.369569 165.237221 \n",
       "L 267.877689 172.639301 \n",
       "L 268.385808 169.519171 \n",
       "L 268.893928 169.402974 \n",
       "L 269.402047 171.204254 \n",
       "L 269.910167 169.730802 \n",
       "L 270.418287 171.947308 \n",
       "L 270.926406 172.127944 \n",
       "L 271.434526 170.198258 \n",
       "L 271.942645 174.036499 \n",
       "L 272.450765 175.297849 \n",
       "L 272.958885 173.817182 \n",
       "L 273.467004 175.911632 \n",
       "L 273.975124 173.606953 \n",
       "L 274.483243 177.056177 \n",
       "L 274.991363 175.046903 \n",
       "L 275.499483 172.288469 \n",
       "L 276.007602 174.885061 \n",
       "L 276.515722 175.867029 \n",
       "L 277.023841 177.33676 \n",
       "L 277.531961 177.335193 \n",
       "L 278.040081 176.014123 \n",
       "L 278.5482 181.797612 \n",
       "L 279.05632 177.97162 \n",
       "L 279.564439 178.051805 \n",
       "L 280.072559 177.700893 \n",
       "L 280.580678 174.643599 \n",
       "L 281.088798 176.401699 \n",
       "L 281.596918 180.237438 \n",
       "L 282.105037 177.819191 \n",
       "L 282.613157 181.125566 \n",
       "L 283.121276 180.435718 \n",
       "L 283.629396 180.977425 \n",
       "L 284.137516 180.426977 \n",
       "L 284.645635 180.264046 \n",
       "L 285.153755 177.674271 \n",
       "L 285.661874 180.551836 \n",
       "L 286.169994 180.816981 \n",
       "L 286.678114 184.957789 \n",
       "L 287.186233 179.430692 \n",
       "L 287.694353 183.800585 \n",
       "L 288.202472 183.282871 \n",
       "L 288.710592 180.443328 \n",
       "L 289.218712 185.91082 \n",
       "L 290.234951 180.260354 \n",
       "L 290.74307 184.157617 \n",
       "L 291.25119 181.782101 \n",
       "L 291.75931 181.392288 \n",
       "L 292.267429 188.645342 \n",
       "L 292.775549 183.752281 \n",
       "L 293.283668 181.483611 \n",
       "L 293.791788 184.611 \n",
       "L 294.299908 182.650289 \n",
       "L 294.808027 185.741749 \n",
       "L 295.316147 185.541634 \n",
       "L 295.824266 183.954499 \n",
       "L 296.332386 186.113703 \n",
       "L 296.840505 185.070227 \n",
       "L 297.348625 186.906272 \n",
       "L 297.856745 185.373633 \n",
       "L 298.364864 188.554494 \n",
       "L 298.872984 183.558578 \n",
       "L 299.381103 187.536611 \n",
       "L 299.889223 189.119364 \n",
       "L 300.397343 190.24318 \n",
       "L 300.905462 190.021997 \n",
       "L 301.413582 184.390878 \n",
       "L 301.921701 188.419944 \n",
       "L 302.429821 189.232262 \n",
       "L 302.937941 189.552249 \n",
       "L 303.44606 189.690435 \n",
       "L 303.95418 185.591136 \n",
       "L 304.462299 192.064588 \n",
       "L 304.970419 189.069957 \n",
       "L 305.478539 191.988934 \n",
       "L 305.986658 189.967609 \n",
       "L 306.494778 188.8341 \n",
       "L 307.002897 191.260183 \n",
       "L 307.511017 191.166 \n",
       "L 308.019137 189.247865 \n",
       "L 308.527256 190.878318 \n",
       "L 309.035376 194.22381 \n",
       "L 309.543495 192.624404 \n",
       "L 310.051615 188.614227 \n",
       "L 310.559735 194.092622 \n",
       "L 311.067854 191.788104 \n",
       "L 311.575974 196.656958 \n",
       "L 312.084093 191.756094 \n",
       "L 312.592213 195.145322 \n",
       "L 313.100332 193.656904 \n",
       "L 313.608452 195.977715 \n",
       "L 314.116572 198.869268 \n",
       "L 314.624691 190.923864 \n",
       "L 315.132811 193.348019 \n",
       "L 315.64093 195.226427 \n",
       "L 316.14905 195.835101 \n",
       "L 316.65717 195.355362 \n",
       "L 317.165289 197.123016 \n",
       "L 317.673409 198.320163 \n",
       "L 318.181528 195.815103 \n",
       "L 318.689648 195.427709 \n",
       "L 319.197768 192.912417 \n",
       "L 319.705887 196.624219 \n",
       "L 320.214007 196.227987 \n",
       "L 320.722126 195.346492 \n",
       "L 321.738366 197.506615 \n",
       "L 322.246485 199.715375 \n",
       "L 322.754605 198.871928 \n",
       "L 323.262724 198.929606 \n",
       "L 323.770844 198.239566 \n",
       "L 324.278964 201.359641 \n",
       "L 324.787083 200.400265 \n",
       "L 325.295203 201.023425 \n",
       "L 325.803322 198.954199 \n",
       "L 326.311442 202.541633 \n",
       "L 326.819561 194.549999 \n",
       "L 327.327681 200.470803 \n",
       "L 327.835801 198.976558 \n",
       "L 328.34392 204.514783 \n",
       "L 328.85204 199.775414 \n",
       "L 329.360159 200.965507 \n",
       "L 329.868279 203.063156 \n",
       "L 330.376399 202.154816 \n",
       "L 330.884518 204.741399 \n",
       "L 331.392638 202.931838 \n",
       "L 331.900757 203.837971 \n",
       "L 332.408877 205.662498 \n",
       "L 332.916997 203.382786 \n",
       "L 333.425116 201.782101 \n",
       "L 333.933236 201.416641 \n",
       "L 334.441355 205.041181 \n",
       "L 334.949475 203.149781 \n",
       "L 335.457595 208.581023 \n",
       "L 335.965714 204.740475 \n",
       "L 336.473834 208.543978 \n",
       "L 336.981953 204.26805 \n",
       "L 337.490073 206.547061 \n",
       "L 337.998193 206.292002 \n",
       "L 338.506312 208.248113 \n",
       "L 339.014432 205.690807 \n",
       "L 339.522551 209.749076 \n",
       "L 340.030671 206.665232 \n",
       "L 340.538791 205.977686 \n",
       "L 341.04691 211.476739 \n",
       "L 341.55503 204.55635 \n",
       "L 342.063149 209.205078 \n",
       "L 342.571269 205.296834 \n",
       "L 343.079388 205.649297 \n",
       "L 343.587508 207.149663 \n",
       "L 344.095628 209.377408 \n",
       "L 344.603747 209.153648 \n",
       "L 345.111867 209.128028 \n",
       "L 345.619986 206.747881 \n",
       "L 346.128106 210.97991 \n",
       "L 346.636226 210.392499 \n",
       "L 347.144345 208.794346 \n",
       "L 347.652465 210.421316 \n",
       "L 348.160584 215.189008 \n",
       "L 348.668704 212.290776 \n",
       "L 349.176824 212.82094 \n",
       "L 349.684943 212.642707 \n",
       "L 349.684943 212.642707 \n",
       "\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 30.103125 225.072645 \n",
       "L 30.103125 7.632645 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 364.903125 225.072645 \n",
       "L 364.903125 7.632645 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 30.103125 225.072645 \n",
       "L 364.903125 225.072645 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 30.103125 7.632645 \n",
       "L 364.903125 7.632645 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p7683c4ee05\">\n",
       "   <rect height=\"217.44\" width=\"334.8\" x=\"30.103125\" y=\"7.632645\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(lossvalues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Bleu score Calculation** \n",
    "\n",
    "The BLEU score is an evaluation metric used to calculate the capacity of our model to make correct predictions. You can learn more about BLEU from [this tutorial](https://machinelearningmastery.com/calculate-bleu-score-for-text-python/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.5714285714285714 0.09759000729485333\n",
      "1000 0.3951127030754951 0.2937796011438131\n",
      "2000 0.4312971077573911 0.3319028698115165\n",
      "3000 0.4267998983219376 0.3285260652984988\n",
      "4000 0.4234657325837374 0.323943379826693\n",
      "5000 0.42920150514450617 0.3297765764349787\n",
      "6000 0.4327575896991534 0.3342316157865193\n",
      "7000 0.4384867507964905 0.3420372504882278\n",
      "8000 0.4379800292305039 0.3411342892133563\n",
      "Total Bleu Score for 1 grams on testing pairs:  0.43729246701531554\n",
      "Total Bleu Score for 2 grams on testing pairs:  0.34051306772400264\n"
     ]
    }
   ],
   "source": [
    "# Set dropout layers to eval mode\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "from nltk.translate.bleu_score import sentence_bleu,corpus_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "\n",
    "searcher = GreedySearchDecoder(encoder, decoder)\n",
    "gram1_bleu_score = []\n",
    "gram2_bleu_score = []\n",
    "for i in range(0,len(testpairs),1):\n",
    "  \n",
    "  input_sentence = testpairs[i][0]\n",
    "  \n",
    "  reference = testpairs[i][1:]\n",
    "  templist = []\n",
    "  for k in range(len(reference)):\n",
    "    if(reference[k]!=''):\n",
    "      temp = reference[k].split(' ')\n",
    "      templist.append(temp)\n",
    "  \n",
    "  \n",
    "  input_sentence = normalizeString(input_sentence)\n",
    "  output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "  output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "  chencherry = SmoothingFunction()\n",
    "#   print(output_words)\n",
    "#   print(templist)\n",
    "  score1 = sentence_bleu(templist,output_words,weights=(1, 0, 0, 0) ,smoothing_function=chencherry.method1)\n",
    "  score2 = sentence_bleu(templist,output_words,weights=(0.5, 0.5, 0, 0),smoothing_function=chencherry.method1) \n",
    "  gram1_bleu_score.append(score1)\n",
    "  gram2_bleu_score.append(score2)\n",
    "  if i%1000 == 0:\n",
    "    print(i,sum(gram1_bleu_score)/len(gram1_bleu_score),sum(gram2_bleu_score)/len(gram2_bleu_score))\n",
    "print(\"Total Bleu Score for 1 grams on testing pairs: \", sum(gram1_bleu_score)/len(gram1_bleu_score) )  \n",
    "print(\"Total Bleu Score for 2 grams on testing pairs: \", sum(gram2_bleu_score)/len(gram2_bleu_score) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: goodbye . . . . .\n",
      "Bot: i m crash override . it back .\n",
      "Bot: don t i understand ? and young .\n",
      "Bot: i m glad you understand . .\n"
     ]
    }
   ],
   "source": [
    "# Set dropout layers to eval mode\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "searcher = GreedySearchDecoder(encoder, decoder)\n",
    "\n",
    "# Begin chatting (uncomment and run the following line to begin)\n",
    "evaluateInput(encoder, decoder, searcher, voc)\n",
    "# input\n",
    "# Hi, how are you?\n",
    "# What\n",
    "# I don't understand you\n",
    "# hmm, good bye"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam search\n",
    "\n",
    "Beam search is an improved version of greedy search. It has a hyperparameter named beam size,  $k$. At time step 1, selecting $k$ tokens with the highest conditional probabilities. Each of them will be the first token of $k$ candidate output sequences, respectively. At each subsequent time step, based on the $k$ candidate output sequences at the previous time step, $k$ candidate output sequences has been selected with the highest conditional probabilities from $k|\\gamma|$ possible choices.\n",
    "\n",
    "<img src=\"img/lstm_beamsearch.PNG\" title=\"Beam search\" style=\"width: 640px;\" />\n",
    "\n",
    "the sequence with the highest of the following score as the output sequence has been chosen from the equation:\n",
    "\n",
    "$\\frac{1}{L^{\\alpha}}\\log{P(y_1,...,y_L)} = \\frac{1}{L^{\\alpha}} \\sum_{t'=1}^{L} \\log{P(y_{t'}|t_1,...,y_{t'-1}, c)}$\n",
    "\n",
    "Where $L$ is the length of the final candidate sequence and $\\alpha$ is usually set to 0.75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Beam Decoder**\n",
    "\n",
    "The difference between greedy search and beam search is decoder function. Thus, greedy search function name is greedy_decode, and beam search function name is beam_decode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence:\n",
    "    def __init__(self, decoder_hidden, last_idx=SOS_token, sentence_idxes=[], sentence_scores=[]):\n",
    "        if(len(sentence_idxes) != len(sentence_scores)):\n",
    "            raise ValueError(\"length of indexes and scores should be the same\")\n",
    "        self.decoder_hidden = decoder_hidden\n",
    "        self.last_idx = last_idx\n",
    "        self.sentence_idxes =  sentence_idxes\n",
    "        self.sentence_scores = sentence_scores\n",
    "\n",
    "    def avgScore(self):\n",
    "        if len(self.sentence_scores) == 0:\n",
    "            raise ValueError(\"Calculate average score of sentence, but got no word\")\n",
    "        # return mean of sentence_score\n",
    "        return sum(self.sentence_scores) / len(self.sentence_scores)\n",
    "\n",
    "    def addTopk(self, topi, topv, decoder_hidden, beam_size, voc):\n",
    "        topv = torch.log(topv)\n",
    "        terminates, sentences = [], []\n",
    "        for i in range(beam_size):\n",
    "            if topi[0][i] == EOS_token:\n",
    "                terminates.append(([voc.index2word[idx.item()] for idx in self.sentence_idxes] + ['<EOS>'],\n",
    "                                   self.avgScore())) \n",
    "                continue\n",
    "            idxes = self.sentence_idxes[:] \n",
    "            scores = self.sentence_scores[:] \n",
    "            idxes.append(topi[0][i])\n",
    "            scores.append(topv[0][i])\n",
    "            sentences.append(Sentence(decoder_hidden, topi[0][i], idxes, scores))\n",
    "        return terminates, sentences\n",
    "\n",
    "    def toWordScore(self, voc):\n",
    "        \n",
    "        words = []\n",
    "        for i in range(len(self.sentence_idxes)):\n",
    "            if self.sentence_idxes[i] == EOS_token:\n",
    "                words.append('<EOS>')\n",
    "            else:\n",
    "                words.append(voc.index2word[self.sentence_idxes[i].item()])\n",
    "       \n",
    "        if self.sentence_idxes[-1] != EOS_token:\n",
    "            words.append('<EOS>')\n",
    "        return (words, self.avgScore())\n",
    "\n",
    "    def __repr__(self):\n",
    "        res = f\"Sentence with indices {self.sentence_idxes} \"\n",
    "        res += f\"and scores {self.sentence_scores}\"\n",
    "        return res\n",
    "def beam_decode(decoder, decoder_hidden, encoder_outputs, voc, beam_size, max_length=MAX_LENGTH):\n",
    "    terminal_sentences, prev_top_sentences, next_top_sentences = [], [], []\n",
    "    prev_top_sentences.append(Sentence(decoder_hidden))\n",
    "    for i in range(max_length):\n",
    "        \n",
    "        for sentence in prev_top_sentences:\n",
    "            decoder_input = torch.LongTensor([[sentence.last_idx]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "\n",
    "            decoder_hidden = sentence.decoder_hidden\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(beam_size)\n",
    "            term, top = sentence.addTopk(topi, topv, decoder_hidden, beam_size, voc)\n",
    "            terminal_sentences.extend(term)\n",
    "            next_top_sentences.extend(top)\n",
    "           \n",
    "        \n",
    "        next_top_sentences.sort(key=lambda s: s.avgScore(), reverse=True)\n",
    "        prev_top_sentences = next_top_sentences[:beam_size]\n",
    "        next_top_sentences = []\n",
    "        \n",
    "\n",
    "    terminal_sentences += [sentence.toWordScore(voc) for sentence in prev_top_sentences]\n",
    "    terminal_sentences.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    n = min(len(terminal_sentences), 15)\n",
    "    return terminal_sentences[:n]\n",
    "\n",
    "\n",
    "\n",
    "class BeamSearchDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder, decoder, voc, beam_size=10):\n",
    "        super(BeamSearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.voc = voc\n",
    "        self.beam_size = beam_size\n",
    "\n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        \n",
    "        decoder_hidden = encoder_hidden[:self.decoder.n_layers]\n",
    "        \n",
    "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
    "        sentences = beam_decode(self.decoder, decoder_hidden, encoder_outputs, self.voc, self.beam_size, max_length)\n",
    "        \n",
    "        \n",
    "        all_tokens = [torch.tensor(self.voc.word2index.get(w, 0)) for w in sentences[0][0]]\n",
    "        return all_tokens, None\n",
    "\n",
    "    def __str__(self):\n",
    "        res = f\"BeamSearchDecoder with beam size {self.beam_size}\"\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'cb_model'\n",
    "attn_model = 'dot'\n",
    "\n",
    "hidden_size = 512\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 4\n",
    "dropout = 0.5\n",
    "batch_size = 256 \n",
    "loadFilename = None\n",
    "\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: 47.3%; Average loss: 2.8541\n",
      "Iteration: 2850; Percent complete: 47.5%; Average loss: 2.8507\n",
      "Iteration: 2860; Percent complete: 47.7%; Average loss: 2.8857\n",
      "Iteration: 2870; Percent complete: 47.8%; Average loss: 2.8646\n",
      "Iteration: 2880; Percent complete: 48.0%; Average loss: 2.8636\n",
      "Iteration: 2890; Percent complete: 48.2%; Average loss: 2.8416\n",
      "Iteration: 2900; Percent complete: 48.3%; Average loss: 2.8636\n",
      "Iteration: 2910; Percent complete: 48.5%; Average loss: 2.8614\n",
      "Iteration: 2920; Percent complete: 48.7%; Average loss: 2.8436\n",
      "Iteration: 2930; Percent complete: 48.8%; Average loss: 2.8137\n",
      "Iteration: 2940; Percent complete: 49.0%; Average loss: 2.8255\n",
      "Iteration: 2950; Percent complete: 49.2%; Average loss: 2.7930\n",
      "Iteration: 2960; Percent complete: 49.3%; Average loss: 2.8134\n",
      "Iteration: 2970; Percent complete: 49.5%; Average loss: 2.8483\n",
      "Iteration: 2980; Percent complete: 49.7%; Average loss: 2.8250\n",
      "Iteration: 2990; Percent complete: 49.8%; Average loss: 2.7964\n",
      "Iteration: 3000; Percent complete: 50.0%; Average loss: 2.7923\n",
      "Iteration: 3010; Percent complete: 50.2%; Average loss: 2.8217\n",
      "Iteration: 3020; Percent complete: 50.3%; Average loss: 2.7863\n",
      "Iteration: 3030; Percent complete: 50.5%; Average loss: 2.8048\n",
      "Iteration: 3040; Percent complete: 50.7%; Average loss: 2.8122\n",
      "Iteration: 3050; Percent complete: 50.8%; Average loss: 2.7759\n",
      "Iteration: 3060; Percent complete: 51.0%; Average loss: 2.7713\n",
      "Iteration: 3070; Percent complete: 51.2%; Average loss: 2.8139\n",
      "Iteration: 3080; Percent complete: 51.3%; Average loss: 2.7610\n",
      "Iteration: 3090; Percent complete: 51.5%; Average loss: 2.7610\n",
      "Iteration: 3100; Percent complete: 51.7%; Average loss: 2.7535\n",
      "Iteration: 3110; Percent complete: 51.8%; Average loss: 2.7708\n",
      "Iteration: 3120; Percent complete: 52.0%; Average loss: 2.7539\n",
      "Iteration: 3130; Percent complete: 52.2%; Average loss: 2.7411\n",
      "Iteration: 3140; Percent complete: 52.3%; Average loss: 2.7648\n",
      "Iteration: 3150; Percent complete: 52.5%; Average loss: 2.7635\n",
      "Iteration: 3160; Percent complete: 52.7%; Average loss: 2.7595\n",
      "Iteration: 3170; Percent complete: 52.8%; Average loss: 2.7443\n",
      "Iteration: 3180; Percent complete: 53.0%; Average loss: 2.7561\n",
      "Iteration: 3190; Percent complete: 53.2%; Average loss: 2.7742\n",
      "Iteration: 3200; Percent complete: 53.3%; Average loss: 2.7178\n",
      "Iteration: 3210; Percent complete: 53.5%; Average loss: 2.7454\n",
      "Iteration: 3220; Percent complete: 53.7%; Average loss: 2.7598\n",
      "Iteration: 3230; Percent complete: 53.8%; Average loss: 2.6945\n",
      "Iteration: 3240; Percent complete: 54.0%; Average loss: 2.7174\n",
      "Iteration: 3250; Percent complete: 54.2%; Average loss: 2.7026\n",
      "Iteration: 3260; Percent complete: 54.3%; Average loss: 2.7203\n",
      "Iteration: 3270; Percent complete: 54.5%; Average loss: 2.7061\n",
      "Iteration: 3280; Percent complete: 54.7%; Average loss: 2.7060\n",
      "Iteration: 3290; Percent complete: 54.8%; Average loss: 2.6679\n",
      "Iteration: 3300; Percent complete: 55.0%; Average loss: 2.6967\n",
      "Iteration: 3310; Percent complete: 55.2%; Average loss: 2.6948\n",
      "Iteration: 3320; Percent complete: 55.3%; Average loss: 2.7125\n",
      "Iteration: 3330; Percent complete: 55.5%; Average loss: 2.6804\n",
      "Iteration: 3340; Percent complete: 55.7%; Average loss: 2.7173\n",
      "Iteration: 3350; Percent complete: 55.8%; Average loss: 2.6794\n",
      "Iteration: 3360; Percent complete: 56.0%; Average loss: 2.6620\n",
      "Iteration: 3370; Percent complete: 56.2%; Average loss: 2.6643\n",
      "Iteration: 3380; Percent complete: 56.3%; Average loss: 2.6834\n",
      "Iteration: 3390; Percent complete: 56.5%; Average loss: 2.6682\n",
      "Iteration: 3400; Percent complete: 56.7%; Average loss: 2.6754\n",
      "Iteration: 3410; Percent complete: 56.8%; Average loss: 2.6455\n",
      "Iteration: 3420; Percent complete: 57.0%; Average loss: 2.6437\n",
      "Iteration: 3430; Percent complete: 57.2%; Average loss: 2.6416\n",
      "Iteration: 3440; Percent complete: 57.3%; Average loss: 2.6066\n",
      "Iteration: 3450; Percent complete: 57.5%; Average loss: 2.6467\n",
      "Iteration: 3460; Percent complete: 57.7%; Average loss: 2.6392\n",
      "Iteration: 3470; Percent complete: 57.8%; Average loss: 2.6464\n",
      "Iteration: 3480; Percent complete: 58.0%; Average loss: 2.6183\n",
      "Iteration: 3490; Percent complete: 58.2%; Average loss: 2.6290\n",
      "Iteration: 3500; Percent complete: 58.3%; Average loss: 2.6283\n",
      "Iteration: 3510; Percent complete: 58.5%; Average loss: 2.5991\n",
      "Iteration: 3520; Percent complete: 58.7%; Average loss: 2.5868\n",
      "Iteration: 3530; Percent complete: 58.8%; Average loss: 2.5912\n",
      "Iteration: 3540; Percent complete: 59.0%; Average loss: 2.5802\n",
      "Iteration: 3550; Percent complete: 59.2%; Average loss: 2.6047\n",
      "Iteration: 3560; Percent complete: 59.3%; Average loss: 2.5794\n",
      "Iteration: 3570; Percent complete: 59.5%; Average loss: 2.5685\n",
      "Iteration: 3580; Percent complete: 59.7%; Average loss: 2.5912\n",
      "Iteration: 3590; Percent complete: 59.8%; Average loss: 2.5497\n",
      "Iteration: 3600; Percent complete: 60.0%; Average loss: 2.5758\n",
      "Iteration: 3610; Percent complete: 60.2%; Average loss: 2.5776\n",
      "Iteration: 3620; Percent complete: 60.3%; Average loss: 2.5848\n",
      "Iteration: 3630; Percent complete: 60.5%; Average loss: 2.5788\n",
      "Iteration: 3640; Percent complete: 60.7%; Average loss: 2.5542\n",
      "Iteration: 3650; Percent complete: 60.8%; Average loss: 2.5524\n",
      "Iteration: 3660; Percent complete: 61.0%; Average loss: 2.6002\n",
      "Iteration: 3670; Percent complete: 61.2%; Average loss: 2.5539\n",
      "Iteration: 3680; Percent complete: 61.3%; Average loss: 2.5146\n",
      "Iteration: 3690; Percent complete: 61.5%; Average loss: 2.5033\n",
      "Iteration: 3700; Percent complete: 61.7%; Average loss: 2.4924\n",
      "Iteration: 3710; Percent complete: 61.8%; Average loss: 2.5571\n",
      "Iteration: 3720; Percent complete: 62.0%; Average loss: 2.5312\n",
      "Iteration: 3730; Percent complete: 62.2%; Average loss: 2.5084\n",
      "Iteration: 3740; Percent complete: 62.3%; Average loss: 2.5427\n",
      "Iteration: 3750; Percent complete: 62.5%; Average loss: 2.5200\n",
      "Iteration: 3760; Percent complete: 62.7%; Average loss: 2.5318\n",
      "Iteration: 3770; Percent complete: 62.8%; Average loss: 2.5212\n",
      "Iteration: 3780; Percent complete: 63.0%; Average loss: 2.5365\n",
      "Iteration: 3790; Percent complete: 63.2%; Average loss: 2.5097\n",
      "Iteration: 3800; Percent complete: 63.3%; Average loss: 2.4775\n",
      "Iteration: 3810; Percent complete: 63.5%; Average loss: 2.4921\n",
      "Iteration: 3820; Percent complete: 63.7%; Average loss: 2.4915\n",
      "Iteration: 3830; Percent complete: 63.8%; Average loss: 2.5251\n",
      "Iteration: 3840; Percent complete: 64.0%; Average loss: 2.4980\n",
      "Iteration: 3850; Percent complete: 64.2%; Average loss: 2.4695\n",
      "Iteration: 3860; Percent complete: 64.3%; Average loss: 2.4826\n",
      "Iteration: 3870; Percent complete: 64.5%; Average loss: 2.4876\n",
      "Iteration: 3880; Percent complete: 64.7%; Average loss: 2.4928\n",
      "Iteration: 3890; Percent complete: 64.8%; Average loss: 2.4575\n",
      "Iteration: 3900; Percent complete: 65.0%; Average loss: 2.4663\n",
      "Iteration: 3910; Percent complete: 65.2%; Average loss: 2.4760\n",
      "Iteration: 3920; Percent complete: 65.3%; Average loss: 2.4548\n",
      "Iteration: 3930; Percent complete: 65.5%; Average loss: 2.4867\n",
      "Iteration: 3940; Percent complete: 65.7%; Average loss: 2.4360\n",
      "Iteration: 3950; Percent complete: 65.8%; Average loss: 2.4558\n",
      "Iteration: 3960; Percent complete: 66.0%; Average loss: 2.4572\n",
      "Iteration: 3970; Percent complete: 66.2%; Average loss: 2.4188\n",
      "Iteration: 3980; Percent complete: 66.3%; Average loss: 2.4073\n",
      "Iteration: 3990; Percent complete: 66.5%; Average loss: 2.4536\n",
      "Iteration: 4000; Percent complete: 66.7%; Average loss: 2.4279\n",
      "content/cb_model/Chat/2-4_512\n",
      "Iteration: 4010; Percent complete: 66.8%; Average loss: 2.3952\n",
      "Iteration: 4020; Percent complete: 67.0%; Average loss: 2.4385\n",
      "Iteration: 4030; Percent complete: 67.2%; Average loss: 2.4111\n",
      "Iteration: 4040; Percent complete: 67.3%; Average loss: 2.4121\n",
      "Iteration: 4050; Percent complete: 67.5%; Average loss: 2.4251\n",
      "Iteration: 4060; Percent complete: 67.7%; Average loss: 2.3531\n",
      "Iteration: 4070; Percent complete: 67.8%; Average loss: 2.3925\n",
      "Iteration: 4080; Percent complete: 68.0%; Average loss: 2.3914\n",
      "Iteration: 4090; Percent complete: 68.2%; Average loss: 2.3819\n",
      "Iteration: 4100; Percent complete: 68.3%; Average loss: 2.3829\n",
      "Iteration: 4110; Percent complete: 68.5%; Average loss: 2.3883\n",
      "Iteration: 4120; Percent complete: 68.7%; Average loss: 2.3877\n",
      "Iteration: 4130; Percent complete: 68.8%; Average loss: 2.3844\n",
      "Iteration: 4140; Percent complete: 69.0%; Average loss: 2.3863\n",
      "Iteration: 4150; Percent complete: 69.2%; Average loss: 2.3886\n",
      "Iteration: 4160; Percent complete: 69.3%; Average loss: 2.3666\n",
      "Iteration: 4170; Percent complete: 69.5%; Average loss: 2.3692\n",
      "Iteration: 4180; Percent complete: 69.7%; Average loss: 2.3824\n",
      "Iteration: 4190; Percent complete: 69.8%; Average loss: 2.3397\n",
      "Iteration: 4200; Percent complete: 70.0%; Average loss: 2.3358\n",
      "Iteration: 4210; Percent complete: 70.2%; Average loss: 2.3609\n",
      "Iteration: 4220; Percent complete: 70.3%; Average loss: 2.3836\n",
      "Iteration: 4230; Percent complete: 70.5%; Average loss: 2.3325\n",
      "Iteration: 4240; Percent complete: 70.7%; Average loss: 2.3713\n",
      "Iteration: 4250; Percent complete: 70.8%; Average loss: 2.3542\n",
      "Iteration: 4260; Percent complete: 71.0%; Average loss: 2.3207\n",
      "Iteration: 4270; Percent complete: 71.2%; Average loss: 2.3401\n",
      "Iteration: 4280; Percent complete: 71.3%; Average loss: 2.3340\n",
      "Iteration: 4290; Percent complete: 71.5%; Average loss: 2.3038\n",
      "Iteration: 4300; Percent complete: 71.7%; Average loss: 2.3152\n",
      "Iteration: 4310; Percent complete: 71.8%; Average loss: 2.2742\n",
      "Iteration: 4320; Percent complete: 72.0%; Average loss: 2.3416\n",
      "Iteration: 4330; Percent complete: 72.2%; Average loss: 2.3172\n",
      "Iteration: 4340; Percent complete: 72.3%; Average loss: 2.3135\n",
      "Iteration: 4350; Percent complete: 72.5%; Average loss: 2.2840\n",
      "Iteration: 4360; Percent complete: 72.7%; Average loss: 2.3006\n",
      "Iteration: 4370; Percent complete: 72.8%; Average loss: 2.3059\n",
      "Iteration: 4380; Percent complete: 73.0%; Average loss: 2.2675\n",
      "Iteration: 4390; Percent complete: 73.2%; Average loss: 2.2876\n",
      "Iteration: 4400; Percent complete: 73.3%; Average loss: 2.2641\n",
      "Iteration: 4410; Percent complete: 73.5%; Average loss: 2.2726\n",
      "Iteration: 4420; Percent complete: 73.7%; Average loss: 2.2985\n",
      "Iteration: 4430; Percent complete: 73.8%; Average loss: 2.2593\n",
      "Iteration: 4440; Percent complete: 74.0%; Average loss: 2.2978\n",
      "Iteration: 4450; Percent complete: 74.2%; Average loss: 2.2750\n",
      "Iteration: 4460; Percent complete: 74.3%; Average loss: 2.2408\n",
      "Iteration: 4470; Percent complete: 74.5%; Average loss: 2.2340\n",
      "Iteration: 4480; Percent complete: 74.7%; Average loss: 2.2244\n",
      "Iteration: 4490; Percent complete: 74.8%; Average loss: 2.2277\n",
      "Iteration: 4500; Percent complete: 75.0%; Average loss: 2.2336\n",
      "Iteration: 4510; Percent complete: 75.2%; Average loss: 2.2446\n",
      "Iteration: 4520; Percent complete: 75.3%; Average loss: 2.2525\n",
      "Iteration: 4530; Percent complete: 75.5%; Average loss: 2.2181\n",
      "Iteration: 4540; Percent complete: 75.7%; Average loss: 2.2087\n",
      "Iteration: 4550; Percent complete: 75.8%; Average loss: 2.2103\n",
      "Iteration: 4560; Percent complete: 76.0%; Average loss: 2.2181\n",
      "Iteration: 4570; Percent complete: 76.2%; Average loss: 2.2227\n",
      "Iteration: 4580; Percent complete: 76.3%; Average loss: 2.2210\n",
      "Iteration: 4590; Percent complete: 76.5%; Average loss: 2.2128\n",
      "Iteration: 4600; Percent complete: 76.7%; Average loss: 2.1992\n",
      "Iteration: 4610; Percent complete: 76.8%; Average loss: 2.2121\n",
      "Iteration: 4620; Percent complete: 77.0%; Average loss: 2.1937\n",
      "Iteration: 4630; Percent complete: 77.2%; Average loss: 2.2208\n",
      "Iteration: 4640; Percent complete: 77.3%; Average loss: 2.2223\n",
      "Iteration: 4650; Percent complete: 77.5%; Average loss: 2.2014\n",
      "Iteration: 4660; Percent complete: 77.7%; Average loss: 2.1682\n",
      "Iteration: 4670; Percent complete: 77.8%; Average loss: 2.1840\n",
      "Iteration: 4680; Percent complete: 78.0%; Average loss: 2.2003\n",
      "Iteration: 4690; Percent complete: 78.2%; Average loss: 2.1432\n",
      "Iteration: 4700; Percent complete: 78.3%; Average loss: 2.1720\n",
      "Iteration: 4710; Percent complete: 78.5%; Average loss: 2.1645\n",
      "Iteration: 4720; Percent complete: 78.7%; Average loss: 2.1647\n",
      "Iteration: 4730; Percent complete: 78.8%; Average loss: 2.1856\n",
      "Iteration: 4740; Percent complete: 79.0%; Average loss: 2.1833\n",
      "Iteration: 4750; Percent complete: 79.2%; Average loss: 2.1361\n",
      "Iteration: 4760; Percent complete: 79.3%; Average loss: 2.1740\n",
      "Iteration: 4770; Percent complete: 79.5%; Average loss: 2.1460\n",
      "Iteration: 4780; Percent complete: 79.7%; Average loss: 2.1609\n",
      "Iteration: 4790; Percent complete: 79.8%; Average loss: 2.1291\n",
      "Iteration: 4800; Percent complete: 80.0%; Average loss: 2.1304\n",
      "Iteration: 4810; Percent complete: 80.2%; Average loss: 2.1321\n",
      "Iteration: 4820; Percent complete: 80.3%; Average loss: 2.1143\n",
      "Iteration: 4830; Percent complete: 80.5%; Average loss: 2.1475\n",
      "Iteration: 4840; Percent complete: 80.7%; Average loss: 2.1015\n",
      "Iteration: 4850; Percent complete: 80.8%; Average loss: 2.0860\n",
      "Iteration: 4860; Percent complete: 81.0%; Average loss: 2.1068\n",
      "Iteration: 4870; Percent complete: 81.2%; Average loss: 2.1350\n",
      "Iteration: 4880; Percent complete: 81.3%; Average loss: 2.1382\n",
      "Iteration: 4890; Percent complete: 81.5%; Average loss: 2.0934\n",
      "Iteration: 4900; Percent complete: 81.7%; Average loss: 2.0960\n",
      "Iteration: 4910; Percent complete: 81.8%; Average loss: 2.1052\n",
      "Iteration: 4920; Percent complete: 82.0%; Average loss: 2.1286\n",
      "Iteration: 4930; Percent complete: 82.2%; Average loss: 2.0751\n",
      "Iteration: 4940; Percent complete: 82.3%; Average loss: 2.1116\n",
      "Iteration: 4950; Percent complete: 82.5%; Average loss: 2.0852\n",
      "Iteration: 4960; Percent complete: 82.7%; Average loss: 2.0686\n",
      "Iteration: 4970; Percent complete: 82.8%; Average loss: 2.0851\n",
      "Iteration: 4980; Percent complete: 83.0%; Average loss: 2.0591\n",
      "Iteration: 4990; Percent complete: 83.2%; Average loss: 2.0589\n",
      "Iteration: 5000; Percent complete: 83.3%; Average loss: 2.1015\n",
      "Iteration: 5010; Percent complete: 83.5%; Average loss: 2.0340\n",
      "Iteration: 5020; Percent complete: 83.7%; Average loss: 2.0759\n",
      "Iteration: 5030; Percent complete: 83.8%; Average loss: 2.0694\n",
      "Iteration: 5040; Percent complete: 84.0%; Average loss: 2.0300\n",
      "Iteration: 5050; Percent complete: 84.2%; Average loss: 2.0269\n",
      "Iteration: 5060; Percent complete: 84.3%; Average loss: 2.0233\n",
      "Iteration: 5070; Percent complete: 84.5%; Average loss: 2.0366\n",
      "Iteration: 5080; Percent complete: 84.7%; Average loss: 2.0548\n",
      "Iteration: 5090; Percent complete: 84.8%; Average loss: 2.0171\n",
      "Iteration: 5100; Percent complete: 85.0%; Average loss: 2.0194\n",
      "Iteration: 5110; Percent complete: 85.2%; Average loss: 2.0409\n",
      "Iteration: 5120; Percent complete: 85.3%; Average loss: 2.0378\n",
      "Iteration: 5130; Percent complete: 85.5%; Average loss: 2.0033\n",
      "Iteration: 5140; Percent complete: 85.7%; Average loss: 2.0268\n",
      "Iteration: 5150; Percent complete: 85.8%; Average loss: 2.0277\n",
      "Iteration: 5160; Percent complete: 86.0%; Average loss: 2.0090\n",
      "Iteration: 5170; Percent complete: 86.2%; Average loss: 2.0167\n",
      "Iteration: 5180; Percent complete: 86.3%; Average loss: 1.9995\n",
      "Iteration: 5190; Percent complete: 86.5%; Average loss: 1.9735\n",
      "Iteration: 5200; Percent complete: 86.7%; Average loss: 1.9917\n",
      "Iteration: 5210; Percent complete: 86.8%; Average loss: 1.9926\n",
      "Iteration: 5220; Percent complete: 87.0%; Average loss: 1.9889\n",
      "Iteration: 5230; Percent complete: 87.2%; Average loss: 1.9792\n",
      "Iteration: 5240; Percent complete: 87.3%; Average loss: 1.9749\n",
      "Iteration: 5250; Percent complete: 87.5%; Average loss: 1.9934\n",
      "Iteration: 5260; Percent complete: 87.7%; Average loss: 1.9710\n",
      "Iteration: 5270; Percent complete: 87.8%; Average loss: 1.9871\n",
      "Iteration: 5280; Percent complete: 88.0%; Average loss: 1.9755\n",
      "Iteration: 5290; Percent complete: 88.2%; Average loss: 1.9822\n",
      "Iteration: 5300; Percent complete: 88.3%; Average loss: 1.9826\n",
      "Iteration: 5310; Percent complete: 88.5%; Average loss: 1.9659\n",
      "Iteration: 5320; Percent complete: 88.7%; Average loss: 1.9720\n",
      "Iteration: 5330; Percent complete: 88.8%; Average loss: 1.9519\n",
      "Iteration: 5340; Percent complete: 89.0%; Average loss: 1.9185\n",
      "Iteration: 5350; Percent complete: 89.2%; Average loss: 1.9285\n",
      "Iteration: 5360; Percent complete: 89.3%; Average loss: 1.9179\n",
      "Iteration: 5370; Percent complete: 89.5%; Average loss: 1.9257\n",
      "Iteration: 5380; Percent complete: 89.7%; Average loss: 1.9504\n",
      "Iteration: 5390; Percent complete: 89.8%; Average loss: 1.9408\n",
      "Iteration: 5400; Percent complete: 90.0%; Average loss: 1.9386\n",
      "Iteration: 5410; Percent complete: 90.2%; Average loss: 1.9640\n",
      "Iteration: 5420; Percent complete: 90.3%; Average loss: 1.8969\n",
      "Iteration: 5430; Percent complete: 90.5%; Average loss: 1.9123\n",
      "Iteration: 5440; Percent complete: 90.7%; Average loss: 1.9172\n",
      "Iteration: 5450; Percent complete: 90.8%; Average loss: 1.9188\n",
      "Iteration: 5460; Percent complete: 91.0%; Average loss: 1.8925\n",
      "Iteration: 5470; Percent complete: 91.2%; Average loss: 1.9084\n",
      "Iteration: 5480; Percent complete: 91.3%; Average loss: 1.9033\n",
      "Iteration: 5490; Percent complete: 91.5%; Average loss: 1.9053\n",
      "Iteration: 5500; Percent complete: 91.7%; Average loss: 1.9043\n",
      "Iteration: 5510; Percent complete: 91.8%; Average loss: 1.8836\n",
      "Iteration: 5520; Percent complete: 92.0%; Average loss: 1.9319\n",
      "Iteration: 5530; Percent complete: 92.2%; Average loss: 1.8978\n",
      "Iteration: 5540; Percent complete: 92.3%; Average loss: 1.8738\n",
      "Iteration: 5550; Percent complete: 92.5%; Average loss: 1.8626\n",
      "Iteration: 5560; Percent complete: 92.7%; Average loss: 1.8661\n",
      "Iteration: 5570; Percent complete: 92.8%; Average loss: 1.8672\n",
      "Iteration: 5580; Percent complete: 93.0%; Average loss: 1.8595\n",
      "Iteration: 5590; Percent complete: 93.2%; Average loss: 1.8832\n",
      "Iteration: 5600; Percent complete: 93.3%; Average loss: 1.8412\n",
      "Iteration: 5610; Percent complete: 93.5%; Average loss: 1.8735\n",
      "Iteration: 5620; Percent complete: 93.7%; Average loss: 1.8015\n",
      "Iteration: 5630; Percent complete: 93.8%; Average loss: 1.8532\n",
      "Iteration: 5640; Percent complete: 94.0%; Average loss: 1.8378\n",
      "Iteration: 5650; Percent complete: 94.2%; Average loss: 1.8757\n",
      "Iteration: 5660; Percent complete: 94.3%; Average loss: 1.8483\n",
      "Iteration: 5670; Percent complete: 94.5%; Average loss: 1.8231\n",
      "Iteration: 5680; Percent complete: 94.7%; Average loss: 1.8262\n",
      "Iteration: 5690; Percent complete: 94.8%; Average loss: 1.8618\n",
      "Iteration: 5700; Percent complete: 95.0%; Average loss: 1.8334\n",
      "Iteration: 5710; Percent complete: 95.2%; Average loss: 1.8409\n",
      "Iteration: 5720; Percent complete: 95.3%; Average loss: 1.8022\n",
      "Iteration: 5730; Percent complete: 95.5%; Average loss: 1.8237\n",
      "Iteration: 5740; Percent complete: 95.7%; Average loss: 1.8185\n",
      "Iteration: 5750; Percent complete: 95.8%; Average loss: 1.8124\n",
      "Iteration: 5760; Percent complete: 96.0%; Average loss: 1.7865\n",
      "Iteration: 5770; Percent complete: 96.2%; Average loss: 1.8072\n",
      "Iteration: 5780; Percent complete: 96.3%; Average loss: 1.8165\n",
      "Iteration: 5790; Percent complete: 96.5%; Average loss: 1.7759\n",
      "Iteration: 5800; Percent complete: 96.7%; Average loss: 1.7897\n",
      "Iteration: 5810; Percent complete: 96.8%; Average loss: 1.7876\n",
      "Iteration: 5820; Percent complete: 97.0%; Average loss: 1.7800\n",
      "Iteration: 5830; Percent complete: 97.2%; Average loss: 1.7794\n",
      "Iteration: 5840; Percent complete: 97.3%; Average loss: 1.7892\n",
      "Iteration: 5850; Percent complete: 97.5%; Average loss: 1.7743\n",
      "Iteration: 5860; Percent complete: 97.7%; Average loss: 1.7858\n",
      "Iteration: 5870; Percent complete: 97.8%; Average loss: 1.7465\n",
      "Iteration: 5880; Percent complete: 98.0%; Average loss: 1.7486\n",
      "Iteration: 5890; Percent complete: 98.2%; Average loss: 1.7850\n",
      "Iteration: 5900; Percent complete: 98.3%; Average loss: 1.7484\n",
      "Iteration: 5910; Percent complete: 98.5%; Average loss: 1.7810\n",
      "Iteration: 5920; Percent complete: 98.7%; Average loss: 1.7582\n",
      "Iteration: 5930; Percent complete: 98.8%; Average loss: 1.7480\n",
      "Iteration: 5940; Percent complete: 99.0%; Average loss: 1.7526\n",
      "Iteration: 5950; Percent complete: 99.2%; Average loss: 1.7681\n",
      "Iteration: 5960; Percent complete: 99.3%; Average loss: 1.7234\n",
      "Iteration: 5970; Percent complete: 99.5%; Average loss: 1.7059\n",
      "Iteration: 5980; Percent complete: 99.7%; Average loss: 1.7184\n",
      "Iteration: 5990; Percent complete: 99.8%; Average loss: 1.7180\n",
      "Iteration: 6000; Percent complete: 100.0%; Average loss: 1.7259\n",
      "content/cb_model/Chat/2-4_512\n"
     ]
    }
   ],
   "source": [
    "save_dir = 'content/'\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_iteration = 6000\n",
    "print_every = 10\n",
    "save_every = 2000\n",
    "loadFilename = None\n",
    "corpus_name=\"Chat\"\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "print(\"Starting Training!\")\n",
    "lossvalues = trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
    "           print_every, save_every, clip, corpus_name, loadFilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Created with matplotlib (https://matplotlib.org/) -->\n",
       "<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 362.5625 248.518125\" width=\"362.5625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2021-03-30T23:24:26.155993</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 248.518125 \n",
       "L 362.5625 248.518125 \n",
       "L 362.5625 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill:none;\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 20.5625 224.64 \n",
       "L 355.3625 224.64 \n",
       "L 355.3625 7.2 \n",
       "L 20.5625 7.2 \n",
       "z\n",
       "\" style=\"fill:#ffffff;\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" id=\"m01b0389c88\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"35.780682\" xlink:href=\"#m01b0389c88\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(32.599432 239.238437)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path d=\"M 31.78125 66.40625 \n",
       "Q 24.171875 66.40625 20.328125 58.90625 \n",
       "Q 16.5 51.421875 16.5 36.375 \n",
       "Q 16.5 21.390625 20.328125 13.890625 \n",
       "Q 24.171875 6.390625 31.78125 6.390625 \n",
       "Q 39.453125 6.390625 43.28125 13.890625 \n",
       "Q 47.125 21.390625 47.125 36.375 \n",
       "Q 47.125 51.421875 43.28125 58.90625 \n",
       "Q 39.453125 66.40625 31.78125 66.40625 \n",
       "z\n",
       "M 31.78125 74.21875 \n",
       "Q 44.046875 74.21875 50.515625 64.515625 \n",
       "Q 56.984375 54.828125 56.984375 36.375 \n",
       "Q 56.984375 17.96875 50.515625 8.265625 \n",
       "Q 44.046875 -1.421875 31.78125 -1.421875 \n",
       "Q 19.53125 -1.421875 13.0625 8.265625 \n",
       "Q 6.59375 17.96875 6.59375 36.375 \n",
       "Q 6.59375 54.828125 13.0625 64.515625 \n",
       "Q 19.53125 74.21875 31.78125 74.21875 \n",
       "z\n",
       "\" id=\"DejaVuSans-48\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"86.592641\" xlink:href=\"#m01b0389c88\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 100 -->\n",
       "      <g transform=\"translate(77.048891 239.238437)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path d=\"M 12.40625 8.296875 \n",
       "L 28.515625 8.296875 \n",
       "L 28.515625 63.921875 \n",
       "L 10.984375 60.40625 \n",
       "L 10.984375 69.390625 \n",
       "L 28.421875 72.90625 \n",
       "L 38.28125 72.90625 \n",
       "L 38.28125 8.296875 \n",
       "L 54.390625 8.296875 \n",
       "L 54.390625 0 \n",
       "L 12.40625 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-49\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"137.4046\" xlink:href=\"#m01b0389c88\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 200 -->\n",
       "      <g transform=\"translate(127.86085 239.238437)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path d=\"M 19.1875 8.296875 \n",
       "L 53.609375 8.296875 \n",
       "L 53.609375 0 \n",
       "L 7.328125 0 \n",
       "L 7.328125 8.296875 \n",
       "Q 12.9375 14.109375 22.625 23.890625 \n",
       "Q 32.328125 33.6875 34.8125 36.53125 \n",
       "Q 39.546875 41.84375 41.421875 45.53125 \n",
       "Q 43.3125 49.21875 43.3125 52.78125 \n",
       "Q 43.3125 58.59375 39.234375 62.25 \n",
       "Q 35.15625 65.921875 28.609375 65.921875 \n",
       "Q 23.96875 65.921875 18.8125 64.3125 \n",
       "Q 13.671875 62.703125 7.8125 59.421875 \n",
       "L 7.8125 69.390625 \n",
       "Q 13.765625 71.78125 18.9375 73 \n",
       "Q 24.125 74.21875 28.421875 74.21875 \n",
       "Q 39.75 74.21875 46.484375 68.546875 \n",
       "Q 53.21875 62.890625 53.21875 53.421875 \n",
       "Q 53.21875 48.921875 51.53125 44.890625 \n",
       "Q 49.859375 40.875 45.40625 35.40625 \n",
       "Q 44.1875 33.984375 37.640625 27.21875 \n",
       "Q 31.109375 20.453125 19.1875 8.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-50\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"188.21656\" xlink:href=\"#m01b0389c88\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 300 -->\n",
       "      <g transform=\"translate(178.67281 239.238437)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path d=\"M 40.578125 39.3125 \n",
       "Q 47.65625 37.796875 51.625 33 \n",
       "Q 55.609375 28.21875 55.609375 21.1875 \n",
       "Q 55.609375 10.40625 48.1875 4.484375 \n",
       "Q 40.765625 -1.421875 27.09375 -1.421875 \n",
       "Q 22.515625 -1.421875 17.65625 -0.515625 \n",
       "Q 12.796875 0.390625 7.625 2.203125 \n",
       "L 7.625 11.71875 \n",
       "Q 11.71875 9.328125 16.59375 8.109375 \n",
       "Q 21.484375 6.890625 26.8125 6.890625 \n",
       "Q 36.078125 6.890625 40.9375 10.546875 \n",
       "Q 45.796875 14.203125 45.796875 21.1875 \n",
       "Q 45.796875 27.640625 41.28125 31.265625 \n",
       "Q 36.765625 34.90625 28.71875 34.90625 \n",
       "L 20.21875 34.90625 \n",
       "L 20.21875 43.015625 \n",
       "L 29.109375 43.015625 \n",
       "Q 36.375 43.015625 40.234375 45.921875 \n",
       "Q 44.09375 48.828125 44.09375 54.296875 \n",
       "Q 44.09375 59.90625 40.109375 62.90625 \n",
       "Q 36.140625 65.921875 28.71875 65.921875 \n",
       "Q 24.65625 65.921875 20.015625 65.03125 \n",
       "Q 15.375 64.15625 9.8125 62.3125 \n",
       "L 9.8125 71.09375 \n",
       "Q 15.4375 72.65625 20.34375 73.4375 \n",
       "Q 25.25 74.21875 29.59375 74.21875 \n",
       "Q 40.828125 74.21875 47.359375 69.109375 \n",
       "Q 53.90625 64.015625 53.90625 55.328125 \n",
       "Q 53.90625 49.265625 50.4375 45.09375 \n",
       "Q 46.96875 40.921875 40.578125 39.3125 \n",
       "z\n",
       "\" id=\"DejaVuSans-51\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-51\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"239.028519\" xlink:href=\"#m01b0389c88\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 400 -->\n",
       "      <g transform=\"translate(229.484769 239.238437)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path d=\"M 37.796875 64.3125 \n",
       "L 12.890625 25.390625 \n",
       "L 37.796875 25.390625 \n",
       "z\n",
       "M 35.203125 72.90625 \n",
       "L 47.609375 72.90625 \n",
       "L 47.609375 25.390625 \n",
       "L 58.015625 25.390625 \n",
       "L 58.015625 17.1875 \n",
       "L 47.609375 17.1875 \n",
       "L 47.609375 0 \n",
       "L 37.796875 0 \n",
       "L 37.796875 17.1875 \n",
       "L 4.890625 17.1875 \n",
       "L 4.890625 26.703125 \n",
       "z\n",
       "\" id=\"DejaVuSans-52\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-52\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"289.840478\" xlink:href=\"#m01b0389c88\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 500 -->\n",
       "      <g transform=\"translate(280.296728 239.238437)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path d=\"M 10.796875 72.90625 \n",
       "L 49.515625 72.90625 \n",
       "L 49.515625 64.59375 \n",
       "L 19.828125 64.59375 \n",
       "L 19.828125 46.734375 \n",
       "Q 21.96875 47.46875 24.109375 47.828125 \n",
       "Q 26.265625 48.1875 28.421875 48.1875 \n",
       "Q 40.625 48.1875 47.75 41.5 \n",
       "Q 54.890625 34.8125 54.890625 23.390625 \n",
       "Q 54.890625 11.625 47.5625 5.09375 \n",
       "Q 40.234375 -1.421875 26.90625 -1.421875 \n",
       "Q 22.3125 -1.421875 17.546875 -0.640625 \n",
       "Q 12.796875 0.140625 7.71875 1.703125 \n",
       "L 7.71875 11.625 \n",
       "Q 12.109375 9.234375 16.796875 8.0625 \n",
       "Q 21.484375 6.890625 26.703125 6.890625 \n",
       "Q 35.15625 6.890625 40.078125 11.328125 \n",
       "Q 45.015625 15.765625 45.015625 23.390625 \n",
       "Q 45.015625 31 40.078125 35.4375 \n",
       "Q 35.15625 39.890625 26.703125 39.890625 \n",
       "Q 22.75 39.890625 18.8125 39.015625 \n",
       "Q 14.890625 38.140625 10.796875 36.28125 \n",
       "z\n",
       "\" id=\"DejaVuSans-53\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-53\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"340.652438\" xlink:href=\"#m01b0389c88\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 600 -->\n",
       "      <g transform=\"translate(331.108688 239.238437)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path d=\"M 33.015625 40.375 \n",
       "Q 26.375 40.375 22.484375 35.828125 \n",
       "Q 18.609375 31.296875 18.609375 23.390625 \n",
       "Q 18.609375 15.53125 22.484375 10.953125 \n",
       "Q 26.375 6.390625 33.015625 6.390625 \n",
       "Q 39.65625 6.390625 43.53125 10.953125 \n",
       "Q 47.40625 15.53125 47.40625 23.390625 \n",
       "Q 47.40625 31.296875 43.53125 35.828125 \n",
       "Q 39.65625 40.375 33.015625 40.375 \n",
       "z\n",
       "M 52.59375 71.296875 \n",
       "L 52.59375 62.3125 \n",
       "Q 48.875 64.0625 45.09375 64.984375 \n",
       "Q 41.3125 65.921875 37.59375 65.921875 \n",
       "Q 27.828125 65.921875 22.671875 59.328125 \n",
       "Q 17.53125 52.734375 16.796875 39.40625 \n",
       "Q 19.671875 43.65625 24.015625 45.921875 \n",
       "Q 28.375 48.1875 33.59375 48.1875 \n",
       "Q 44.578125 48.1875 50.953125 41.515625 \n",
       "Q 57.328125 34.859375 57.328125 23.390625 \n",
       "Q 57.328125 12.15625 50.6875 5.359375 \n",
       "Q 44.046875 -1.421875 33.015625 -1.421875 \n",
       "Q 20.359375 -1.421875 13.671875 8.265625 \n",
       "Q 6.984375 17.96875 6.984375 36.375 \n",
       "Q 6.984375 53.65625 15.1875 63.9375 \n",
       "Q 23.390625 74.21875 37.203125 74.21875 \n",
       "Q 40.921875 74.21875 44.703125 73.484375 \n",
       "Q 48.484375 72.75 52.59375 71.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-54\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-54\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" id=\"m7076a0c65b\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m7076a0c65b\" y=\"205.92663\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(7.2 209.725849)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m7076a0c65b\" y=\"175.908236\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 3 -->\n",
       "      <g transform=\"translate(7.2 179.707455)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-51\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m7076a0c65b\" y=\"145.889842\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(7.2 149.689061)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-52\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m7076a0c65b\" y=\"115.871448\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 5 -->\n",
       "      <g transform=\"translate(7.2 119.670667)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m7076a0c65b\" y=\"85.853054\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 6 -->\n",
       "      <g transform=\"translate(7.2 89.652273)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-54\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m7076a0c65b\" y=\"55.83466\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 7 -->\n",
       "      <g transform=\"translate(7.2 59.633879)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path d=\"M 8.203125 72.90625 \n",
       "L 55.078125 72.90625 \n",
       "L 55.078125 68.703125 \n",
       "L 28.609375 0 \n",
       "L 18.3125 0 \n",
       "L 43.21875 64.59375 \n",
       "L 8.203125 64.59375 \n",
       "z\n",
       "\" id=\"DejaVuSans-55\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-55\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_7\">\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m7076a0c65b\" y=\"25.816266\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_14\">\n",
       "      <!-- 8 -->\n",
       "      <g transform=\"translate(7.2 29.615485)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path d=\"M 31.78125 34.625 \n",
       "Q 24.75 34.625 20.71875 30.859375 \n",
       "Q 16.703125 27.09375 16.703125 20.515625 \n",
       "Q 16.703125 13.921875 20.71875 10.15625 \n",
       "Q 24.75 6.390625 31.78125 6.390625 \n",
       "Q 38.8125 6.390625 42.859375 10.171875 \n",
       "Q 46.921875 13.96875 46.921875 20.515625 \n",
       "Q 46.921875 27.09375 42.890625 30.859375 \n",
       "Q 38.875 34.625 31.78125 34.625 \n",
       "z\n",
       "M 21.921875 38.8125 \n",
       "Q 15.578125 40.375 12.03125 44.71875 \n",
       "Q 8.5 49.078125 8.5 55.328125 \n",
       "Q 8.5 64.0625 14.71875 69.140625 \n",
       "Q 20.953125 74.21875 31.78125 74.21875 \n",
       "Q 42.671875 74.21875 48.875 69.140625 \n",
       "Q 55.078125 64.0625 55.078125 55.328125 \n",
       "Q 55.078125 49.078125 51.53125 44.71875 \n",
       "Q 48 40.375 41.703125 38.8125 \n",
       "Q 48.828125 37.15625 52.796875 32.3125 \n",
       "Q 56.78125 27.484375 56.78125 20.515625 \n",
       "Q 56.78125 9.90625 50.3125 4.234375 \n",
       "Q 43.84375 -1.421875 31.78125 -1.421875 \n",
       "Q 19.734375 -1.421875 13.25 4.234375 \n",
       "Q 6.78125 9.90625 6.78125 20.515625 \n",
       "Q 6.78125 27.484375 10.78125 32.3125 \n",
       "Q 14.796875 37.15625 21.921875 38.8125 \n",
       "z\n",
       "M 18.3125 54.390625 \n",
       "Q 18.3125 48.734375 21.84375 45.5625 \n",
       "Q 25.390625 42.390625 31.78125 42.390625 \n",
       "Q 38.140625 42.390625 41.71875 45.5625 \n",
       "Q 45.3125 48.734375 45.3125 54.390625 \n",
       "Q 45.3125 60.0625 41.71875 63.234375 \n",
       "Q 38.140625 66.40625 31.78125 66.40625 \n",
       "Q 25.390625 66.40625 21.84375 63.234375 \n",
       "Q 18.3125 60.0625 18.3125 54.390625 \n",
       "z\n",
       "\" id=\"DejaVuSans-56\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-56\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_15\">\n",
       "    <path clip-path=\"url(#p500af5bb16)\" d=\"M 35.780682 17.083636 \n",
       "L 36.288801 87.16678 \n",
       "L 36.796921 115.911326 \n",
       "L 37.305041 115.675592 \n",
       "L 37.81316 117.351907 \n",
       "L 38.32128 120.231647 \n",
       "L 38.829399 120.916639 \n",
       "L 39.337519 120.745212 \n",
       "L 39.845639 122.766344 \n",
       "L 40.353758 123.512153 \n",
       "L 40.861878 123.69998 \n",
       "L 41.369997 124.136388 \n",
       "L 41.878117 123.81136 \n",
       "L 42.386237 126.450196 \n",
       "L 42.894356 126.684974 \n",
       "L 43.910595 131.062289 \n",
       "L 44.418715 133.01304 \n",
       "L 44.926834 133.633053 \n",
       "L 45.434954 132.17791 \n",
       "L 45.943074 132.223884 \n",
       "L 46.451193 132.692337 \n",
       "L 46.959313 135.471521 \n",
       "L 47.467432 133.890146 \n",
       "L 47.975552 134.227609 \n",
       "L 48.483672 135.266062 \n",
       "L 48.991791 135.437419 \n",
       "L 49.499911 135.442777 \n",
       "L 50.00803 135.6427 \n",
       "L 50.51615 136.913803 \n",
       "L 51.02427 136.824669 \n",
       "L 51.532389 136.893675 \n",
       "L 52.040509 138.735596 \n",
       "L 52.548628 139.198709 \n",
       "L 53.056748 138.417052 \n",
       "L 53.564868 138.852309 \n",
       "L 54.072987 140.439243 \n",
       "L 54.581107 140.556001 \n",
       "L 55.089226 141.070116 \n",
       "L 55.597346 140.326434 \n",
       "L 56.105466 140.630922 \n",
       "L 56.613585 139.832255 \n",
       "L 57.121705 141.85746 \n",
       "L 57.629824 143.175352 \n",
       "L 58.646064 143.13656 \n",
       "L 59.154183 142.687319 \n",
       "L 59.662303 143.159149 \n",
       "L 60.170422 142.77052 \n",
       "L 60.678542 142.896077 \n",
       "L 61.186661 144.527265 \n",
       "L 61.694781 144.614925 \n",
       "L 62.202901 144.067689 \n",
       "L 62.71102 143.725453 \n",
       "L 63.21914 144.719041 \n",
       "L 63.727259 144.775554 \n",
       "L 64.235379 145.678081 \n",
       "L 64.743499 147.254913 \n",
       "L 65.251618 146.060494 \n",
       "L 65.759738 146.072126 \n",
       "L 66.267857 145.450535 \n",
       "L 66.775977 145.765989 \n",
       "L 67.284097 147.79973 \n",
       "L 67.792216 147.878056 \n",
       "L 68.300336 148.3943 \n",
       "L 68.808455 149.415393 \n",
       "L 69.316575 147.27999 \n",
       "L 70.332814 148.99797 \n",
       "L 70.840934 148.962195 \n",
       "L 71.349053 148.412836 \n",
       "L 71.857173 149.23996 \n",
       "L 72.365293 148.766941 \n",
       "L 72.873412 150.140247 \n",
       "L 73.381532 149.063714 \n",
       "L 74.397771 149.349826 \n",
       "L 74.90589 150.682952 \n",
       "L 75.41401 151.042495 \n",
       "L 75.92213 148.817127 \n",
       "L 76.430249 151.489789 \n",
       "L 76.938369 151.311901 \n",
       "L 77.446488 150.923536 \n",
       "L 77.954608 151.680902 \n",
       "L 78.462728 151.760232 \n",
       "L 78.970847 153.488652 \n",
       "L 79.478967 152.170019 \n",
       "L 80.495206 153.291372 \n",
       "L 81.003326 151.965538 \n",
       "L 81.511445 153.892467 \n",
       "L 82.019565 153.876697 \n",
       "L 82.527684 153.975391 \n",
       "L 83.035804 153.925147 \n",
       "L 83.543924 155.539122 \n",
       "L 84.052043 154.974723 \n",
       "L 84.560163 154.745715 \n",
       "L 85.068282 155.537378 \n",
       "L 85.576402 153.551818 \n",
       "L 86.084522 154.191772 \n",
       "L 86.592641 154.482322 \n",
       "L 88.117 156.516535 \n",
       "L 88.62512 156.556091 \n",
       "L 89.133239 157.648228 \n",
       "L 89.641359 154.758784 \n",
       "L 91.165717 157.165456 \n",
       "L 91.673837 157.234208 \n",
       "L 92.181957 157.970099 \n",
       "L 92.690076 157.055067 \n",
       "L 93.198196 157.242014 \n",
       "L 93.706315 157.724002 \n",
       "L 94.722555 158.438646 \n",
       "L 95.230674 158.184993 \n",
       "L 95.738794 158.141239 \n",
       "L 96.246913 158.315548 \n",
       "L 96.755033 159.870862 \n",
       "L 97.263153 158.872977 \n",
       "L 97.771272 159.150267 \n",
       "L 98.279392 158.800218 \n",
       "L 98.787511 159.94987 \n",
       "L 99.295631 158.968766 \n",
       "L 99.803751 159.752818 \n",
       "L 100.31187 159.02234 \n",
       "L 100.81999 159.762645 \n",
       "L 101.328109 160.900138 \n",
       "L 101.836229 159.627214 \n",
       "L 102.344349 161.426089 \n",
       "L 102.852468 159.79991 \n",
       "L 103.360588 161.455559 \n",
       "L 103.868707 160.87078 \n",
       "L 104.376827 160.694406 \n",
       "L 104.884947 160.846798 \n",
       "L 105.393066 162.414348 \n",
       "L 105.901186 161.026505 \n",
       "L 106.409305 160.841382 \n",
       "L 107.425544 162.23297 \n",
       "L 107.933664 162.249344 \n",
       "L 108.441784 162.539404 \n",
       "L 108.949903 162.495361 \n",
       "L 109.458023 161.544407 \n",
       "L 109.966142 162.33919 \n",
       "L 110.474262 163.672749 \n",
       "L 110.982382 162.864826 \n",
       "L 111.998621 162.791337 \n",
       "L 113.01486 163.579181 \n",
       "L 113.52298 162.562694 \n",
       "L 114.031099 163.512367 \n",
       "L 114.539219 163.453175 \n",
       "L 115.047338 163.281427 \n",
       "L 115.555458 164.632736 \n",
       "L 116.063578 163.7984 \n",
       "L 116.571697 164.639337 \n",
       "L 117.079817 164.096054 \n",
       "L 117.587936 164.999371 \n",
       "L 118.604176 164.221408 \n",
       "L 119.112295 163.157799 \n",
       "L 119.620415 166.10187 \n",
       "L 120.128534 165.29517 \n",
       "L 120.636654 163.133283 \n",
       "L 121.144773 164.582477 \n",
       "L 121.652893 166.439364 \n",
       "L 122.161013 167.195091 \n",
       "L 122.669132 166.88337 \n",
       "L 123.177252 166.885568 \n",
       "L 123.685371 166.040212 \n",
       "L 124.193491 166.030778 \n",
       "L 124.701611 165.824901 \n",
       "L 125.20973 164.812388 \n",
       "L 125.71785 165.87066 \n",
       "L 126.225969 165.792899 \n",
       "L 126.734089 166.848858 \n",
       "L 127.242209 168.285016 \n",
       "L 127.750328 167.351189 \n",
       "L 128.258448 168.290342 \n",
       "L 128.766567 167.874084 \n",
       "L 129.274687 167.217211 \n",
       "L 129.782807 167.513239 \n",
       "L 130.290926 167.678352 \n",
       "L 130.799046 167.373615 \n",
       "L 131.307165 167.280251 \n",
       "L 132.831524 169.513845 \n",
       "L 133.339644 168.525271 \n",
       "L 133.847763 169.642519 \n",
       "L 134.355883 167.98788 \n",
       "L 134.864003 168.859825 \n",
       "L 135.880242 169.517745 \n",
       "L 136.388361 168.169092 \n",
       "L 136.896481 169.780851 \n",
       "L 137.4046 168.853816 \n",
       "L 137.91272 171.081721 \n",
       "L 138.42084 169.350706 \n",
       "L 138.928959 170.15515 \n",
       "L 139.437079 169.811914 \n",
       "L 139.945198 169.875305 \n",
       "L 140.453318 171.254596 \n",
       "L 140.961438 171.249633 \n",
       "L 141.469557 169.968901 \n",
       "L 141.977677 171.748015 \n",
       "L 142.485796 170.762612 \n",
       "L 142.993916 170.153405 \n",
       "L 143.502036 170.686248 \n",
       "L 144.010155 170.200737 \n",
       "L 145.026394 171.730621 \n",
       "L 145.534514 171.3376 \n",
       "L 146.042634 171.91318 \n",
       "L 147.058873 170.142795 \n",
       "L 147.566992 172.217564 \n",
       "L 148.075112 171.822981 \n",
       "L 148.583232 172.783053 \n",
       "L 149.091351 172.559983 \n",
       "L 149.599471 172.468253 \n",
       "L 150.10759 172.953484 \n",
       "L 150.61571 171.548287 \n",
       "L 151.123829 173.139633 \n",
       "L 151.631949 172.381812 \n",
       "L 152.648188 173.692127 \n",
       "L 153.156308 173.867668 \n",
       "L 153.664427 173.631832 \n",
       "L 154.172547 173.03802 \n",
       "L 154.680667 173.265967 \n",
       "L 155.188786 174.432593 \n",
       "L 155.696906 172.87811 \n",
       "L 156.205025 176.014805 \n",
       "L 157.221265 173.824299 \n",
       "L 157.729384 174.955985 \n",
       "L 158.237504 174.906374 \n",
       "L 158.745623 175.062366 \n",
       "L 159.253743 174.631362 \n",
       "L 159.761863 176.045815 \n",
       "L 160.269982 175.726407 \n",
       "L 160.778102 175.084148 \n",
       "L 161.286221 175.671025 \n",
       "L 161.794341 175.089434 \n",
       "L 162.302461 175.724345 \n",
       "L 162.81058 175.881048 \n",
       "L 163.3187 175.506797 \n",
       "L 163.826819 175.793285 \n",
       "L 164.334939 175.836489 \n",
       "L 164.843059 176.317895 \n",
       "L 165.351178 176.400987 \n",
       "L 165.859298 177.657093 \n",
       "L 166.367417 177.638881 \n",
       "L 166.875537 176.489424 \n",
       "L 167.383656 176.560146 \n",
       "L 167.891776 176.134948 \n",
       "L 168.399896 177.302533 \n",
       "L 168.908015 177.85662 \n",
       "L 169.416135 177.005829 \n",
       "L 170.940494 177.666735 \n",
       "L 171.448613 178.137766 \n",
       "L 171.956733 178.288688 \n",
       "L 172.464852 178.759244 \n",
       "L 172.972972 178.273362 \n",
       "L 173.989211 178.469871 \n",
       "L 174.497331 178.705287 \n",
       "L 175.00545 178.267384 \n",
       "L 175.51357 178.830453 \n",
       "L 176.02169 178.803911 \n",
       "L 176.529809 178.029071 \n",
       "L 177.037929 179.535654 \n",
       "L 177.546048 178.882006 \n",
       "L 178.054168 179.0111 \n",
       "L 178.562288 180.145883 \n",
       "L 179.070407 179.554452 \n",
       "L 179.578527 180.286682 \n",
       "L 180.086646 180.389148 \n",
       "L 180.594766 179.339743 \n",
       "L 181.102885 179.971947 \n",
       "L 181.611005 180.003944 \n",
       "L 182.119125 180.66181 \n",
       "L 182.627244 180.001641 \n",
       "L 183.135364 180.069473 \n",
       "L 183.643483 180.60328 \n",
       "L 184.151603 181.502127 \n",
       "L 184.659723 181.145244 \n",
       "L 185.167842 182.121267 \n",
       "L 185.675962 181.508641 \n",
       "L 186.184081 180.461574 \n",
       "L 187.200321 182.021473 \n",
       "L 187.70844 182.141648 \n",
       "L 188.21656 181.261321 \n",
       "L 188.724679 182.321714 \n",
       "L 189.232799 181.766589 \n",
       "L 189.740919 181.547058 \n",
       "L 190.249038 182.634657 \n",
       "L 190.757158 182.774484 \n",
       "L 191.265277 181.494298 \n",
       "L 191.773397 183.083564 \n",
       "L 192.281517 183.083834 \n",
       "L 192.789636 183.307157 \n",
       "L 193.297756 182.787707 \n",
       "L 194.313995 183.679386 \n",
       "L 194.822115 182.967551 \n",
       "L 195.838354 183.127063 \n",
       "L 196.346473 183.584 \n",
       "L 196.854593 183.228305 \n",
       "L 197.362712 182.687214 \n",
       "L 197.870832 184.379476 \n",
       "L 198.378952 183.5502 \n",
       "L 198.887071 183.120087 \n",
       "L 199.395191 185.079871 \n",
       "L 199.90331 184.392763 \n",
       "L 200.41143 184.836476 \n",
       "L 200.91955 184.305525 \n",
       "L 201.427669 184.731929 \n",
       "L 201.935789 184.734259 \n",
       "L 202.443908 185.878286 \n",
       "L 202.952028 185.012625 \n",
       "L 203.460148 185.069396 \n",
       "L 203.968267 184.538312 \n",
       "L 204.476387 185.501732 \n",
       "L 204.984506 184.393705 \n",
       "L 205.492626 185.532654 \n",
       "L 206.000746 186.055747 \n",
       "L 206.508865 185.984045 \n",
       "L 207.016985 185.412337 \n",
       "L 207.525104 185.868686 \n",
       "L 208.033224 185.651671 \n",
       "L 208.541344 186.550206 \n",
       "L 209.557583 186.665419 \n",
       "L 210.065702 187.718316 \n",
       "L 210.573822 186.514621 \n",
       "L 211.081941 186.738921 \n",
       "L 211.590061 186.523293 \n",
       "L 212.098181 187.365768 \n",
       "L 212.6063 187.045365 \n",
       "L 213.11442 187.067312 \n",
       "L 213.622539 187.943568 \n",
       "L 214.130659 188.311765 \n",
       "L 214.638779 188.180649 \n",
       "L 215.146898 188.509738 \n",
       "L 215.655018 187.774231 \n",
       "L 216.163137 188.534051 \n",
       "L 216.671257 188.860574 \n",
       "L 217.179377 188.180487 \n",
       "L 217.687496 189.426277 \n",
       "L 218.195616 188.643026 \n",
       "L 218.703735 188.587088 \n",
       "L 219.211855 188.371447 \n",
       "L 219.719975 188.552236 \n",
       "L 220.228094 189.289006 \n",
       "L 220.736214 189.344324 \n",
       "L 221.244333 187.911017 \n",
       "L 222.260573 190.478158 \n",
       "L 223.276812 191.145832 \n",
       "L 223.784931 189.203852 \n",
       "L 224.801171 190.665197 \n",
       "L 225.30929 189.6366 \n",
       "L 225.81741 190.316052 \n",
       "L 226.325529 189.961719 \n",
       "L 226.833649 190.280307 \n",
       "L 227.341768 189.8209 \n",
       "L 228.358008 191.591973 \n",
       "L 228.866127 191.154142 \n",
       "L 229.374247 191.17208 \n",
       "L 229.882366 190.163471 \n",
       "L 230.898606 191.83439 \n",
       "L 231.406725 191.4392 \n",
       "L 232.422964 191.133768 \n",
       "L 232.931084 192.193606 \n",
       "L 233.947323 191.638609 \n",
       "L 234.455443 192.273197 \n",
       "L 234.963562 191.315445 \n",
       "L 235.471682 192.839517 \n",
       "L 235.979802 192.24297 \n",
       "L 236.487921 192.201845 \n",
       "L 236.996041 193.35504 \n",
       "L 237.50416 193.698815 \n",
       "L 238.01228 192.310521 \n",
       "L 238.5204 193.080699 \n",
       "L 239.028519 194.064283 \n",
       "L 239.536639 192.762814 \n",
       "L 240.044758 193.587388 \n",
       "L 240.552878 193.556752 \n",
       "L 241.060997 193.16672 \n",
       "L 241.569117 195.326824 \n",
       "L 242.077237 194.143933 \n",
       "L 242.585356 194.178565 \n",
       "L 243.093476 194.462521 \n",
       "L 243.601595 194.431554 \n",
       "L 244.109715 194.269129 \n",
       "L 246.142193 194.262624 \n",
       "L 246.650313 194.920812 \n",
       "L 247.158433 194.844744 \n",
       "L 247.666552 194.448856 \n",
       "L 248.174672 195.730118 \n",
       "L 248.682791 195.847399 \n",
       "L 249.699031 194.410721 \n",
       "L 250.20715 195.944277 \n",
       "L 250.71527 194.781426 \n",
       "L 251.223389 195.294495 \n",
       "L 251.731509 196.299646 \n",
       "L 252.239629 195.718487 \n",
       "L 252.747748 195.898986 \n",
       "L 253.255868 196.806876 \n",
       "L 253.763987 196.463669 \n",
       "L 254.272107 197.695899 \n",
       "L 254.780227 195.672323 \n",
       "L 255.288346 196.404414 \n",
       "L 255.796466 196.515752 \n",
       "L 256.304585 197.400091 \n",
       "L 256.812705 196.904036 \n",
       "L 257.320824 196.745463 \n",
       "L 257.828944 197.896953 \n",
       "L 258.337064 197.292393 \n",
       "L 258.845183 197.999491 \n",
       "L 259.353303 197.743464 \n",
       "L 259.861422 196.965111 \n",
       "L 260.369542 198.142501 \n",
       "L 260.877662 196.986215 \n",
       "L 261.385781 197.671636 \n",
       "L 261.893901 198.698218 \n",
       "L 262.91014 199.190921 \n",
       "L 263.926379 198.914492 \n",
       "L 264.942618 198.348019 \n",
       "L 265.450738 199.380704 \n",
       "L 265.958858 199.662585 \n",
       "L 266.466977 199.612905 \n",
       "L 267.483216 199.240482 \n",
       "L 267.991336 199.293908 \n",
       "L 268.499456 199.538445 \n",
       "L 269.007575 199.947041 \n",
       "L 269.515695 199.560642 \n",
       "L 270.023814 200.111515 \n",
       "L 270.531934 199.297761 \n",
       "L 271.040053 199.253347 \n",
       "L 271.548173 199.882151 \n",
       "L 272.056293 200.877242 \n",
       "L 273.072532 199.915031 \n",
       "L 273.580651 201.62917 \n",
       "L 274.088771 200.764199 \n",
       "L 274.596891 200.988193 \n",
       "L 275.10501 200.983861 \n",
       "L 275.61313 200.356676 \n",
       "L 276.121249 200.422818 \n",
       "L 276.629369 201.842592 \n",
       "L 277.137489 200.704594 \n",
       "L 277.645608 201.544291 \n",
       "L 278.153728 201.096001 \n",
       "L 278.661847 202.05271 \n",
       "L 279.678087 201.959868 \n",
       "L 280.186206 202.496258 \n",
       "L 280.694326 201.500255 \n",
       "L 281.202445 202.881162 \n",
       "L 281.710565 203.343591 \n",
       "L 282.218685 202.719744 \n",
       "L 282.726804 201.875271 \n",
       "L 283.234924 201.77671 \n",
       "L 283.743043 203.122712 \n",
       "L 284.251163 203.04565 \n",
       "L 284.759283 202.767993 \n",
       "L 285.267402 202.066018 \n",
       "L 285.775522 203.67307 \n",
       "L 286.283641 202.57508 \n",
       "L 286.791761 203.370296 \n",
       "L 287.29988 203.867777 \n",
       "L 287.808 203.3713 \n",
       "L 288.31612 204.153959 \n",
       "L 288.824239 204.157296 \n",
       "L 289.332359 202.881125 \n",
       "L 289.840478 204.906635 \n",
       "L 290.348598 203.648899 \n",
       "L 290.856718 203.842959 \n",
       "L 291.364837 205.026172 \n",
       "L 292.381076 205.226303 \n",
       "L 292.889196 204.828879 \n",
       "L 293.397316 204.280141 \n",
       "L 293.905435 205.414004 \n",
       "L 294.413555 205.344848 \n",
       "L 294.921674 204.698798 \n",
       "L 295.429794 204.790972 \n",
       "L 295.937914 205.828399 \n",
       "L 296.446033 205.122129 \n",
       "L 296.954153 205.094329 \n",
       "L 297.462272 205.656931 \n",
       "L 297.970392 205.424654 \n",
       "L 298.478512 205.941177 \n",
       "L 298.986631 206.723237 \n",
       "L 299.494751 206.176493 \n",
       "L 300.00287 206.147986 \n",
       "L 300.51099 206.25896 \n",
       "L 301.01911 206.551193 \n",
       "L 301.527229 206.679341 \n",
       "L 302.035349 206.12382 \n",
       "L 302.543468 206.797556 \n",
       "L 303.051588 206.315274 \n",
       "L 303.559707 206.661654 \n",
       "L 304.067827 206.461874 \n",
       "L 304.575947 206.449199 \n",
       "L 305.084066 206.949985 \n",
       "L 305.592186 206.767789 \n",
       "L 306.100305 207.371547 \n",
       "L 306.608425 208.373069 \n",
       "L 307.116545 208.072531 \n",
       "L 307.624664 208.391205 \n",
       "L 308.132784 208.156717 \n",
       "L 308.640903 207.416111 \n",
       "L 309.149023 207.703818 \n",
       "L 309.657143 207.77026 \n",
       "L 310.165262 207.005858 \n",
       "L 310.673382 209.021622 \n",
       "L 311.181501 208.559721 \n",
       "L 312.197741 208.3646 \n",
       "L 312.70586 209.155099 \n",
       "L 313.21398 208.676694 \n",
       "L 313.722099 208.828883 \n",
       "L 314.738339 208.797934 \n",
       "L 315.246458 209.421236 \n",
       "L 315.754578 207.971656 \n",
       "L 316.262697 208.995489 \n",
       "L 316.770817 209.714399 \n",
       "L 317.278936 210.051626 \n",
       "L 318.295176 209.913916 \n",
       "L 318.803295 210.142886 \n",
       "L 319.311415 209.432772 \n",
       "L 319.819534 210.693173 \n",
       "L 320.327654 209.724373 \n",
       "L 320.835774 211.886389 \n",
       "L 321.343893 210.332129 \n",
       "L 321.852013 210.796889 \n",
       "L 322.360132 209.659212 \n",
       "L 323.376372 211.2361 \n",
       "L 323.884491 211.143398 \n",
       "L 324.392611 210.074133 \n",
       "L 324.90073 210.927755 \n",
       "L 325.40885 210.703177 \n",
       "L 325.91697 211.86408 \n",
       "L 326.425089 211.218893 \n",
       "L 327.441328 211.559515 \n",
       "L 327.949448 212.334794 \n",
       "L 328.457568 211.712883 \n",
       "L 328.965687 211.433793 \n",
       "L 329.473807 212.652519 \n",
       "L 329.981926 212.23943 \n",
       "L 330.490046 212.301069 \n",
       "L 330.998166 212.529511 \n",
       "L 331.506285 212.547906 \n",
       "L 332.014405 212.255014 \n",
       "L 332.522524 212.702483 \n",
       "L 333.030644 212.355221 \n",
       "L 333.538763 213.53683 \n",
       "L 334.046883 213.473469 \n",
       "L 334.555003 212.381531 \n",
       "L 335.063122 213.478524 \n",
       "L 335.571242 212.500541 \n",
       "L 336.079361 213.18614 \n",
       "L 336.587481 213.490306 \n",
       "L 337.095601 213.353697 \n",
       "L 337.60372 212.88661 \n",
       "L 338.11184 214.228599 \n",
       "L 338.619959 214.756364 \n",
       "L 339.128079 214.380985 \n",
       "L 339.636199 214.391008 \n",
       "L 340.144318 214.154056 \n",
       "L 340.144318 214.154056 \n",
       "\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 20.5625 224.64 \n",
       "L 20.5625 7.2 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 355.3625 224.64 \n",
       "L 355.3625 7.2 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 20.5625 224.64 \n",
       "L 355.3625 224.64 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 20.5625 7.2 \n",
       "L 355.3625 7.2 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p500af5bb16\">\n",
       "   <rect height=\"217.44\" width=\"334.8\" x=\"20.5625\" y=\"7.2\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(lossvalues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Bleu score Calculation** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.14285714285714285 0.048795003647426664\n",
      "1000 0.14979451906883545 0.060252677782607476\n",
      "2000 0.14273325562059064 0.06094017436746255\n",
      "3000 0.1444428779713085 0.06211674657110902\n",
      "4000 0.14634442648336757 0.06354047545891911\n",
      "5000 0.14458258293883028 0.0627104346052938\n",
      "6000 0.14311352700981422 0.06200749686284637\n",
      "7000 0.1499265379577537 0.06814706564857818\n",
      "8000 0.14945890222419272 0.06756435794669739\n",
      "Total Bleu Score for 1 grams on testing pairs:  0.14905095899531023\n",
      "Total Bleu Score for 2 grams on testing pairs:  0.06728188595393607\n"
     ]
    }
   ],
   "source": [
    "# Set dropout layers to eval mode\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "from nltk.translate.bleu_score import sentence_bleu,corpus_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "############################################################################\n",
    "# Difference between greedy search and beam search is here\n",
    "\n",
    "# greedy search\n",
    "# searcher = GreedySearchDecoder(encoder, decoder)\n",
    "\n",
    "# beam search\n",
    "searcher = BeamSearchDecoder(encoder, decoder, voc, 10)\n",
    "############################################################################\n",
    "gram1_bleu_score = []\n",
    "gram2_bleu_score = []\n",
    "for i in range(0,len(testpairs),1):\n",
    "  \n",
    "  input_sentence = testpairs[i][0]\n",
    "  \n",
    "  reference = testpairs[i][1:]\n",
    "  templist = []\n",
    "  for k in range(len(reference)):\n",
    "    if(reference[k]!=''):\n",
    "      temp = reference[k].split(' ')\n",
    "      templist.append(temp)\n",
    "  \n",
    "  \n",
    "  input_sentence = normalizeString(input_sentence)\n",
    "  output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "  output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "  chencherry = SmoothingFunction()\n",
    "  score1 = sentence_bleu(templist,output_words,weights=(1, 0, 0, 0) ,smoothing_function=chencherry.method1)\n",
    "  score2 = sentence_bleu(templist,output_words,weights=(0.5, 0.5, 0, 0),smoothing_function=chencherry.method1) \n",
    "  gram1_bleu_score.append(score1)\n",
    "  gram2_bleu_score.append(score2)\n",
    "  if i%1000 == 0:\n",
    "    print(i,sum(gram1_bleu_score)/len(gram1_bleu_score),sum(gram2_bleu_score)/len(gram2_bleu_score))\n",
    "print(\"Total Bleu Score for 1 grams on testing pairs: \", sum(gram1_bleu_score)/len(gram1_bleu_score))  \n",
    "print(\"Total Bleu Score for 2 grams on testing pairs: \", sum(gram2_bleu_score)/len(gram2_bleu_score))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Chatting with BOT / Evaluation**\n",
    "Lets do chat with our newly trained chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: hi jabez . . .\n",
      "Bot: it s me .\n",
      "Bot: it s me .\n",
      "Bot: jim .\n",
      "Bot: no . . . i m sorry .\n",
      "Bot: mrs . robinson . . .\n",
      "Bot: i m sorry . i m sorry .\n",
      "Bot: because i don t want to\n",
      "Bot: no . . . no .\n",
      "Bot: i ll see you later .\n",
      "Bot: i ll see you later .\n"
     ]
    }
   ],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "searcher = BeamSearchDecoder(encoder, decoder, voc, 10)\n",
    "evaluateInput(encoder, decoder, searcher, voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent work\n",
    "\n",
    "To be done on your own: find another dataset of sentence pairs in a different domain and see\n",
    "if you can preprocess the data and train a chatbot model on it using the same code we developed today. Report your results.\n",
    "\n",
    "Convert LSTM into transformer :-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
