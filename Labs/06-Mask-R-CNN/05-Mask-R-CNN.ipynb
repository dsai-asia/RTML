{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Mask R-CNN\n",
    "\n",
    "Mask R-CNN was originally released under the Caffe2 framework by the FAIR team as part of a project called \"Detectron.\"\n",
    "\n",
    "After Caffe2 and PyTorch merged, eventually, FAIR released a ground-up reimplementation of Detectron with new models and\n",
    "features. This framework is [Detectron2](https://github.com/facebookresearch/detectron2). If you're looking for a complete\n",
    "approach to bounding box regression, mask regression, and skeleton point (pose) estimation, take a look.\n",
    "\n",
    "In the meantime, however, other groups quickly implemented Mask R-CNN directly in TensorFlow and Keras.\n",
    "Today we'll work with a [PyTorch implementation of Mask R-CNN](https://github.com/multimodallearning/pytorch-mask-rcnn) derived\n",
    "from those efforts.\n",
    "\n",
    "The full of Mask R-CNN structure is below:\n",
    "\n",
    "<img src=\"img/MaskRCNN.png\" title=\"FullMaskR-CNN\" style=\"width: 800px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each intance of an object in an image, Mask R-CNN attempts to generate\n",
    " - A bounding box\n",
    " - A segmentation mask\n",
    "\n",
    "The backbone and neck of Mask R-CNN are based on\n",
    " - A feature pyramid networks (FPN)\n",
    " - ResNet 101\n",
    "\n",
    "## Feature Pyramid Networks\n",
    "\n",
    "We've seen the feature pyramid network (FPN) in YOLOv3 and YOLOv4. It is a feature extractor using a pyramid concept.\n",
    "We begin with ordinary progressive downsampling of the input to get a multiscale representation of the input, but rather\n",
    "than using that \"low-level\" multiscale representation directly, we progressively upsample the coarse representation of\n",
    "the input using input from the low-level feature maps. The idea is shown in the left-hand panel of the diagram above.\n",
    "By incorporating information from both the low-level \"bottom-up\" fine grained feature map and the upsampled coarser grained feature\n",
    "map in the pyramid, the fine grained representation at the bottom of the pyramid contain much more useful or more \"semantic\"\n",
    "information about the input.\n",
    "\n",
    "Feature pyramids can, in principle, come in many different forms. Refer to the figure below, which is\n",
    "from [Towards Data Science](https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610):\n",
    "\n",
    "<img src=\"img/Pyramid.png\" title=\"PyramidNets\" style=\"width: 800px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classical image pyramid, used by many techniques aiming at multiscale detection, looks like (a).\n",
    "Classifiers we built early on in the class, such as AlexNet, look like (b). (c) and (d) utilize multiple\n",
    "feature maps derived from the input through progressive downscaling. The difference in the FPN (d) is\n",
    "the inclusion of both bottom up and top down pathways:\n",
    "\n",
    "<img src=\"img/bottomup.jpeg\" title=\"Bottom-up\" style=\"width: 300px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that as we process the input in multiple progressively downsampled layers, we are increasingly analyzing higher-level features\n",
    "with larger receptive fields with some invariance to imaging conditions (translation, scale, lighting, etc.).\n",
    "\n",
    "The top-down representations use progressively *upsampled* layers in which we are increasingly analyzing the input at high resolution but\n",
    "with all the benefits of the downsampled representation.\n",
    "\n",
    "The main risk of the top-down upsampling is that we would lose information about the details of the original input in constructing the\n",
    "fine-grained feature maps. For that reason, we add lateral connections to the bottom-up feature maps of the same size:\n",
    "\n",
    "<img src=\"img/lateralconnection.png\" title=\"lateralconnection\" style=\"width: 300px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet backbone\n",
    "\n",
    "The bottom-up part of the FPN used in Mask-RCNN is ResNet. It is used similar to how\n",
    "Darknet-53 is used in YOLO. We take the classifier structure as the bottom-up half of\n",
    "the pyramid, then we add the top down part to obtain the FPN.\n",
    "\n",
    "Mask R-CNN taps into ResNet in 4 or 5 places according to the implementation, at the ouptut of various residual blocks.\n",
    "\n",
    "Here's a figure to explain this, this time from [Jonathan Hui's Medium site](https://jonathan-hui.medium.com/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c):\n",
    "\n",
    "<img src=\"img/upanddown.png\" title=\"upanddown\" style=\"width: 400px;\" />\n",
    "\n",
    "(There would be a P6 there as well if we're extracting a 5-scale pyramid)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Region Proposal Network (RPN)\n",
    "\n",
    "The Faster R-CNN RPN connects to the top of the pyramid. It performs classification and bounding box regression for each possible proposal.\n",
    "\n",
    "<img src=\"img/RPN.png\" title=\"RPN\" style=\"width: 600px;\" />\n",
    "\n",
    "## Detection network\n",
    "\n",
    "The detection network uses the results of the RPN as well as the output of the FPN. With the RPN bounding box as input, we assign the box to one of the levels of the pyramid.\n",
    "Specifically, we use\n",
    "\n",
    "$$ k = \\left\\lfloor k_0 +\\log_2\\left( \\frac{\\sqrt{wh}}{224} \\right) \\right\\rfloor $$\n",
    "\n",
    "Then the ROIAlign block interpolates the appropriate set of features from the best level (level $k$) of the pyramid. The region is aligned and scaled to a size of\n",
    "56$\\times$56, and the resulting representation is forwarded to the mask prediction head.\n",
    "\n",
    "## Mask Head\n",
    "\n",
    "The mask head is a FCN that up-samples from the detection result, and the patch size is finally re-scaled back to the input size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding\n",
    "\n",
    "So let's start investigating how Mask R-CNN works in detail.\n",
    "First, clone the Github respository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'pytorch-mask-rcnn'...\n",
      "remote: Enumerating objects: 91, done.\u001b[K\n",
      "remote: Total 91 (delta 0), reused 0 (delta 0), pack-reused 91\u001b[K\n",
      "Unpacking objects: 100% (91/91), 8.75 MiB | 1.03 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/multimodallearning/pytorch-mask-rcnn.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MaskRCNN class (model.py)\n",
    "\n",
    "First, let's cisit the MaskRCNN class and its init function:\n",
    "\n",
    "<img src=\"img/MaskRCNNc1.JPG\" title=\"MaskRCNNc2\" style=\"width: 400px;\" />\n",
    "\n",
    "The most important action here is calling `build()`, which creates the network based on a\n",
    "configuration object. `set_log_dir()` just sets up saving to a log file, and `initialize_weights()` loads\n",
    "weights. Let's take a look at the `build()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build(self, config):\n",
    "    \"\"\"Build Mask R-CNN architecture.\n",
    "    \"\"\"\n",
    "\n",
    "    # Image size must be dividable by 2 multiple times\n",
    "    h, w = config.IMAGE_SHAPE[:2]\n",
    "    if h / 2**6 != int(h / 2**6) or w / 2**6 != int(w / 2**6):\n",
    "        raise Exception(\"Image size must be dividable by 2 at least 6 times \"\n",
    "                        \"to avoid fractions when downscaling and upscaling.\"\n",
    "                        \"For example, use 256, 320, 384, 448, 512, ... etc. \")\n",
    "\n",
    "    # Build the shared convolutional layers.\n",
    "    # Bottom-up Layers\n",
    "    # Returns a list of the last layers of each stage, 5 in total.\n",
    "    # Don't create the thead (stage 5), so we pick the 4th item in the list.\n",
    "    resnet = ResNet(\"resnet101\", stage5=True)\n",
    "    C1, C2, C3, C4, C5 = resnet.stages()\n",
    "\n",
    "    # Top-down Layers\n",
    "    # TODO: add assert to varify feature map sizes match what's in config\n",
    "    self.fpn = FPN(C1, C2, C3, C4, C5, out_channels=256)\n",
    "\n",
    "    # Generate Anchors\n",
    "    self.anchors = Variable(torch.from_numpy(utils.generate_pyramid_anchors(config.RPN_ANCHOR_SCALES,\n",
    "                                                                            config.RPN_ANCHOR_RATIOS,\n",
    "                                                                            config.BACKBONE_SHAPES,\n",
    "                                                                            config.BACKBONE_STRIDES,\n",
    "                                                                            config.RPN_ANCHOR_STRIDE)).float(), requires_grad=False)\n",
    "    if self.config.GPU_COUNT:\n",
    "        self.anchors = self.anchors.cuda()\n",
    "\n",
    "    # RPN\n",
    "    self.rpn = RPN(len(config.RPN_ANCHOR_RATIOS), config.RPN_ANCHOR_STRIDE, 256)\n",
    "\n",
    "    # FPN Classifier\n",
    "    self.classifier = Classifier(256, config.POOL_SIZE, config.IMAGE_SHAPE, config.NUM_CLASSES)\n",
    "\n",
    "    # FPN Mask\n",
    "    self.mask = Mask(256, config.MASK_POOL_SIZE, config.IMAGE_SHAPE, config.NUM_CLASSES)\n",
    "\n",
    "    # Fix batch norm layers\n",
    "    def set_bn_fix(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if classname.find('BatchNorm') != -1:\n",
    "            for p in m.parameters(): p.requires_grad = False\n",
    "\n",
    "    self.apply(set_bn_fix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backbone\n",
    "\n",
    "The backbone is initialized then tapped into in the lines\n",
    "\n",
    "    resnet = ResNet(\"resnet101\", stage5=True)\n",
    "    C1, C2, C3, C4, C5 = resnet.stages()\n",
    "\n",
    "We see that this version is using ResNet101 and extracting a 5-stages pyramid. How to find out what stages are being used? Take\n",
    "a look at the ResNet class itself and take a look at its `__init__` and forward methods. We see that the C1-C5 are the main\n",
    "blocks of the network. The first (C1) is a single 7$\\times$7 convolution with batch norm, ReLU, and a MaxPool operation. The others\n",
    "(C2-C5) are ResNet residual blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, architecture, stage5=False):\n",
    "        super(ResNet, self).__init__()\n",
    "        assert architecture in [\"resnet50\", \"resnet101\"]\n",
    "        self.inplanes = 64\n",
    "        self.layers = [3, 4, {\"resnet50\": 6, \"resnet101\": 23}[architecture], 3]\n",
    "        self.block = Bottleneck\n",
    "        self.stage5 = stage5\n",
    "\n",
    "        self.C1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.BatchNorm2d(64, eps=0.001, momentum=0.01),\n",
    "            nn.ReLU(inplace=True),\n",
    "            SamePad2d(kernel_size=3, stride=2),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.C2 = self.make_layer(self.block, 64, self.layers[0])\n",
    "        self.C3 = self.make_layer(self.block, 128, self.layers[1], stride=2)\n",
    "        self.C4 = self.make_layer(self.block, 256, self.layers[2], stride=2)\n",
    "        if self.stage5:\n",
    "            self.C5 = self.make_layer(self.block, 512, self.layers[3], stride=2)\n",
    "        else:\n",
    "            self.C5 = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.C1(x)\n",
    "        x = self.C2(x)\n",
    "        x = self.C3(x)\n",
    "        x = self.C4(x)\n",
    "        x = self.C5(x)\n",
    "        return x\n",
    "\n",
    "    def stages(self):\n",
    "        return [self.C1, self.C2, self.C3, self.C4, self.C5]\n",
    "\n",
    "    def make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(planes * block.expansion, eps=0.001, momentum=0.01),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FPN\n",
    "\n",
    "Let's go back to the MaskRCNN class. The next stage is FPN (Top-down layers)\n",
    "\n",
    "    self.fpn = FPN(C1, C2, C3, C4, C5, out_channels=256)\n",
    "\n",
    "The main idea of the FPN is in its `forward()` method. Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FPN(nn.Module):\n",
    "    def __init__(self, C1, C2, C3, C4, C5, out_channels):\n",
    "        super(FPN, self).__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.C1 = C1\n",
    "        self.C2 = C2\n",
    "        self.C3 = C3\n",
    "        self.C4 = C4\n",
    "        self.C5 = C5\n",
    "        self.P6 = nn.MaxPool2d(kernel_size=1, stride=2)\n",
    "        self.P5_conv1 = nn.Conv2d(2048, self.out_channels, kernel_size=1, stride=1)\n",
    "        self.P5_conv2 = nn.Sequential(\n",
    "            SamePad2d(kernel_size=3, stride=1),\n",
    "            nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, stride=1),\n",
    "        )\n",
    "        self.P4_conv1 =  nn.Conv2d(1024, self.out_channels, kernel_size=1, stride=1)\n",
    "        self.P4_conv2 = nn.Sequential(\n",
    "            SamePad2d(kernel_size=3, stride=1),\n",
    "            nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, stride=1),\n",
    "        )\n",
    "        self.P3_conv1 = nn.Conv2d(512, self.out_channels, kernel_size=1, stride=1)\n",
    "        self.P3_conv2 = nn.Sequential(\n",
    "            SamePad2d(kernel_size=3, stride=1),\n",
    "            nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, stride=1),\n",
    "        )\n",
    "        self.P2_conv1 = nn.Conv2d(256, self.out_channels, kernel_size=1, stride=1)\n",
    "        self.P2_conv2 = nn.Sequential(\n",
    "            SamePad2d(kernel_size=3, stride=1),\n",
    "            nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, stride=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.C1(x)\n",
    "        x = self.C2(x)\n",
    "        c2_out = x          # keep C2 output\n",
    "        x = self.C3(x)\n",
    "        c3_out = x          # keep C3 output\n",
    "        x = self.C4(x)\n",
    "        c4_out = x          # keep C4 output\n",
    "        x = self.C5(x)\n",
    "        p5_out = self.P5_conv1(x)       # top-most of pyramid\n",
    "        p4_out = self.P4_conv1(c4_out) + F.upsample(p5_out, scale_factor=2)         # lateral connections, 2nd top output\n",
    "        p3_out = self.P3_conv1(c3_out) + F.upsample(p4_out, scale_factor=2)         # lateral connections, 3rd top output\n",
    "        p2_out = self.P2_conv1(c2_out) + F.upsample(p3_out, scale_factor=2)         # lateral connections, 4th top output\n",
    "\n",
    "        p5_out = self.P5_conv2(p5_out)\n",
    "        p4_out = self.P4_conv2(p4_out)\n",
    "        p3_out = self.P3_conv2(p3_out)\n",
    "        p2_out = self.P2_conv2(p2_out)\n",
    "\n",
    "        # P6 is used for the 5th anchor scale in RPN. Generated by\n",
    "        # subsampling from P5 with stride of 2.\n",
    "        p6_out = self.P6(p5_out)        # max pooling for RPN\n",
    "\n",
    "        return [p2_out, p3_out, p4_out, p5_out, p6_out]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anchors\n",
    "\n",
    "Anchor box sizes are set in the configuration file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RPN\n",
    "\n",
    "Next, let's see how the RPN is produced.\n",
    "\n",
    "    self.rpn = RPN(len(config.RPN_ANCHOR_RATIOS), config.RPN_ANCHOR_STRIDE, 256)\n",
    "\n",
    "The RPN class is reproduced below. But also take a look at its output. It releases classes (score, and softmax), and bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RPN(nn.Module):\n",
    "    \"\"\"Builds the model of Region Proposal Network.\n",
    "\n",
    "    anchors_per_location: number of anchors per pixel in the feature map\n",
    "    anchor_stride: Controls the density of anchors. Typically 1 (anchors for\n",
    "                   every pixel in the feature map), or 2 (every other pixel).\n",
    "\n",
    "    Returns:\n",
    "        rpn_logits: [batch, H, W, 2] Anchor classifier logits (before softmax)\n",
    "        rpn_probs: [batch, W, W, 2] Anchor classifier probabilities.\n",
    "        rpn_bbox: [batch, H, W, (dy, dx, log(dh), log(dw))] Deltas to be\n",
    "                  applied to anchors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, anchors_per_location, anchor_stride, depth):\n",
    "        super(RPN, self).__init__()\n",
    "        self.anchors_per_location = anchors_per_location\n",
    "        self.anchor_stride = anchor_stride\n",
    "        self.depth = depth\n",
    "\n",
    "        self.padding = SamePad2d(kernel_size=3, stride=self.anchor_stride)\n",
    "        self.conv_shared = nn.Conv2d(self.depth, 512, kernel_size=3, stride=self.anchor_stride)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv_class = nn.Conv2d(512, 2 * anchors_per_location, kernel_size=1, stride=1)     # class, score\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        self.conv_bbox = nn.Conv2d(512, 4 * anchors_per_location, kernel_size=1, stride=1)      # x,y,w,h\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shared convolutional base of the RPN\n",
    "        x = self.relu(self.conv_shared(self.padding(x)))\n",
    "\n",
    "        # Anchor Score. [batch, anchors per location * 2, height, width].\n",
    "        rpn_class_logits = self.conv_class(x)\n",
    "\n",
    "        # Reshape to [batch, 2, anchors]\n",
    "        rpn_class_logits = rpn_class_logits.permute(0,2,3,1)\n",
    "        rpn_class_logits = rpn_class_logits.contiguous()\n",
    "        rpn_class_logits = rpn_class_logits.view(x.size()[0], -1, 2)\n",
    "\n",
    "        # Softmax on last dimension of BG/FG.\n",
    "        rpn_probs = self.softmax(rpn_class_logits)              # output class\n",
    "\n",
    "        # Bounding box refinement. [batch, H, W, anchors per location, depth]\n",
    "        # where depth is [x, y, log(w), log(h)]\n",
    "        rpn_bbox = self.conv_bbox(x)\n",
    "\n",
    "        # Reshape to [batch, 4, anchors]\n",
    "        rpn_bbox = rpn_bbox.permute(0,2,3,1)\n",
    "        rpn_bbox = rpn_bbox.contiguous()\n",
    "        rpn_bbox = rpn_bbox.view(x.size()[0], -1, 4)\n",
    "\n",
    "        return [rpn_class_logits, rpn_probs, rpn_bbox]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposal classifier\n",
    "\n",
    "The Faster R-CNN head contains the region proposal classifier.\n",
    "\n",
    "    self.classifier = Classifier(256, config.POOL_SIZE, config.IMAGE_SHAPE, config.NUM_CLASSES)\n",
    "\n",
    "The classifier is mainly composed of convolutional layers. The most\n",
    "interesting process is the pyramidal ROI alignment at multiple scales.\n",
    "At the Classifier class, the combine ROIs aligned is in function pyramid_roi_align.\n",
    "\n",
    "Let's take a look the `pyramid_roi_align()` method, which contains the cropping and resizing of incoming feature maps according to region proposals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyramid_roi_align(inputs, pool_size, image_shape):\n",
    "    \"\"\"Implements ROI Pooling on multiple levels of the feature pyramid.\n",
    "\n",
    "    Params:\n",
    "    - pool_size: [height, width] of the output pooled regions. Usually [7, 7]\n",
    "    - image_shape: [height, width, channels]. Shape of input image in pixels\n",
    "\n",
    "    Inputs:\n",
    "    - boxes: [batch, num_boxes, (y1, x1, y2, x2)] in normalized\n",
    "             coordinates.\n",
    "    - Feature maps: List of feature maps from different levels of the pyramid.\n",
    "                    Each is [batch, channels, height, width]\n",
    "\n",
    "    Output:\n",
    "    Pooled regions in the shape: [num_boxes, height, width, channels].\n",
    "    The width and height are those specific in the pool_shape in the layer\n",
    "    constructor.\n",
    "    \"\"\"\n",
    "\n",
    "    # Currently only supports batchsize 1\n",
    "    for i in range(len(inputs)):\n",
    "        inputs[i] = inputs[i].squeeze(0)\n",
    "\n",
    "    # Crop boxes [batch, num_boxes, (y1, x1, y2, x2)] in normalized coords\n",
    "    boxes = inputs[0]\n",
    "\n",
    "    # Feature Maps. List of feature maps from different level of the\n",
    "    # feature pyramid. Each is [batch, height, width, channels]\n",
    "    feature_maps = inputs[1:]\n",
    "\n",
    "    # Assign each ROI to a level in the pyramid based on the ROI area.\n",
    "    y1, x1, y2, x2 = boxes.chunk(4, dim=1)\n",
    "    h = y2 - y1\n",
    "    w = x2 - x1\n",
    "\n",
    "    # Equation 1 in the Feature Pyramid Networks paper. Account for\n",
    "    # the fact that our coordinates are normalized here.\n",
    "    # e.g. a 224x224 ROI (in pixels) maps to P4\n",
    "    image_area = Variable(torch.FloatTensor([float(image_shape[0]*image_shape[1])]), requires_grad=False)\n",
    "    if boxes.is_cuda:\n",
    "        image_area = image_area.cuda()\n",
    "    roi_level = 4 + log2(torch.sqrt(h*w)/(224.0/torch.sqrt(image_area)))\n",
    "    roi_level = roi_level.round().int()\n",
    "    roi_level = roi_level.clamp(2,5)\n",
    "\n",
    "\n",
    "    # Loop through levels and apply ROI pooling to each. P2 to P5.\n",
    "    pooled = []\n",
    "    box_to_level = []\n",
    "    for i, level in enumerate(range(2, 6)):\n",
    "        ix  = roi_level==level\n",
    "        if not ix.any():\n",
    "            continue\n",
    "        ix = torch.nonzero(ix)[:,0]\n",
    "        level_boxes = boxes[ix.data, :]\n",
    "\n",
    "        # Keep track of which box is mapped to which level\n",
    "        box_to_level.append(ix.data)\n",
    "\n",
    "        # Stop gradient propogation to ROI proposals\n",
    "        level_boxes = level_boxes.detach()\n",
    "\n",
    "        # Crop and Resize\n",
    "        # From Mask R-CNN paper: \"We sample four regular locations, so\n",
    "        # that we can evaluate either max or average pooling. In fact,\n",
    "        # interpolating only a single value at each bin center (without\n",
    "        # pooling) is nearly as effective.\"\n",
    "        #\n",
    "        # Here we use the simplified approach of a single value per bin,\n",
    "        # which is how it's done in tf.crop_and_resize()\n",
    "        # Result: [batch * num_boxes, pool_height, pool_width, channels]\n",
    "        ind = Variable(torch.zeros(level_boxes.size()[0]),requires_grad=False).int()\n",
    "        if level_boxes.is_cuda:\n",
    "            ind = ind.cuda()\n",
    "        feature_maps[i] = feature_maps[i].unsqueeze(0)  #CropAndResizeFunction needs batch dimension\n",
    "        pooled_features = CropAndResizeFunction(pool_size, pool_size, 0)(feature_maps[i], level_boxes, ind)\n",
    "        pooled.append(pooled_features)\n",
    "\n",
    "    # Pack pooled features into one tensor\n",
    "    pooled = torch.cat(pooled, dim=0)\n",
    "\n",
    "    # Pack box_to_level mapping into one array and add another\n",
    "    # column representing the order of pooled boxes\n",
    "    box_to_level = torch.cat(box_to_level, dim=0)\n",
    "\n",
    "    # Rearrange pooled features to match the order of the original boxes\n",
    "    _, box_to_level = torch.sort(box_to_level)\n",
    "    pooled = pooled[box_to_level, :, :]\n",
    "\n",
    "    return pooled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask Head\n",
    "\n",
    "The last step creates the mask head. We have convolutions and upsampling to the original image size.\n",
    "The difference between the classifier head and the mask head is that the classifier head takes the proposal bounding boxes\n",
    "and outputs final bounding boxes whereas the mask head uses the final bounding boxes to create masks.\n",
    "\n",
    "The mask creation in the MaskRCNN class is here:\n",
    "\n",
    "    self.mask = Mask(256, config.MASK_POOL_SIZE, config.IMAGE_SHAPE, config.NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction function\n",
    "\n",
    "Now let's look at the overall\n",
    "`predict()` method. There are two modes there: inference (evaluate) mode and training mode.\n",
    "Both modes have the same steps, but during training, we have to calculate ROI sizes for comparisons between the predicted output and the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def predict(self, input, mode):\n",
    "        molded_images = input[0]\n",
    "        image_metas = input[1]\n",
    "\n",
    "        if mode == 'inference':\n",
    "            self.eval()\n",
    "        elif mode == 'training':\n",
    "            self.train()\n",
    "\n",
    "            # Set batchnorm always in eval mode during training\n",
    "            def set_bn_eval(m):\n",
    "                classname = m.__class__.__name__\n",
    "                if classname.find('BatchNorm') != -1:\n",
    "                    m.eval()\n",
    "\n",
    "            self.apply(set_bn_eval)\n",
    "\n",
    "        # Feature extraction\n",
    "        [p2_out, p3_out, p4_out, p5_out, p6_out] = self.fpn(molded_images)\n",
    "\n",
    "        # Note that P6 is used in RPN, but not in the classifier heads.\n",
    "        rpn_feature_maps = [p2_out, p3_out, p4_out, p5_out, p6_out]\n",
    "        mrcnn_feature_maps = [p2_out, p3_out, p4_out, p5_out]\n",
    "\n",
    "        # Loop through pyramid layers\n",
    "        layer_outputs = []  # list of lists\n",
    "        for p in rpn_feature_maps:\n",
    "            layer_outputs.append(self.rpn(p))\n",
    "\n",
    "        # Concatenate layer outputs\n",
    "        # Convert from list of lists of level outputs to list of lists\n",
    "        # of outputs across levels.\n",
    "        # e.g. [[a1, b1, c1], [a2, b2, c2]] => [[a1, a2], [b1, b2], [c1, c2]]\n",
    "        outputs = list(zip(*layer_outputs))\n",
    "        outputs = [torch.cat(list(o), dim=1) for o in outputs]\n",
    "        rpn_class_logits, rpn_class, rpn_bbox = outputs\n",
    "\n",
    "        # Generate proposals\n",
    "        # Proposals are [batch, N, (y1, x1, y2, x2)] in normalized coordinates\n",
    "        # and zero padded.\n",
    "        proposal_count = self.config.POST_NMS_ROIS_TRAINING if mode == \"training\" \\\n",
    "            else self.config.POST_NMS_ROIS_INFERENCE\n",
    "        rpn_rois = proposal_layer([rpn_class, rpn_bbox],\n",
    "                                 proposal_count=proposal_count,\n",
    "                                 nms_threshold=self.config.RPN_NMS_THRESHOLD,\n",
    "                                 anchors=self.anchors,\n",
    "                                 config=self.config)\n",
    "\n",
    "        if mode == 'inference':\n",
    "            # Network Heads\n",
    "            # Proposal classifier and BBox regressor heads\n",
    "            mrcnn_class_logits, mrcnn_class, mrcnn_bbox = self.classifier(mrcnn_feature_maps, rpn_rois)\n",
    "\n",
    "            # Detections\n",
    "            # output is [batch, num_detections, (y1, x1, y2, x2, class_id, score)] in image coordinates\n",
    "            detections = detection_layer(self.config, rpn_rois, mrcnn_class, mrcnn_bbox, image_metas)\n",
    "\n",
    "            # Convert boxes to normalized coordinates\n",
    "            # TODO: let DetectionLayer return normalized coordinates to avoid\n",
    "            #       unnecessary conversions\n",
    "            h, w = self.config.IMAGE_SHAPE[:2]\n",
    "            scale = Variable(torch.from_numpy(np.array([h, w, h, w])).float(), requires_grad=False)\n",
    "            if self.config.GPU_COUNT:\n",
    "                scale = scale.cuda()\n",
    "            detection_boxes = detections[:, :4] / scale\n",
    "\n",
    "            # Add back batch dimension\n",
    "            detection_boxes = detection_boxes.unsqueeze(0)\n",
    "\n",
    "            # Create masks for detections\n",
    "            mrcnn_mask = self.mask(mrcnn_feature_maps, detection_boxes)\n",
    "\n",
    "            # Add back batch dimension\n",
    "            detections = detections.unsqueeze(0)\n",
    "            mrcnn_mask = mrcnn_mask.unsqueeze(0)\n",
    "\n",
    "            return [detections, mrcnn_mask]\n",
    "\n",
    "        elif mode == 'training':\n",
    "\n",
    "            gt_class_ids = input[2]\n",
    "            gt_boxes = input[3]\n",
    "            gt_masks = input[4]\n",
    "\n",
    "            # Normalize coordinates\n",
    "            h, w = self.config.IMAGE_SHAPE[:2]\n",
    "            scale = Variable(torch.from_numpy(np.array([h, w, h, w])).float(), requires_grad=False)\n",
    "            if self.config.GPU_COUNT:\n",
    "                scale = scale.cuda()\n",
    "            gt_boxes = gt_boxes / scale\n",
    "\n",
    "            # Generate detection targets\n",
    "            # Subsamples proposals and generates target outputs for training\n",
    "            # Note that proposal class IDs, gt_boxes, and gt_masks are zero\n",
    "            # padded. Equally, returned rois and targets are zero padded.\n",
    "            rois, target_class_ids, target_deltas, target_mask = \\\n",
    "                detection_target_layer(rpn_rois, gt_class_ids, gt_boxes, gt_masks, self.config)\n",
    "\n",
    "            if not rois.size():\n",
    "                mrcnn_class_logits = Variable(torch.FloatTensor())\n",
    "                mrcnn_class = Variable(torch.IntTensor())\n",
    "                mrcnn_bbox = Variable(torch.FloatTensor())\n",
    "                mrcnn_mask = Variable(torch.FloatTensor())\n",
    "                if self.config.GPU_COUNT:\n",
    "                    mrcnn_class_logits = mrcnn_class_logits.cuda()\n",
    "                    mrcnn_class = mrcnn_class.cuda()\n",
    "                    mrcnn_bbox = mrcnn_bbox.cuda()\n",
    "                    mrcnn_mask = mrcnn_mask.cuda()\n",
    "            else:\n",
    "                # Network Heads\n",
    "                # Proposal classifier and BBox regressor heads\n",
    "                mrcnn_class_logits, mrcnn_class, mrcnn_bbox = self.classifier(mrcnn_feature_maps, rois)\n",
    "\n",
    "                # Create masks for detections\n",
    "                mrcnn_mask = self.mask(mrcnn_feature_maps, rois)\n",
    "\n",
    "            return [rpn_class_logits, rpn_bbox, target_class_ids, mrcnn_class_logits, target_deltas, mrcnn_bbox, target_mask, mrcnn_mask]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "Check `config.py` to see the possible\n",
    "configuration templates to change configuration information such as image size and number of classes.\n",
    "Sample configuration and dataset setup can be seen in coco.py.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "OK, let's get it all working.\n",
    "\n",
    "1. Clone this repository.\n",
    "\n",
    "    git clone https://github.com/multimodallearning/pytorch-mask-rcnn.git\n",
    "    \n",
    "2. We use functions from two more repositories that need to be build with the right --arch option for cuda support. The two functions are Non-Maximum Suppression from ruotianluo's pytorch-faster-rcnn repository and longcw's RoiAlign.\n",
    "\n",
    "|GPU\t|arch|\n",
    "|-----|-----|\n",
    "|TitanX|\tsm_52|\n",
    "|GTX 960M|\tsm_50|\n",
    "|GTX 1070|\tsm_61|\n",
    "|GTX 1080 (Ti)|\tsm_61|\n",
    "    cd nms/src/cuda/\n",
    "    nvcc -c -o nms_kernel.cu.o nms_kernel.cu -x cu -Xcompiler -fPIC -arch=[arch]\n",
    "    cd ../../\n",
    "    python build.py\n",
    "    cd ../\n",
    "\n",
    "    cd roialign/roi_align/src/cuda/\n",
    "    nvcc -c -o crop_and_resize_kernel.cu.o crop_and_resize_kernel.cu -x cu -Xcompiler -fPIC -arch=[arch]\n",
    "    cd ../../\n",
    "    python build.py\n",
    "    cd ../../\n",
    "\n",
    "3. As we use the COCO dataset (https://cocodataset.org/#home) install the Python COCO API(https://github.com/cocodataset/cocoapi) and create a symlink.\n",
    "\n",
    "    ln -s /path/to/coco/cocoapi/PythonAPI/pycocotools/ pycocotools\n",
    "    \n",
    "Download the pretrained models on COCO and ImageNet from https://drive.google.com/drive/folders/1LXUgC2IZUYNEoXr05tdqyKFZY0pZyPDc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo\n",
    "\n",
    "To test your installation simply run the demo with\n",
    "\n",
    "    python demo.py\n",
    "\n",
    "## Training on COCO\n",
    "\n",
    "Training and evaluation code is in coco.py. You can run it from the command line as such:\n",
    "\n",
    "    # Train a new model starting from pre-trained COCO weights\n",
    "    python coco.py train --dataset=/path/to/coco/ --model=coco\n",
    "\n",
    "    # Train a new model starting from ImageNet weights\n",
    "    python coco.py train --dataset=/path/to/coco/ --model=imagenet\n",
    "\n",
    "    # Continue training a model that you had trained earlier\n",
    "    python coco.py train --dataset=/path/to/coco/ --model=/path/to/weights.h5\n",
    "\n",
    "    # Continue training the last model you trained. This will find\n",
    "    # the last trained weights in the model directory.\n",
    "    python coco.py train --dataset=/path/to/coco/ --model=last\n",
    "\n",
    "If you have not yet downloaded the COCO dataset you should run the command with the download option set, e.g.:\n",
    "\n",
    "    # Train a new model starting from pre-trained COCO weights\n",
    "    python coco.py train --dataset=/path/to/coco/ --model=coco --download=true\n",
    "\n",
    "You can also run the COCO evaluation code with:\n",
    "\n",
    "    # Run COCO evaluation on the last trained model\n",
    "    python coco.py evaluate --dataset=/path/to/coco/ --model=last"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent Work\n",
    "\n",
    "Do the following:\n",
    "\n",
    " 1. Get the demo up and running.\n",
    " 2. Evaluate the pretrained COCO model on the COCO validation set we used last week.\n",
    " 3. Download the Cityscapes dataset and run Mask R-CNN on it in inference model. Report your results and see what errors you find.\n",
    " 4. Fine tune the COCO Mask R-CNN on Cityscapes and report the results. Are they close to what's reported in the Mask R-CNN paper?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
